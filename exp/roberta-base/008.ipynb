{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bed992f",
   "metadata": {
    "papermill": {
     "duration": 0.005838,
     "end_time": "2022-10-18T03:23:01.359058",
     "exception": false,
     "start_time": "2022-10-18T03:23:01.353220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da5f0fd",
   "metadata": {
    "papermill": {
     "duration": 6.305043,
     "end_time": "2022-10-18T03:23:07.670360",
     "exception": false,
     "start_time": "2022-10-18T03:23:01.365317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.6.4\n",
      "transformers version: 4.20.1\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f'TF version: {tf.__version__}')\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import transformers\n",
    "print(f'transformers version: {transformers.__version__}')\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../input/iterativestratification')\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d59b566-d4d4-415c-a0f3-0a9c141f0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb = False\n",
    "    DEBUG = False\n",
    "    TO_KAGGLE = True\n",
    "    MEMO = \"tensorflow ベースライン\"\n",
    "    file_name = \"008\"\n",
    "    score_path = \"/home/jupyter/output/scores/scores003.csv\"\n",
    "    model=\"roberta-base\"\n",
    "    epochs = 10\n",
    "    trn_fold=[0,1,2,3,4]\n",
    "    n_fold=5\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    \n",
    "if CFG.DEBUG:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9143d4ef-7c56-44f2-9874-3cd60e91a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# ====================================================\n",
    "# datetime\n",
    "# ====================================================\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)\n",
    "date = now.strftime('%Y%m%d')\n",
    "date2 = now.strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# file_path\n",
    "# ====================================================\n",
    "if \"/\" in CFG.model:\n",
    "    model_name = CFG.model.split(\"/\")[1]\n",
    "else:\n",
    "    model_name = CFG.model\n",
    "\n",
    "path =\"/home/jupyter/feedback-prize-english-language-learning/\"\n",
    "if CFG.DEBUG:\n",
    "    OUTPUT_DIR = f'/home/jupyter/output/ex/DEBUG/{model_name}/{CFG.file_name}/{date2}/'\n",
    "else:\n",
    "    OUTPUT_DIR = f'/home/jupyter/output/ex/{model_name}/{CFG.file_name}/{date2}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR+\"tokenizer/\")\n",
    "    \n",
    "# ====================================================\n",
    "# wandb \n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    import wandb\n",
    "    \n",
    "    def class2dict(f):\n",
    "            return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "        \n",
    "    project='Feedback Prize - English Language Learning'\n",
    "    if CFG.DEBUG:\n",
    "        group = \"DEBUG\"\n",
    "    else:\n",
    "        group = model_name\n",
    "    wandb_name = f\"{CFG.file_name}_{date2}\"\n",
    "    job_type = CFG.file_name  #\"train\"\n",
    "\n",
    "    wandb_api = \"your_id\"\n",
    "    wandb.login(key=wandb_api)\n",
    "    anony = None\n",
    "    run = wandb.init(project=project, \n",
    "                         name = wandb_name,\n",
    "                         config=class2dict(CFG),\n",
    "                         group=group,\n",
    "                         job_type=job_type,\n",
    "                         anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65e04c0",
   "metadata": {
    "papermill": {
     "duration": 0.016564,
     "end_time": "2022-10-18T03:23:07.693848",
     "exception": false,
     "start_time": "2022-10-18T03:23:07.677284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23fdae",
   "metadata": {
    "papermill": {
     "duration": 0.006128,
     "end_time": "2022-10-18T03:23:07.706400",
     "exception": false,
     "start_time": "2022-10-18T03:23:07.700272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2451c0",
   "metadata": {
    "papermill": {
     "duration": 0.27115,
     "end_time": "2022-10-18T03:23:07.983848",
     "exception": false,
     "start_time": "2022-10-18T03:23:07.712698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    783\n",
       "0    782\n",
       "4    782\n",
       "3    782\n",
       "2    782\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/jupyter/feedback-prize-english-language-learning/train.csv\")\n",
    "fold = np.load(\"/home/jupyter/output/fold/5folds.npy\")\n",
    "df['fold'] = fold\n",
    "\n",
    "if CFG.DEBUG:\n",
    "    df = df.head(5)\n",
    "    df[\"fold\"] = list(range(5))\n",
    "df['fold'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d4f1b",
   "metadata": {
    "papermill": {
     "duration": 0.010006,
     "end_time": "2022-10-18T03:23:08.682451",
     "exception": false,
     "start_time": "2022-10-18T03:23:08.672445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbaca64",
   "metadata": {
    "papermill": {
     "duration": 0.020928,
     "end_time": "2022-10-18T03:23:08.713191",
     "exception": false,
     "start_time": "2022-10-18T03:23:08.692263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "ROBERTA_MODEL = \"/home/jupyter/models_tensorflow/roberta-base/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccb61c",
   "metadata": {
    "papermill": {
     "duration": 0.01004,
     "end_time": "2022-10-18T03:23:08.733117",
     "exception": false,
     "start_time": "2022-10-18T03:23:08.723077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Why we should disable dropout in regression task, check this [discussion](https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/260729)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b823b47",
   "metadata": {
    "papermill": {
     "duration": 0.554599,
     "end_time": "2022-10-18T03:23:09.297554",
     "exception": false,
     "start_time": "2022-10-18T03:23:08.742955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(ROBERTA_MODEL)\n",
    "#tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "\n",
    "cfg = transformers.AutoConfig.from_pretrained(ROBERTA_MODEL)\n",
    "cfg.hidden_dropout_prob = 0\n",
    "cfg.attention_probs_dropout_prob = 0\n",
    "#cfg.save_pretrained(OUTPUT_DIR+'/tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548bd3e",
   "metadata": {
    "papermill": {
     "duration": 0.0073,
     "end_time": "2022-10-18T03:23:09.312646",
     "exception": false,
     "start_time": "2022-10-18T03:23:09.305346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Process Function\n",
    "\n",
    "To make use of HugggingFace RoBERTa model, we have to tokenize our input texts as the pretrained RoBERTa model requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420e2464",
   "metadata": {
    "papermill": {
     "duration": 0.019729,
     "end_time": "2022-10-18T03:23:09.339873",
     "exception": false,
     "start_time": "2022-10-18T03:23:09.320144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def roberta_encode(texts, tokenizer=tokenizer):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for text in texts.tolist():\n",
    "        token = tokenizer(text, \n",
    "                          add_special_tokens=True, \n",
    "                          max_length=MAX_LENGTH, \n",
    "                          return_attention_mask=True, \n",
    "                          return_tensors=\"np\", \n",
    "                          truncation=True, \n",
    "                          padding='max_length')\n",
    "        input_ids.append(token['input_ids'][0])\n",
    "        attention_mask.append(token['attention_mask'][0])\n",
    "    \n",
    "    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e77ff4f8",
   "metadata": {
    "papermill": {
     "duration": 0.016788,
     "end_time": "2022-10-18T03:23:09.364125",
     "exception": false,
     "start_time": "2022-10-18T03:23:09.347337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(df):\n",
    "    inputs = roberta_encode(df['full_text'])\n",
    "    targets = np.array(df[CFG.target_cols], dtype=\"float32\")\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efdeec",
   "metadata": {
    "papermill": {
     "duration": 0.007273,
     "end_time": "2022-10-18T03:23:09.378727",
     "exception": false,
     "start_time": "2022-10-18T03:23:09.371454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MeanPool Layer\n",
    "\n",
    "Instead of using '[CLS]' token, MeanPool layer averaging roberta's last hidden states along the sequence axis with masking out padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e13a0f",
   "metadata": {
    "papermill": {
     "duration": 0.016815,
     "end_time": "2022-10-18T03:23:09.403257",
     "exception": false,
     "start_time": "2022-10-18T03:23:09.386442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPool(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
    "        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n",
    "        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n",
    "        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n",
    "        return embedding_sum / mask_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5202801",
   "metadata": {
    "papermill": {
     "duration": 0.007377,
     "end_time": "2022-10-18T03:23:09.418057",
     "exception": false,
     "start_time": "2022-10-18T03:23:09.410680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a66971a-2a9c-4193-a694-31c8b1cb93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "\n",
    "    roberta_model = transformers.TFAutoModel.from_pretrained(ROBERTA_MODEL, config=cfg)\n",
    "\n",
    "    ## re-init#\n",
    "    REINIT_LAYERS = 1\n",
    "    normal_initializer = tf.keras.initializers.GlorotUniform()\n",
    "    zeros_initializer = tf.keras.initializers.Zeros()\n",
    "    ones_initializer = tf.keras.initializers.Ones()\n",
    "\n",
    "    for encoder_block in roberta_model.roberta.encoder.layer[-REINIT_LAYERS:]:\n",
    "        for layer in encoder_block.submodules:\n",
    "            if isinstance(layer, tf.keras.layers.Dense):\n",
    "                layer.kernel.assign(normal_initializer(shape=layer.kernel.shape, dtype=layer.kernel.dtype))\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.assign(zeros_initializer(shape=layer.bias.shape, dtype=layer.bias.dtype))\n",
    "\n",
    "            elif isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "                layer.beta.assign(zeros_initializer(shape=layer.beta.shape, dtype=layer.beta.dtype))\n",
    "                layer.gamma.assign(ones_initializer(shape=layer.gamma.shape, dtype=layer.gamma.dtype))\n",
    "    ####\n",
    "\n",
    "\n",
    "    roberta_output = roberta_model.roberta(\n",
    "        input_ids, attention_mask=attention_masks\n",
    "    )\n",
    "    x = roberta_output.last_hidden_state\n",
    "    x = MeanPool()(x, mask=attention_masks)\n",
    "    x = layers.Dense(6, activation='sigmoid')(x)\n",
    "    output = layers.Rescaling(scale=4.0, offset=1.0)(x)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "    ### LLRD###\n",
    "    layer_list = [roberta_model.roberta.embeddings] + list(roberta_model.roberta.encoder.layer)\n",
    "    layer_list.reverse()\n",
    "\n",
    "    INIT_LR = 1e-5\n",
    "    LLRDR = 0.9\n",
    "    LR_SCH_DECAY_STEPS = 1600 # 2 * len(train_df) // BATCH_SIZE\n",
    "\n",
    "    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=INIT_LR * LLRDR ** i, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3) for i in range(len(layer_list))]\n",
    "    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3)\n",
    "\n",
    "    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n",
    "\n",
    "    optimizers_and_layers = [(tf.keras.optimizers.Adam(learning_rate=lr_schedule_head), model.layers[-4:])] +\\\n",
    "        list(zip(optimizers, layer_list))\n",
    "\n",
    "    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='huber_loss',\n",
    "                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                 )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4511336",
   "metadata": {
    "papermill": {
     "duration": 19.973644,
     "end_time": "2022-10-18T03:23:29.424560",
     "exception": false,
     "start_time": "2022-10-18T03:23:09.450916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 10:20:44.981010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:44.983311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:44.985088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:44.987449: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-01 10:20:44.988383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:44.990230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:44.992010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:47.020156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:47.022394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:47.024177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-01 10:20:47.025879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13150 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-11-01 10:20:49.022552: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 124645632   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "mean_pool (MeanPool)            (None, 768)          0           roberta[0][0]                    \n",
      "                                                                 attention_masks[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 6)            4614        mean_pool[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "rescaling (Rescaling)           (None, 6)            0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 124,650,246\n",
      "Trainable params: 124,650,246\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e999506d-49d3-47c0-a08e-95b2a2ce4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1ab6a",
   "metadata": {
    "papermill": {
     "duration": 0.007087,
     "end_time": "2022-10-18T03:23:29.439384",
     "exception": false,
     "start_time": "2022-10-18T03:23:29.432297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5 Folds Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = []\n",
    "valid_rmses = []\n",
    "for fold in CFG.trn_fold:\n",
    "    print(f'\\n-----------FOLD {fold} ------------')\n",
    "    \n",
    "    #Create dataset\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "    train_dataset = get_dataset(train_df)\n",
    "    valid_dataset = get_dataset(valid_df)\n",
    "    \n",
    "    print('Data prepared.')\n",
    "    print(f'Training data input_ids shape: {train_dataset[0][0].shape} dtype: {train_dataset[0][0].dtype}') \n",
    "    print(f'Training data attention_mask shape: {train_dataset[0][1].shape} dtype: {train_dataset[0][1].dtype}')\n",
    "    print(f'Training data targets shape: {train_dataset[1].shape} dtype: {train_dataset[1].dtype}')\n",
    "    print(f'Validation data input_ids shape: {valid_dataset[0][0].shape} dtype: {valid_dataset[0][0].dtype}')\n",
    "    print(f'Validation data attention_mask shape: {valid_dataset[0][1].shape} dtype: {valid_dataset[0][1].dtype}')\n",
    "    print(f'Validation data targets shape: {valid_dataset[1].shape} dtype: {valid_dataset[1].dtype}')\n",
    "    \n",
    "    #Create model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = get_model()\n",
    "    \n",
    "    callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint(OUTPUT_DIR+f\"best_model_fold{fold}.h5\",\n",
    "                                                   monitor=\"val_loss\",\n",
    "                                                   mode=\"min\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   verbose=1,\n",
    "                                                   save_weights_only=True,),\n",
    "                tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                 min_delta=1e-5, \n",
    "                                                 patience=3, \n",
    "                                                 verbose=1,\n",
    "                                                 mode='min',)\n",
    "                ]\n",
    "    history = model.fit(x=train_dataset[0],\n",
    "                        y=train_dataset[1],\n",
    "                        validation_data=valid_dataset, \n",
    "                        epochs=CFG.epochs,\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        callbacks=callbacks\n",
    "                       )\n",
    "      \n",
    "    valid_rmses.append(np.min(history.history['val_root_mean_squared_error']))\n",
    "    \n",
    "    model2 = get_model()\n",
    "    model2.load_weights(OUTPUT_DIR+f\"best_model_fold{fold}.h5\")\n",
    "    pred = model2.predict(valid_dataset[0], batch_size=BATCH_SIZE)\n",
    "    df.loc[df['fold'] == fold,[f\"pred_{c}\" for c in CFG.target_cols]] = pred\n",
    "    _oof = df[df[\"fold\"]==fold]\n",
    "    oof.append(_oof)\n",
    "\n",
    "    score, scores = get_score(_oof[CFG.target_cols].to_numpy(), pred)\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "    LOGGER.info(f'Score: {score:.4f}  Scores: {scores}')\n",
    "    \n",
    "    del train_dataset, valid_dataset, train_df, valid_df,model,model2,_oof,score,scores\n",
    "    gc.collect()\n",
    "    \n",
    "    if CFG.wandb:\n",
    "        for epoch in range(len(history.history['val_root_mean_squared_error'])):\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_score\": history.history['root_mean_squared_error'][epoch], \n",
    "                       f\"[fold{fold}] avg_val_score\": history.history['val_root_mean_squared_error'][epoch],})\n",
    "            \n",
    "oof = pd.concat(oof)\n",
    "oof.to_pickle(OUTPUT_DIR+f'oof_df.pkl')\n",
    "score, scores = get_score(oof[CFG.target_cols].to_numpy(), oof[[f\"pred_{c}\" for c in CFG.target_cols]].to_numpy())\n",
    "LOGGER.info(f\"========== result ==========\")\n",
    "LOGGER.info(f'Score: {score:.4f}  Scores: {scores}')\n",
    "\n",
    "if CFG.wandb:\n",
    "        wandb.config.update(class2dict(CFG))\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ecff4",
   "metadata": {
    "papermill": {
     "duration": 0.720554,
     "end_time": "2022-10-18T05:25:52.529266",
     "exception": false,
     "start_time": "2022-10-18T05:25:51.808712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'{len(valid_rmses)} Folds validation RMSE:\\n{valid_rmses}')\n",
    "print(f'Local CV Average score: {np.mean(valid_rmses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae4366-3a7c-49e8-8af9-0ff046be877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.TO_KAGGLE:\n",
    "    UPLOAD_DIR = OUTPUT_DIR\n",
    "    EX_NO = f\"{CFG.model}-{CFG.file_name}\" # 実験番号などを入れる、folderのpathにする\n",
    "    USERID = 'your_id'\n",
    "\n",
    "\n",
    "    def dataset_upload():\n",
    "        import json\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "        id = f'{USERID}/{EX_NO}'\n",
    "\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata['id'] = id\n",
    "        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "        dataset_metadata['title'] = f'{EX_NO}'\n",
    "\n",
    "        with open(UPLOAD_DIR +'dataset-metadata.json', 'w') as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "\n",
    "        # データセットがない場合\n",
    "        if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n",
    "            api.dataset_create_new(folder=UPLOAD_DIR,\n",
    "                                   convert_to_csv=False,\n",
    "                                   dir_mode='skip')\n",
    "            \n",
    "            #apiコマンドを書き込む\n",
    "            f = open(f'{CFG.model}_api_command.txt', 'a')\n",
    "            api_command = f\"!kaggle datasets download -d hiroki8383/{EX_NO}\\n\"\n",
    "            f.write(api_command)\n",
    "            f.close()\n",
    "            \n",
    "            #フォルダーを削除\n",
    "            if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n",
    "                remove_files = glob.glob(OUTPUT_DIR+\"*\")\n",
    "                remove_files.remove(OUTPUT_DIR+\"oof_df.pkl\")\n",
    "                for file in remove_files:\n",
    "                    os.remove(file)\n",
    "                print(\"folder upload\")\n",
    "            else:\n",
    "                print(\"folder not upload\")\n",
    "            \n",
    "            \n",
    "        # データセットがある場合→更新されない場合がある（後で原因追及)\n",
    "        else:\n",
    "            print(\"this folder exsits\")\n",
    "            # api.dataset_create_version(folder=UPLOAD_DIR,\n",
    "            #                            version_notes='update',\n",
    "            #                            convert_to_csv=False,\n",
    "            #                            delete_old_versions=False,\n",
    "            #                            dir_mode='zip')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    dataset_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2f09d-0ddc-4c99-acd3-ebcf08a36b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CFG.DEBUG:\n",
    "    def to_write_score(CFG):\n",
    "        df = pd.read_csv(CFG.score_path)\n",
    "        def get_result2(oof_df):\n",
    "                labels = oof_df[CFG.target_cols].values\n",
    "                preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "                score, scores = get_score(labels, preds)\n",
    "                LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "                return score,scores\n",
    "\n",
    "        score,scores = get_result2(oof)\n",
    "        name = \"-\".join(OUTPUT_DIR.split(\"/\")[-4:-2])\n",
    "        base = {\"name\":name,\"score\":score,\"memo\":CFG.MEMO} \n",
    "        base.update(dict(zip(CFG.target_cols,scores)))\n",
    "        df = df.append(base,ignore_index=True)\n",
    "        df.to_csv(CFG.score_path,index=False)\n",
    "    to_write_score(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d621e-48e2-4732-a6f6-fda9f2eeadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CFG.score_path)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7450.07993,
   "end_time": "2022-10-18T05:27:03.651111",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-18T03:22:53.571181",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
