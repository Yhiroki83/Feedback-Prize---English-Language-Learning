{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006569,
     "end_time": "2022-08-31T07:01:57.395826",
     "exception": false,
     "start_time": "2022-08-31T07:01:57.389257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.015868,
     "end_time": "2022-08-31T07:01:57.417077",
     "exception": false,
     "start_time": "2022-08-31T07:01:57.401209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb = True\n",
    "    DEBUG = False\n",
    "    TO_KAGGLE = True\n",
    "    DL = False\n",
    "    file_name = \"002\"\n",
    "    name = \"electra\"\n",
    "    model=\"base-discriminator\"\n",
    "    \n",
    "    n_fold=4\n",
    "    trn_fold=[0,1,2,3]\n",
    "    model_config_path = f\"/home/jupyter/models/{name}/{model}/\"\n",
    "    model_bin_path = f\"/home/jupyter/models/{name}/{model}/\"\n",
    "    competition='FB3'\n",
    "    apex=True\n",
    "    print_freq=20\n",
    "    num_workers=4\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=6\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=8\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    train=True\n",
    "    \n",
    "if CFG.DEBUG:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhiroki8383\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/exp/electra/wandb/run-20221016_025544-8d4syu96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning/runs/8d4syu96\" target=\"_blank\">002_202210161155</a></strong> to <a href=\"https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "# ====================================================\n",
    "# datetime\n",
    "# ====================================================\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)\n",
    "date = now.strftime('%Y%m%d')\n",
    "date2 = now.strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# file_path\n",
    "# ====================================================\n",
    "if \"/\" in CFG.model:\n",
    "    model_name = CFG.model.split(\"/\")[1]\n",
    "else:\n",
    "    model_name = CFG.model\n",
    "\n",
    "path =\"/home/jupyter/feedback-prize-english-language-learning/\"\n",
    "if CFG.DEBUG:\n",
    "    OUTPUT_DIR = f'/home/jupyter/output/ex/DEBUG/{CFG.name}/{model_name}/{CFG.file_name}/{date2}/'\n",
    "else:\n",
    "    OUTPUT_DIR = f'/home/jupyter/output/ex/{CFG.name}/{model_name}/{CFG.file_name}/{date2}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# wandb \n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    import wandb\n",
    "    \n",
    "    def class2dict(f):\n",
    "            return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "        \n",
    "    project='Feedback Prize - English Language Learning'\n",
    "    if CFG.DEBUG:\n",
    "        group = \"DEBUG\"\n",
    "    else:\n",
    "        group = model_name\n",
    "    wandb_name = f\"{CFG.file_name}_{date2}\"\n",
    "    job_type = CFG.file_name  #\"train\"\n",
    "\n",
    "    wandb_api = \"your_id\"\n",
    "    wandb.login(key=wandb_api)\n",
    "    anony = None\n",
    "    run = wandb.init(project=project, \n",
    "                         name = wandb_name,\n",
    "                         config=class2dict(CFG),\n",
    "                         group=group,\n",
    "                         job_type=job_type,\n",
    "                         anonymous=anony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008876,
     "end_time": "2022-08-31T07:02:06.467728",
     "exception": false,
     "start_time": "2022-08-31T07:02:06.458852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip download transformers==4.21.2\n",
    "# !pip download tokenizers==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex084.py\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "import logging\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import ElectraTokenizer\n",
    "from transformers import ElectraModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 57.587416,
     "end_time": "2022-08-31T07:03:04.064247",
     "exception": false,
     "start_time": "2022-08-31T07:02:06.476831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iterative-stratification==0.1.7 in /opt/conda/lib/python3.7/site-packages (0.1.7)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.7.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.21.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "os.system('pip install iterative-stratification==0.1.7')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# os.system('pip uninstall -y transformers')\n",
    "# os.system('pip uninstall -y tokenizers')\n",
    "# os.system('python -m pip install --no-index --find-links=/home/jupyter/code_baseline/FB3_pip_wheels transformers')\n",
    "# os.system('python -m pip install --no-index --find-links=/home/jupyter/code_baseline/FB3_pip_wheels tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "# print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "# print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007998,
     "end_time": "2022-08-31T07:03:04.079768",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.07177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.024132,
     "end_time": "2022-08-31T07:03:04.111108",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.086976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012589,
     "end_time": "2022-08-31T07:03:04.13341",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.120821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.242687,
     "end_time": "2022-08-31T07:03:04.383434",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.140747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "submission = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "if CFG.DEBUG:\n",
    "    # display(train.groupby('fold').size())\n",
    "    train = train.sample(n=50, random_state=0).reset_index(drop=True)\n",
    "    # display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007561,
     "end_time": "2022-08-31T07:03:04.604916",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.597355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 7.351568,
     "end_time": "2022-08-31T07:03:11.964298",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.61273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer models/roberta/roberta-base/config.json\n",
    "# ====================================================\n",
    "tokenizer = ElectraTokenizer.from_pretrained(f\"/home/jupyter/models/{CFG.name}/{CFG.model}/\")\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008127,
     "end_time": "2022-08-31T07:03:11.985369",
     "exception": false,
     "start_time": "2022-08-31T07:03:11.977242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 5.893032,
     "end_time": "2022-08-31T07:03:17.886504",
     "exception": false,
     "start_time": "2022-08-31T07:03:11.993472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "# lengths = []\n",
    "# tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
    "# for text in tk0:\n",
    "#     length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "#     lengths.append(length)\n",
    "# CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
    "# LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.020447,
     "end_time": "2022-08-31T07:03:17.916566",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.896119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008073,
     "end_time": "2022-08-31T07:03:17.933189",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.925116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.033105,
     "end_time": "2022-08-31T07:03:17.97447",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.941365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "#MeanPoolingはoutput_hidden_statesに関係している   https://qiita.com/niship2/items/f84751aed893da869cec\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model_config_path, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model_bin_path, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008106,
     "end_time": "2022-08-31T07:03:17.993861",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.985755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.021697,
     "end_time": "2022-08-31T07:03:18.02376",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.002063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Loss\n",
    "# ====================================================\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008452,
     "end_time": "2022-08-31T07:03:18.041557",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.033105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.030759,
     "end_time": "2022-08-31T07:03:18.08056",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.049801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0081,
     "end_time": "2022-08-31T07:03:18.100232",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.092132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.033332,
     "end_time": "2022-08-31T07:03:18.141812",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.10848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
    "    \n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 11935.46951,
     "end_time": "2022-08-31T10:22:13.621316",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.151806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/electra/base-discriminator/\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/electra/base-discriminator/ were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 1s (remain 10m 48s) Loss: 2.9248(2.9248) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 25s (remain 7m 7s) Loss: 1.5654(2.0445) Grad: 237423.2344  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 0m 45s (remain 5m 57s) Loss: 0.4315(1.3608) Grad: 107250.3125  LR: 0.00001998  \n",
      "Epoch: [1][60/366] Elapsed 0m 56s (remain 4m 44s) Loss: 0.1321(0.9760) Grad: 137811.9375  LR: 0.00001996  \n",
      "Epoch: [1][80/366] Elapsed 1m 7s (remain 3m 56s) Loss: 0.2220(0.7871) Grad: 64123.6016  LR: 0.00001993  \n",
      "Epoch: [1][100/366] Elapsed 1m 16s (remain 3m 21s) Loss: 0.2842(0.6774) Grad: 63002.1641  LR: 0.00001990  \n",
      "Epoch: [1][120/366] Elapsed 1m 25s (remain 2m 53s) Loss: 0.1970(0.5968) Grad: 49532.1016  LR: 0.00001985  \n",
      "Epoch: [1][140/366] Elapsed 1m 34s (remain 2m 31s) Loss: 0.1832(0.5306) Grad: 85606.8047  LR: 0.00001980  \n",
      "Epoch: [1][160/366] Elapsed 1m 43s (remain 2m 11s) Loss: 0.1697(0.4806) Grad: 212213.7344  LR: 0.00001974  \n",
      "Epoch: [1][180/366] Elapsed 1m 52s (remain 1m 54s) Loss: 0.1341(0.4426) Grad: 90100.6953  LR: 0.00001967  \n",
      "Epoch: [1][200/366] Elapsed 2m 0s (remain 1m 39s) Loss: 0.1670(0.4109) Grad: 44151.8711  LR: 0.00001959  \n",
      "Epoch: [1][220/366] Elapsed 2m 9s (remain 1m 24s) Loss: 0.1320(0.3889) Grad: 83969.9453  LR: 0.00001951  \n",
      "Epoch: [1][240/366] Elapsed 2m 18s (remain 1m 11s) Loss: 0.1165(0.3676) Grad: 43465.1484  LR: 0.00001941  \n",
      "Epoch: [1][260/366] Elapsed 2m 26s (remain 0m 59s) Loss: 0.1844(0.3495) Grad: 80076.2734  LR: 0.00001931  \n",
      "Epoch: [1][280/366] Elapsed 2m 35s (remain 0m 46s) Loss: 0.1031(0.3341) Grad: 113315.3672  LR: 0.00001920  \n",
      "Epoch: [1][300/366] Elapsed 2m 43s (remain 0m 35s) Loss: 0.1289(0.3209) Grad: 150384.1719  LR: 0.00001909  \n",
      "Epoch: [1][320/366] Elapsed 2m 52s (remain 0m 24s) Loss: 0.1073(0.3089) Grad: 70235.0938  LR: 0.00001897  \n",
      "Epoch: [1][340/366] Elapsed 3m 1s (remain 0m 13s) Loss: 0.1596(0.2988) Grad: 72222.5938  LR: 0.00001884  \n",
      "Epoch: [1][360/366] Elapsed 3m 9s (remain 0m 2s) Loss: 0.1017(0.2895) Grad: 30975.1543  LR: 0.00001870  \n",
      "Epoch: [1][365/366] Elapsed 3m 11s (remain 0m 0s) Loss: 0.1112(0.2871) Grad: 35162.9727  LR: 0.00001866  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 19s) Loss: 0.0983(0.0983) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1037(0.1164) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.1034(0.1160) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2871  avg_val_loss: 0.1183  time: 229s\n",
      "Epoch 1 - Score: 0.4876  Scores: [0.5264274753663797, 0.4736666617860685, 0.4411118505853099, 0.47980765842302003, 0.5182021695974368, 0.48656476999592535]\n",
      "Epoch 1 - Save Best Score: 0.4876 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1022(0.1183) \n",
      "EVAL: [61/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1192(0.1183) \n",
      "Epoch: [2][0/366] Elapsed 0m 0s (remain 6m 2s) Loss: 0.1407(0.1407) Grad: 171696.5469  LR: 0.00001866  \n",
      "Epoch: [2][20/366] Elapsed 0m 9s (remain 2m 36s) Loss: 0.1037(0.1094) Grad: 84079.2891  LR: 0.00001851  \n",
      "Epoch: [2][40/366] Elapsed 0m 18s (remain 2m 22s) Loss: 0.1223(0.1131) Grad: 88962.5547  LR: 0.00001836  \n",
      "Epoch: [2][60/366] Elapsed 0m 26s (remain 2m 12s) Loss: 0.0789(0.1150) Grad: 117102.5781  LR: 0.00001820  \n",
      "Epoch: [2][80/366] Elapsed 0m 34s (remain 2m 3s) Loss: 0.0753(0.1103) Grad: 83879.9609  LR: 0.00001803  \n",
      "Epoch: [2][100/366] Elapsed 0m 43s (remain 1m 54s) Loss: 0.0921(0.1088) Grad: 88944.1562  LR: 0.00001786  \n",
      "Epoch: [2][120/366] Elapsed 0m 51s (remain 1m 45s) Loss: 0.1026(0.1090) Grad: 97006.2891  LR: 0.00001768  \n",
      "Epoch: [2][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.1508(0.1088) Grad: 137814.9375  LR: 0.00001749  \n",
      "Epoch: [2][160/366] Elapsed 1m 9s (remain 1m 27s) Loss: 0.1197(0.1106) Grad: 128797.9141  LR: 0.00001730  \n",
      "Epoch: [2][180/366] Elapsed 1m 17s (remain 1m 19s) Loss: 0.0960(0.1112) Grad: 118425.4766  LR: 0.00001710  \n",
      "Epoch: [2][200/366] Elapsed 1m 26s (remain 1m 10s) Loss: 0.1065(0.1130) Grad: 189860.1094  LR: 0.00001689  \n",
      "Epoch: [2][220/366] Elapsed 1m 34s (remain 1m 2s) Loss: 0.0948(0.1134) Grad: 112964.9219  LR: 0.00001668  \n",
      "Epoch: [2][240/366] Elapsed 1m 43s (remain 0m 53s) Loss: 0.1165(0.1143) Grad: 88356.5312  LR: 0.00001647  \n",
      "Epoch: [2][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.1107(0.1146) Grad: 165307.1719  LR: 0.00001625  \n",
      "Epoch: [2][280/366] Elapsed 2m 0s (remain 0m 36s) Loss: 0.1615(0.1144) Grad: 110903.3594  LR: 0.00001602  \n",
      "Epoch: [2][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.1061(0.1138) Grad: 255475.5000  LR: 0.00001579  \n",
      "Epoch: [2][320/366] Elapsed 2m 17s (remain 0m 19s) Loss: 0.0989(0.1136) Grad: 68901.6250  LR: 0.00001556  \n",
      "Epoch: [2][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.1124(0.1137) Grad: 301697.3750  LR: 0.00001532  \n",
      "Epoch: [2][360/366] Elapsed 2m 34s (remain 0m 2s) Loss: 0.1693(0.1130) Grad: 287419.7500  LR: 0.00001507  \n",
      "Epoch: [2][365/366] Elapsed 2m 36s (remain 0m 0s) Loss: 0.0740(0.1127) Grad: 96178.9062  LR: 0.00001501  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 30s) Loss: 0.0879(0.0879) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1127(0.1187) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.0857(0.1197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1127  avg_val_loss: 0.1210  time: 194s\n",
      "Epoch 2 - Score: 0.4936  Scores: [0.5256025580804254, 0.4813395422031335, 0.44381832926463344, 0.5135958420794992, 0.5107918965283877, 0.4864689074536067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0997(0.1212) \n",
      "EVAL: [61/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0601(0.1210) \n",
      "Epoch: [3][0/366] Elapsed 0m 0s (remain 5m 42s) Loss: 0.0901(0.0901) Grad: 106644.0625  LR: 0.00001500  \n",
      "Epoch: [3][20/366] Elapsed 0m 9s (remain 2m 34s) Loss: 0.0886(0.1087) Grad: 98724.2344  LR: 0.00001475  \n",
      "Epoch: [3][40/366] Elapsed 0m 17s (remain 2m 22s) Loss: 0.0953(0.1046) Grad: 157949.0156  LR: 0.00001450  \n",
      "Epoch: [3][60/366] Elapsed 0m 26s (remain 2m 12s) Loss: 0.1076(0.1034) Grad: 137534.2188  LR: 0.00001424  \n",
      "Epoch: [3][80/366] Elapsed 0m 34s (remain 2m 2s) Loss: 0.0836(0.0990) Grad: 96094.6797  LR: 0.00001398  \n",
      "Epoch: [3][100/366] Elapsed 0m 43s (remain 1m 53s) Loss: 0.0956(0.0961) Grad: 111962.3594  LR: 0.00001372  \n",
      "Epoch: [3][120/366] Elapsed 0m 51s (remain 1m 45s) Loss: 0.0994(0.0960) Grad: 91114.4609  LR: 0.00001345  \n",
      "Epoch: [3][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.1031(0.0969) Grad: 167570.5625  LR: 0.00001318  \n",
      "Epoch: [3][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.1188(0.0963) Grad: 136882.3281  LR: 0.00001291  \n",
      "Epoch: [3][180/366] Elapsed 1m 17s (remain 1m 19s) Loss: 0.1157(0.0968) Grad: 152136.9688  LR: 0.00001263  \n",
      "Epoch: [3][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0916(0.0980) Grad: 81461.4297  LR: 0.00001236  \n",
      "Epoch: [3][220/366] Elapsed 1m 34s (remain 1m 1s) Loss: 0.0802(0.0983) Grad: 94106.5781  LR: 0.00001208  \n",
      "Epoch: [3][240/366] Elapsed 1m 43s (remain 0m 53s) Loss: 0.1241(0.0994) Grad: 148019.5312  LR: 0.00001180  \n",
      "Epoch: [3][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.1248(0.1005) Grad: 196384.3281  LR: 0.00001152  \n",
      "Epoch: [3][280/366] Elapsed 2m 0s (remain 0m 36s) Loss: 0.1063(0.0998) Grad: 73313.4141  LR: 0.00001123  \n",
      "Epoch: [3][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.0683(0.0993) Grad: 149474.5938  LR: 0.00001095  \n",
      "Epoch: [3][320/366] Elapsed 2m 17s (remain 0m 19s) Loss: 0.1170(0.0988) Grad: 158862.1562  LR: 0.00001066  \n",
      "Epoch: [3][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.0906(0.1002) Grad: 144668.1250  LR: 0.00001038  \n",
      "Epoch: [3][360/366] Elapsed 2m 34s (remain 0m 2s) Loss: 0.1252(0.1004) Grad: 113585.8984  LR: 0.00001009  \n",
      "Epoch: [3][365/366] Elapsed 2m 36s (remain 0m 0s) Loss: 0.0807(0.1004) Grad: 141077.2188  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 18s) Loss: 0.0750(0.0750) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0993(0.1054) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.0841(0.1071) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1004  avg_val_loss: 0.1087  time: 193s\n",
      "Epoch 3 - Score: 0.4670  Scores: [0.5014305178385129, 0.4554597646427725, 0.42847728509086147, 0.4656897655925051, 0.48590425345399085, 0.4647803782335157]\n",
      "Epoch 3 - Save Best Score: 0.4670 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0930(0.1088) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0776(0.1087) \n",
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 6m 10s) Loss: 0.0583(0.0583) Grad: 72346.5703  LR: 0.00001001  \n",
      "Epoch: [4][20/366] Elapsed 0m 9s (remain 2m 31s) Loss: 0.0661(0.0898) Grad: 205379.0781  LR: 0.00000972  \n",
      "Epoch: [4][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.1482(0.0969) Grad: 141653.5156  LR: 0.00000944  \n",
      "Epoch: [4][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.0635(0.0935) Grad: 111029.2031  LR: 0.00000915  \n",
      "Epoch: [4][80/366] Elapsed 0m 34s (remain 2m 2s) Loss: 0.0690(0.0907) Grad: 117990.9531  LR: 0.00000887  \n",
      "Epoch: [4][100/366] Elapsed 0m 43s (remain 1m 53s) Loss: 0.1051(0.0903) Grad: 122445.5156  LR: 0.00000858  \n",
      "Epoch: [4][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0501(0.0908) Grad: 103104.1328  LR: 0.00000830  \n",
      "Epoch: [4][140/366] Elapsed 1m 0s (remain 1m 35s) Loss: 0.0824(0.0907) Grad: 113487.7812  LR: 0.00000802  \n",
      "Epoch: [4][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0773(0.0901) Grad: 58932.2344  LR: 0.00000774  \n",
      "Epoch: [4][180/366] Elapsed 1m 17s (remain 1m 18s) Loss: 0.0809(0.0895) Grad: 85080.3516  LR: 0.00000746  \n",
      "Epoch: [4][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0702(0.0889) Grad: 77297.6250  LR: 0.00000719  \n",
      "Epoch: [4][220/366] Elapsed 1m 34s (remain 1m 1s) Loss: 0.1017(0.0887) Grad: 184419.8125  LR: 0.00000692  \n",
      "Epoch: [4][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0878(0.0894) Grad: 127843.0781  LR: 0.00000664  \n",
      "Epoch: [4][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.1034(0.0894) Grad: 122747.4688  LR: 0.00000638  \n",
      "Epoch: [4][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0609(0.0895) Grad: 117173.4922  LR: 0.00000611  \n",
      "Epoch: [4][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.1821(0.0892) Grad: 159826.5000  LR: 0.00000585  \n",
      "Epoch: [4][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1305(0.0894) Grad: 105820.0703  LR: 0.00000559  \n",
      "Epoch: [4][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.0913(0.0888) Grad: 135169.5781  LR: 0.00000534  \n",
      "Epoch: [4][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0709(0.0883) Grad: 67744.0000  LR: 0.00000509  \n",
      "Epoch: [4][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.1170(0.0884) Grad: 160009.2031  LR: 0.00000502  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 29s) Loss: 0.0712(0.0712) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1050(0.1116) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.1020(0.1124) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0884  avg_val_loss: 0.1162  time: 193s\n",
      "Epoch 4 - Score: 0.4832  Scores: [0.5241913407564591, 0.4795396044516489, 0.4395902311040395, 0.47659090947604016, 0.5084874948402379, 0.4705822302518511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1074(0.1162) \n",
      "EVAL: [61/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1049(0.1162) \n",
      "Epoch: [5][0/366] Elapsed 0m 0s (remain 5m 44s) Loss: 0.0854(0.0854) Grad: 91633.7734  LR: 0.00000501  \n",
      "Epoch: [5][20/366] Elapsed 0m 9s (remain 2m 35s) Loss: 0.0879(0.0838) Grad: 73634.6875  LR: 0.00000477  \n",
      "Epoch: [5][40/366] Elapsed 0m 17s (remain 2m 22s) Loss: 0.0976(0.0792) Grad: 121752.2578  LR: 0.00000453  \n",
      "Epoch: [5][60/366] Elapsed 0m 26s (remain 2m 12s) Loss: 0.0765(0.0767) Grad: 86518.0859  LR: 0.00000429  \n",
      "Epoch: [5][80/366] Elapsed 0m 35s (remain 2m 3s) Loss: 0.0676(0.0782) Grad: 123437.2266  LR: 0.00000406  \n",
      "Epoch: [5][100/366] Elapsed 0m 43s (remain 1m 54s) Loss: 0.0708(0.0786) Grad: 71648.0547  LR: 0.00000383  \n",
      "Epoch: [5][120/366] Elapsed 0m 52s (remain 1m 45s) Loss: 0.0990(0.0795) Grad: 130871.8672  LR: 0.00000361  \n",
      "Epoch: [5][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.0938(0.0788) Grad: 98273.0938  LR: 0.00000339  \n",
      "Epoch: [5][160/366] Elapsed 1m 9s (remain 1m 27s) Loss: 0.1705(0.0795) Grad: 134877.9062  LR: 0.00000318  \n",
      "Epoch: [5][180/366] Elapsed 1m 17s (remain 1m 19s) Loss: 0.0821(0.0798) Grad: 124803.8906  LR: 0.00000297  \n",
      "Epoch: [5][200/366] Elapsed 1m 26s (remain 1m 10s) Loss: 0.0804(0.0798) Grad: 120665.3906  LR: 0.00000277  \n",
      "Epoch: [5][220/366] Elapsed 1m 34s (remain 1m 2s) Loss: 0.0749(0.0800) Grad: 99555.1328  LR: 0.00000258  \n",
      "Epoch: [5][240/366] Elapsed 1m 43s (remain 0m 53s) Loss: 0.1101(0.0800) Grad: 210950.9531  LR: 0.00000239  \n",
      "Epoch: [5][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.0945(0.0792) Grad: 183665.7031  LR: 0.00000221  \n",
      "Epoch: [5][280/366] Elapsed 2m 0s (remain 0m 36s) Loss: 0.0571(0.0789) Grad: 121300.0234  LR: 0.00000203  \n",
      "Epoch: [5][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.0658(0.0787) Grad: 84777.3438  LR: 0.00000186  \n",
      "Epoch: [5][320/366] Elapsed 2m 17s (remain 0m 19s) Loss: 0.0931(0.0788) Grad: 92142.9062  LR: 0.00000170  \n",
      "Epoch: [5][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.0734(0.0783) Grad: 170615.0938  LR: 0.00000154  \n",
      "Epoch: [5][360/366] Elapsed 2m 34s (remain 0m 2s) Loss: 0.0787(0.0781) Grad: 111796.5859  LR: 0.00000139  \n",
      "Epoch: [5][365/366] Elapsed 2m 36s (remain 0m 0s) Loss: 0.0750(0.0781) Grad: 83243.1875  LR: 0.00000136  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 22s) Loss: 0.0642(0.0642) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1077(0.1087) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.0840(0.1097) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0781  avg_val_loss: 0.1114  time: 193s\n",
      "Epoch 5 - Score: 0.4727  Scores: [0.5069764630769062, 0.4656028204663751, 0.429498058323544, 0.4785780734259435, 0.49026263905524736, 0.4652553468989745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0945(0.1115) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0580(0.1114) \n",
      "Epoch: [6][0/366] Elapsed 0m 0s (remain 5m 6s) Loss: 0.0604(0.0604) Grad: 74229.5859  LR: 0.00000135  \n",
      "Epoch: [6][20/366] Elapsed 0m 9s (remain 2m 33s) Loss: 0.0561(0.0750) Grad: 72937.4375  LR: 0.00000121  \n",
      "Epoch: [6][40/366] Elapsed 0m 17s (remain 2m 21s) Loss: 0.0754(0.0698) Grad: 79752.6094  LR: 0.00000108  \n",
      "Epoch: [6][60/366] Elapsed 0m 26s (remain 2m 11s) Loss: 0.0877(0.0702) Grad: 147469.5000  LR: 0.00000095  \n",
      "Epoch: [6][80/366] Elapsed 0m 34s (remain 2m 2s) Loss: 0.0580(0.0712) Grad: 77745.8203  LR: 0.00000083  \n",
      "Epoch: [6][100/366] Elapsed 0m 43s (remain 1m 53s) Loss: 0.0589(0.0707) Grad: 81380.9453  LR: 0.00000072  \n",
      "Epoch: [6][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0666(0.0712) Grad: 127374.2031  LR: 0.00000062  \n",
      "Epoch: [6][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.0610(0.0719) Grad: 51617.6289  LR: 0.00000053  \n",
      "Epoch: [6][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0623(0.0720) Grad: 67877.6484  LR: 0.00000044  \n",
      "Epoch: [6][180/366] Elapsed 1m 17s (remain 1m 19s) Loss: 0.0675(0.0719) Grad: 115895.6484  LR: 0.00000036  \n",
      "Epoch: [6][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0780(0.0719) Grad: 66139.6562  LR: 0.00000029  \n",
      "Epoch: [6][220/366] Elapsed 1m 34s (remain 1m 1s) Loss: 0.0820(0.0717) Grad: 166397.9375  LR: 0.00000022  \n",
      "Epoch: [6][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0522(0.0719) Grad: 64223.3672  LR: 0.00000017  \n",
      "Epoch: [6][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.0769(0.0726) Grad: 76718.3594  LR: 0.00000012  \n",
      "Epoch: [6][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0756(0.0726) Grad: 65390.5742  LR: 0.00000008  \n",
      "Epoch: [6][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.0627(0.0728) Grad: 121257.6641  LR: 0.00000005  \n",
      "Epoch: [6][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0596(0.0725) Grad: 124861.3203  LR: 0.00000002  \n",
      "Epoch: [6][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.0610(0.0725) Grad: 97932.0156  LR: 0.00000001  \n",
      "Epoch: [6][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0910(0.0724) Grad: 109544.5938  LR: 0.00000000  \n",
      "Epoch: [6][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0500(0.0723) Grad: 56066.1250  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 26s) Loss: 0.0627(0.0627) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1061(0.1076) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.0884(0.1085) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0723  avg_val_loss: 0.1107  time: 193s\n",
      "Epoch 6 - Score: 0.4712  Scores: [0.5052019948508099, 0.462680016135976, 0.4276503557398644, 0.4758462961528135, 0.4892573059959531, 0.46637993053813676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0969(0.1108) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0649(0.1107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.4670  Scores: [0.5014305178385129, 0.4554597646427725, 0.42847728509086147, 0.4656897655925051, 0.48590425345399085, 0.4647803782335157]\n",
      "========== fold: 1 training ==========\n",
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/electra/base-discriminator/\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/electra/base-discriminator/ were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 0s (remain 5m 59s) Loss: 2.4766(2.4766) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 9s (remain 2m 33s) Loss: 1.4742(2.0861) Grad: 206634.8125  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 0m 17s (remain 2m 20s) Loss: 0.3287(1.5032) Grad: 173834.7812  LR: 0.00001998  \n",
      "Epoch: [1][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.2495(1.0993) Grad: 86492.5000  LR: 0.00001996  \n",
      "Epoch: [1][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.3666(0.8859) Grad: 332854.4688  LR: 0.00001993  \n",
      "Epoch: [1][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.1418(0.7502) Grad: 67879.4141  LR: 0.00001990  \n",
      "Epoch: [1][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.1760(0.6548) Grad: 75180.0859  LR: 0.00001985  \n",
      "Epoch: [1][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.1697(0.5882) Grad: 165976.7969  LR: 0.00001980  \n",
      "Epoch: [1][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.1995(0.5348) Grad: 229430.5312  LR: 0.00001974  \n",
      "Epoch: [1][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1881(0.4930) Grad: 84016.1016  LR: 0.00001967  \n",
      "Epoch: [1][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.1676(0.4597) Grad: 95287.4688  LR: 0.00001959  \n",
      "Epoch: [1][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.1458(0.4299) Grad: 53747.1289  LR: 0.00001951  \n",
      "Epoch: [1][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.1381(0.4064) Grad: 142231.0938  LR: 0.00001941  \n",
      "Epoch: [1][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1112(0.3862) Grad: 59315.1211  LR: 0.00001931  \n",
      "Epoch: [1][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.1144(0.3690) Grad: 60470.7578  LR: 0.00001921  \n",
      "Epoch: [1][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0793(0.3534) Grad: 68791.0703  LR: 0.00001909  \n",
      "Epoch: [1][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1405(0.3397) Grad: 84445.8750  LR: 0.00001897  \n",
      "Epoch: [1][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.1246(0.3274) Grad: 78539.0625  LR: 0.00001884  \n",
      "Epoch: [1][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0868(0.3161) Grad: 56541.8281  LR: 0.00001870  \n",
      "Epoch: [1][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.2030(0.3142) Grad: 123628.0234  LR: 0.00001867  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 30s) Loss: 0.1138(0.1138) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1557(0.1490) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.1360(0.1522) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.3142  avg_val_loss: 0.1531  time: 192s\n",
      "Epoch 1 - Score: 0.5575  Scores: [0.5669881257895465, 0.5126471572174647, 0.5107344124289668, 0.5603647610496394, 0.6260757986971696, 0.5682628887503922]\n",
      "Epoch 1 - Save Best Score: 0.5575 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1356(0.1532) \n",
      "EVAL: [61/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0510(0.1531) \n",
      "Epoch: [2][0/366] Elapsed 0m 0s (remain 5m 41s) Loss: 0.1477(0.1477) Grad: 169243.9688  LR: 0.00001866  \n",
      "Epoch: [2][20/366] Elapsed 0m 9s (remain 2m 33s) Loss: 0.0850(0.1217) Grad: 70952.2422  LR: 0.00001851  \n",
      "Epoch: [2][40/366] Elapsed 0m 17s (remain 2m 21s) Loss: 0.0848(0.1147) Grad: 195640.0938  LR: 0.00001836  \n",
      "Epoch: [2][60/366] Elapsed 0m 26s (remain 2m 11s) Loss: 0.1800(0.1145) Grad: 284270.3438  LR: 0.00001820  \n",
      "Epoch: [2][80/366] Elapsed 0m 34s (remain 2m 2s) Loss: 0.1585(0.1151) Grad: 244282.8594  LR: 0.00001803  \n",
      "Epoch: [2][100/366] Elapsed 0m 43s (remain 1m 53s) Loss: 0.0478(0.1140) Grad: 80788.9922  LR: 0.00001786  \n",
      "Epoch: [2][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.1485(0.1167) Grad: 187667.1875  LR: 0.00001768  \n",
      "Epoch: [2][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.1758(0.1193) Grad: 98787.0234  LR: 0.00001749  \n",
      "Epoch: [2][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.1339(0.1212) Grad: 197403.8750  LR: 0.00001730  \n",
      "Epoch: [2][180/366] Elapsed 1m 17s (remain 1m 18s) Loss: 0.0932(0.1202) Grad: 133195.2969  LR: 0.00001710  \n",
      "Epoch: [2][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.1451(0.1212) Grad: 103389.8125  LR: 0.00001690  \n",
      "Epoch: [2][220/366] Elapsed 1m 34s (remain 1m 1s) Loss: 0.1238(0.1208) Grad: 180339.6406  LR: 0.00001669  \n",
      "Epoch: [2][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0823(0.1199) Grad: 64558.7500  LR: 0.00001647  \n",
      "Epoch: [2][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.1167(0.1198) Grad: 94950.7422  LR: 0.00001625  \n",
      "Epoch: [2][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0985(0.1191) Grad: 87230.5156  LR: 0.00001603  \n",
      "Epoch: [2][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.0767(0.1189) Grad: 85595.6875  LR: 0.00001580  \n",
      "Epoch: [2][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1327(0.1189) Grad: 90187.3828  LR: 0.00001556  \n",
      "Epoch: [2][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.0845(0.1187) Grad: 51837.8594  LR: 0.00001532  \n",
      "Epoch: [2][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1163(0.1192) Grad: 108254.8750  LR: 0.00001508  \n",
      "Epoch: [2][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0716(0.1189) Grad: 62108.1758  LR: 0.00001502  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 53s) Loss: 0.0953(0.0953) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 26s) Loss: 0.1411(0.1285) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 13s) Loss: 0.1126(0.1260) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1189  avg_val_loss: 0.1214  time: 193s\n",
      "Epoch 2 - Score: 0.4947  Scores: [0.5167920486186781, 0.49528797292720445, 0.43979574295794915, 0.4941365025686143, 0.5237299568608932, 0.4984119674143185]\n",
      "Epoch 2 - Save Best Score: 0.4947 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1015(0.1214) \n",
      "EVAL: [61/62] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0571(0.1214) \n",
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 6m 20s) Loss: 0.1238(0.1238) Grad: 158129.2656  LR: 0.00001500  \n",
      "Epoch: [3][20/366] Elapsed 0m 9s (remain 2m 32s) Loss: 0.1148(0.1120) Grad: 74838.8438  LR: 0.00001475  \n",
      "Epoch: [3][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.1228(0.1075) Grad: 105996.3906  LR: 0.00001450  \n",
      "Epoch: [3][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.1007(0.1072) Grad: 170439.1250  LR: 0.00001424  \n",
      "Epoch: [3][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.0781(0.1079) Grad: 166332.2031  LR: 0.00001398  \n",
      "Epoch: [3][100/366] Elapsed 0m 43s (remain 1m 52s) Loss: 0.0938(0.1075) Grad: 85815.3203  LR: 0.00001372  \n",
      "Epoch: [3][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0964(0.1087) Grad: 87816.2266  LR: 0.00001345  \n",
      "Epoch: [3][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.1098(0.1069) Grad: 184455.9062  LR: 0.00001319  \n",
      "Epoch: [3][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0852(0.1078) Grad: 86533.7578  LR: 0.00001291  \n",
      "Epoch: [3][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1200(0.1079) Grad: 84733.7578  LR: 0.00001264  \n",
      "Epoch: [3][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.1764(0.1086) Grad: 139624.9219  LR: 0.00001236  \n",
      "Epoch: [3][220/366] Elapsed 1m 34s (remain 1m 1s) Loss: 0.1327(0.1079) Grad: 175263.1875  LR: 0.00001208  \n",
      "Epoch: [3][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0728(0.1070) Grad: 131660.2031  LR: 0.00001180  \n",
      "Epoch: [3][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.0961(0.1061) Grad: 98105.1797  LR: 0.00001152  \n",
      "Epoch: [3][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.1152(0.1058) Grad: 169099.8438  LR: 0.00001124  \n",
      "Epoch: [3][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0763(0.1064) Grad: 147226.3438  LR: 0.00001096  \n",
      "Epoch: [3][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0692(0.1067) Grad: 79973.1094  LR: 0.00001067  \n",
      "Epoch: [3][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0741(0.1067) Grad: 84265.7734  LR: 0.00001039  \n",
      "Epoch: [3][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1018(0.1064) Grad: 89607.5859  LR: 0.00001010  \n",
      "Epoch: [3][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.1021(0.1063) Grad: 97741.1328  LR: 0.00001003  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 23s) Loss: 0.0928(0.0928) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1374(0.1206) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.1161(0.1197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1063  avg_val_loss: 0.1162  time: 193s\n",
      "Epoch 3 - Score: 0.4833  Scores: [0.5077111903836857, 0.4808837843225271, 0.43237412509096734, 0.46973826982174555, 0.5160938445675187, 0.49291712247680625]\n",
      "Epoch 3 - Save Best Score: 0.4833 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1047(0.1162) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0530(0.1162) \n",
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 6m 13s) Loss: 0.0824(0.0824) Grad: 64370.7656  LR: 0.00001001  \n",
      "Epoch: [4][20/366] Elapsed 0m 9s (remain 2m 33s) Loss: 0.0870(0.0982) Grad: 162947.3750  LR: 0.00000973  \n",
      "Epoch: [4][40/366] Elapsed 0m 17s (remain 2m 20s) Loss: 0.1147(0.0989) Grad: 101807.7891  LR: 0.00000944  \n",
      "Epoch: [4][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.1388(0.1006) Grad: 151723.1250  LR: 0.00000916  \n",
      "Epoch: [4][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.1049(0.0974) Grad: 140728.4062  LR: 0.00000887  \n",
      "Epoch: [4][100/366] Elapsed 0m 43s (remain 1m 52s) Loss: 0.0992(0.0977) Grad: 135730.9062  LR: 0.00000859  \n",
      "Epoch: [4][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0959(0.0973) Grad: 86048.0312  LR: 0.00000831  \n",
      "Epoch: [4][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.0759(0.0966) Grad: 102880.1172  LR: 0.00000803  \n",
      "Epoch: [4][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0885(0.0963) Grad: 98873.4062  LR: 0.00000775  \n",
      "Epoch: [4][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1283(0.0959) Grad: 113400.3438  LR: 0.00000747  \n",
      "Epoch: [4][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0766(0.0956) Grad: 88767.5781  LR: 0.00000720  \n",
      "Epoch: [4][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.1046(0.0960) Grad: 137496.4062  LR: 0.00000692  \n",
      "Epoch: [4][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.1168(0.0969) Grad: 87810.2891  LR: 0.00000665  \n",
      "Epoch: [4][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1080(0.0972) Grad: 123364.1562  LR: 0.00000639  \n",
      "Epoch: [4][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0964(0.0980) Grad: 79485.9219  LR: 0.00000612  \n",
      "Epoch: [4][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0587(0.0973) Grad: 64439.6875  LR: 0.00000586  \n",
      "Epoch: [4][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0645(0.0967) Grad: 67284.3594  LR: 0.00000560  \n",
      "Epoch: [4][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0561(0.0971) Grad: 113014.2188  LR: 0.00000535  \n",
      "Epoch: [4][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1396(0.0970) Grad: 119965.9375  LR: 0.00000510  \n",
      "Epoch: [4][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0997(0.0969) Grad: 87801.2500  LR: 0.00000503  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 29s) Loss: 0.0988(0.0988) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1228(0.1209) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.1226(0.1174) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0969  avg_val_loss: 0.1149  time: 193s\n",
      "Epoch 4 - Score: 0.4805  Scores: [0.5101672027163113, 0.46807407832733156, 0.42931964366346775, 0.4770526219286468, 0.516133950773095, 0.48228153602629226]\n",
      "Epoch 4 - Save Best Score: 0.4805 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1024(0.1149) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0766(0.1149) \n",
      "Epoch: [5][0/366] Elapsed 0m 0s (remain 6m 2s) Loss: 0.0814(0.0814) Grad: 100044.3281  LR: 0.00000502  \n",
      "Epoch: [5][20/366] Elapsed 0m 9s (remain 2m 31s) Loss: 0.0705(0.0867) Grad: 119602.0391  LR: 0.00000478  \n",
      "Epoch: [5][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.0948(0.0903) Grad: 127318.0781  LR: 0.00000453  \n",
      "Epoch: [5][60/366] Elapsed 0m 25s (remain 2m 9s) Loss: 0.0862(0.0909) Grad: 83583.2109  LR: 0.00000430  \n",
      "Epoch: [5][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.0759(0.0918) Grad: 81041.1953  LR: 0.00000406  \n",
      "Epoch: [5][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.1400(0.0912) Grad: 221052.2344  LR: 0.00000384  \n",
      "Epoch: [5][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0904(0.0899) Grad: 131174.1406  LR: 0.00000361  \n",
      "Epoch: [5][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.1258(0.0912) Grad: 190405.5938  LR: 0.00000340  \n",
      "Epoch: [5][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0848(0.0909) Grad: 101102.2109  LR: 0.00000319  \n",
      "Epoch: [5][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1580(0.0911) Grad: 120212.5234  LR: 0.00000298  \n",
      "Epoch: [5][200/366] Elapsed 1m 25s (remain 1m 9s) Loss: 0.0889(0.0902) Grad: 104438.2969  LR: 0.00000278  \n",
      "Epoch: [5][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0853(0.0900) Grad: 187867.5469  LR: 0.00000258  \n",
      "Epoch: [5][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0707(0.0890) Grad: 98985.5000  LR: 0.00000240  \n",
      "Epoch: [5][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.0850(0.0894) Grad: 134552.0469  LR: 0.00000221  \n",
      "Epoch: [5][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0742(0.0888) Grad: 102004.5312  LR: 0.00000204  \n",
      "Epoch: [5][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0962(0.0884) Grad: 164328.4375  LR: 0.00000187  \n",
      "Epoch: [5][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1170(0.0888) Grad: 97041.6484  LR: 0.00000171  \n",
      "Epoch: [5][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0938(0.0887) Grad: 127091.2422  LR: 0.00000155  \n",
      "Epoch: [5][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0697(0.0888) Grad: 107308.0469  LR: 0.00000140  \n",
      "Epoch: [5][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0746(0.0887) Grad: 226058.1719  LR: 0.00000136  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 10s) Loss: 0.0992(0.0992) \n",
      "EVAL: [20/62] Elapsed 0m 12s (remain 0m 25s) Loss: 0.1182(0.1178) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1211(0.1156) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0887  avg_val_loss: 0.1135  time: 192s\n",
      "Epoch 5 - Score: 0.4776  Scores: [0.5069098319595355, 0.46546587155635644, 0.42802009815051506, 0.4751810075704612, 0.5118904570972813, 0.4779775583867825]\n",
      "Epoch 5 - Save Best Score: 0.4776 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1010(0.1136) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0655(0.1135) \n",
      "Epoch: [6][0/366] Elapsed 0m 1s (remain 6m 24s) Loss: 0.0726(0.0726) Grad: 76291.9219  LR: 0.00000136  \n",
      "Epoch: [6][20/366] Elapsed 0m 9s (remain 2m 31s) Loss: 0.0866(0.0730) Grad: 96895.4531  LR: 0.00000122  \n",
      "Epoch: [6][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.1077(0.0780) Grad: 68441.8516  LR: 0.00000108  \n",
      "Epoch: [6][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.0669(0.0828) Grad: 80642.2031  LR: 0.00000096  \n",
      "Epoch: [6][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.0612(0.0824) Grad: 73207.6641  LR: 0.00000084  \n",
      "Epoch: [6][100/366] Elapsed 0m 43s (remain 1m 52s) Loss: 0.0910(0.0826) Grad: 88186.7969  LR: 0.00000073  \n",
      "Epoch: [6][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0901(0.0833) Grad: 140429.7188  LR: 0.00000063  \n",
      "Epoch: [6][140/366] Elapsed 1m 0s (remain 1m 35s) Loss: 0.0846(0.0853) Grad: 97494.5859  LR: 0.00000053  \n",
      "Epoch: [6][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0642(0.0855) Grad: 80594.3125  LR: 0.00000044  \n",
      "Epoch: [6][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.0491(0.0845) Grad: 90500.1094  LR: 0.00000036  \n",
      "Epoch: [6][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0723(0.0849) Grad: 103055.8359  LR: 0.00000029  \n",
      "Epoch: [6][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0923(0.0852) Grad: 81378.2109  LR: 0.00000023  \n",
      "Epoch: [6][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0808(0.0848) Grad: 115719.0859  LR: 0.00000017  \n",
      "Epoch: [6][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1239(0.0851) Grad: 147051.3594  LR: 0.00000012  \n",
      "Epoch: [6][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0773(0.0850) Grad: 80018.1797  LR: 0.00000008  \n",
      "Epoch: [6][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0974(0.0844) Grad: 187497.5156  LR: 0.00000005  \n",
      "Epoch: [6][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0747(0.0843) Grad: 162739.5938  LR: 0.00000002  \n",
      "Epoch: [6][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0759(0.0843) Grad: 79534.3750  LR: 0.00000001  \n",
      "Epoch: [6][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0681(0.0843) Grad: 87400.3203  LR: 0.00000000  \n",
      "Epoch: [6][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0514(0.0843) Grad: 78033.2344  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 19s) Loss: 0.0989(0.0989) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1162(0.1169) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1198(0.1150) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0843  avg_val_loss: 0.1133  time: 192s\n",
      "Epoch 6 - Score: 0.4771  Scores: [0.5072377362200694, 0.465362830819333, 0.4274769319166135, 0.4736530646418325, 0.5112689137010643, 0.4775067517614279]\n",
      "Epoch 6 - Save Best Score: 0.4771 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0999(0.1134) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0595(0.1133) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score: 0.4771  Scores: [0.5072377362200694, 0.465362830819333, 0.4274769319166135, 0.4736530646418325, 0.5112689137010643, 0.4775067517614279]\n",
      "========== fold: 2 training ==========\n",
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/electra/base-discriminator/\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/electra/base-discriminator/ were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 0s (remain 5m 23s) Loss: 2.8275(2.8275) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 9s (remain 2m 29s) Loss: 1.1101(1.9579) Grad: 237778.4062  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.3023(1.3214) Grad: 127808.6172  LR: 0.00001998  \n",
      "Epoch: [1][60/366] Elapsed 0m 25s (remain 2m 9s) Loss: 0.2863(0.9551) Grad: 170327.0469  LR: 0.00001996  \n",
      "Epoch: [1][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.2460(0.7652) Grad: 58871.4414  LR: 0.00001993  \n",
      "Epoch: [1][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.1310(0.6444) Grad: 57058.1211  LR: 0.00001990  \n",
      "Epoch: [1][120/366] Elapsed 0m 51s (remain 1m 43s) Loss: 0.1311(0.5616) Grad: 72111.8203  LR: 0.00001985  \n",
      "Epoch: [1][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.2043(0.5045) Grad: 80681.4844  LR: 0.00001980  \n",
      "Epoch: [1][160/366] Elapsed 1m 8s (remain 1m 26s) Loss: 0.1223(0.4579) Grad: 91646.1562  LR: 0.00001974  \n",
      "Epoch: [1][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1027(0.4203) Grad: 98039.9297  LR: 0.00001967  \n",
      "Epoch: [1][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.1307(0.3913) Grad: 108517.0703  LR: 0.00001959  \n",
      "Epoch: [1][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.1486(0.3720) Grad: 124805.4609  LR: 0.00001951  \n",
      "Epoch: [1][240/366] Elapsed 1m 42s (remain 0m 52s) Loss: 0.1473(0.3536) Grad: 130373.7500  LR: 0.00001941  \n",
      "Epoch: [1][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1580(0.3373) Grad: 62888.6680  LR: 0.00001931  \n",
      "Epoch: [1][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.1031(0.3228) Grad: 50566.0820  LR: 0.00001920  \n",
      "Epoch: [1][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.1147(0.3094) Grad: 38562.0977  LR: 0.00001909  \n",
      "Epoch: [1][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1191(0.2984) Grad: 101311.7266  LR: 0.00001897  \n",
      "Epoch: [1][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.2014(0.2894) Grad: 178824.8125  LR: 0.00001884  \n",
      "Epoch: [1][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0944(0.2805) Grad: 32452.1367  LR: 0.00001870  \n",
      "Epoch: [1][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.1355(0.2785) Grad: 68281.8984  LR: 0.00001866  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 27s) Loss: 0.1303(0.1303) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.1003(0.1322) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1177(0.1291) \n",
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0918(0.1280) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.2982(0.1284) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2785  avg_val_loss: 0.1284  time: 192s\n",
      "Epoch 1 - Score: 0.5093  Scores: [0.5374291445727659, 0.4974767577497383, 0.4753591512416707, 0.500700914167811, 0.5427860936741334, 0.5023049954636872]\n",
      "Epoch 1 - Save Best Score: 0.5093 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 7m 49s) Loss: 0.1047(0.1047) Grad: 71336.9375  LR: 0.00001866  \n",
      "Epoch: [2][20/366] Elapsed 0m 9s (remain 2m 39s) Loss: 0.0868(0.1139) Grad: 89516.1875  LR: 0.00001851  \n",
      "Epoch: [2][40/366] Elapsed 0m 18s (remain 2m 23s) Loss: 0.1244(0.1123) Grad: 93380.1719  LR: 0.00001836  \n",
      "Epoch: [2][60/366] Elapsed 0m 26s (remain 2m 12s) Loss: 0.1033(0.1153) Grad: 72581.9375  LR: 0.00001820  \n",
      "Epoch: [2][80/366] Elapsed 0m 35s (remain 2m 3s) Loss: 0.1902(0.1156) Grad: 204019.1406  LR: 0.00001803  \n",
      "Epoch: [2][100/366] Elapsed 0m 43s (remain 1m 54s) Loss: 0.1331(0.1161) Grad: 105417.7109  LR: 0.00001786  \n",
      "Epoch: [2][120/366] Elapsed 0m 51s (remain 1m 45s) Loss: 0.1040(0.1157) Grad: 157714.6094  LR: 0.00001768  \n",
      "Epoch: [2][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.1028(0.1148) Grad: 89830.7031  LR: 0.00001749  \n",
      "Epoch: [2][160/366] Elapsed 1m 9s (remain 1m 28s) Loss: 0.0668(0.1149) Grad: 102382.5781  LR: 0.00001730  \n",
      "Epoch: [2][180/366] Elapsed 1m 17s (remain 1m 19s) Loss: 0.1201(0.1173) Grad: 249312.5938  LR: 0.00001710  \n",
      "Epoch: [2][200/366] Elapsed 1m 26s (remain 1m 10s) Loss: 0.0733(0.1176) Grad: 90490.9219  LR: 0.00001689  \n",
      "Epoch: [2][220/366] Elapsed 1m 34s (remain 1m 2s) Loss: 0.1903(0.1168) Grad: 77431.8438  LR: 0.00001668  \n",
      "Epoch: [2][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.1196(0.1162) Grad: 113629.9297  LR: 0.00001647  \n",
      "Epoch: [2][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.1094(0.1153) Grad: 98743.6094  LR: 0.00001625  \n",
      "Epoch: [2][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.1287(0.1152) Grad: 155123.4062  LR: 0.00001602  \n",
      "Epoch: [2][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.0910(0.1148) Grad: 98752.5469  LR: 0.00001579  \n",
      "Epoch: [2][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1231(0.1148) Grad: 213353.5625  LR: 0.00001556  \n",
      "Epoch: [2][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.1198(0.1152) Grad: 164488.7656  LR: 0.00001532  \n",
      "Epoch: [2][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1445(0.1158) Grad: 128955.6328  LR: 0.00001507  \n",
      "Epoch: [2][365/366] Elapsed 2m 36s (remain 0m 0s) Loss: 0.1208(0.1156) Grad: 117505.4922  LR: 0.00001501  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 28s) Loss: 0.1342(0.1342) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0889(0.1264) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1071(0.1226) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1156  avg_val_loss: 0.1217  time: 193s\n",
      "Epoch 2 - Score: 0.4955  Scores: [0.51935457245552, 0.4759610539944864, 0.46348390296586117, 0.49603132001524436, 0.5117853211793795, 0.5064908829477626]\n",
      "Epoch 2 - Save Best Score: 0.4955 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0851(0.1215) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.2225(0.1217) \n",
      "Epoch: [3][0/366] Elapsed 0m 0s (remain 5m 50s) Loss: 0.1844(0.1844) Grad: 217940.5469  LR: 0.00001500  \n",
      "Epoch: [3][20/366] Elapsed 0m 9s (remain 2m 30s) Loss: 0.1268(0.1173) Grad: 210625.2812  LR: 0.00001475  \n",
      "Epoch: [3][40/366] Elapsed 0m 17s (remain 2m 18s) Loss: 0.0733(0.1089) Grad: 73929.2734  LR: 0.00001450  \n",
      "Epoch: [3][60/366] Elapsed 0m 25s (remain 2m 9s) Loss: 0.1454(0.1044) Grad: 227717.6719  LR: 0.00001424  \n",
      "Epoch: [3][80/366] Elapsed 0m 34s (remain 2m 0s) Loss: 0.0911(0.1053) Grad: 123730.7188  LR: 0.00001398  \n",
      "Epoch: [3][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.0676(0.1072) Grad: 79569.9844  LR: 0.00001372  \n",
      "Epoch: [3][120/366] Elapsed 0m 51s (remain 1m 43s) Loss: 0.0821(0.1071) Grad: 107484.6094  LR: 0.00001345  \n",
      "Epoch: [3][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.1251(0.1057) Grad: 293404.5312  LR: 0.00001318  \n",
      "Epoch: [3][160/366] Elapsed 1m 8s (remain 1m 26s) Loss: 0.1142(0.1054) Grad: 75031.0391  LR: 0.00001291  \n",
      "Epoch: [3][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1118(0.1047) Grad: 131328.3125  LR: 0.00001263  \n",
      "Epoch: [3][200/366] Elapsed 1m 25s (remain 1m 9s) Loss: 0.1408(0.1035) Grad: 180579.4375  LR: 0.00001236  \n",
      "Epoch: [3][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.1157(0.1039) Grad: 85215.9219  LR: 0.00001208  \n",
      "Epoch: [3][240/366] Elapsed 1m 42s (remain 0m 52s) Loss: 0.1327(0.1045) Grad: 215289.0312  LR: 0.00001180  \n",
      "Epoch: [3][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.0942(0.1049) Grad: 63357.5273  LR: 0.00001152  \n",
      "Epoch: [3][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.1072(0.1055) Grad: 71896.8828  LR: 0.00001123  \n",
      "Epoch: [3][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0727(0.1051) Grad: 76856.8359  LR: 0.00001095  \n",
      "Epoch: [3][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1099(0.1056) Grad: 173991.0625  LR: 0.00001066  \n",
      "Epoch: [3][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.1388(0.1049) Grad: 158935.8750  LR: 0.00001038  \n",
      "Epoch: [3][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0875(0.1046) Grad: 95760.5781  LR: 0.00001009  \n",
      "Epoch: [3][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.1369(0.1043) Grad: 131301.1406  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 23s) Loss: 0.1320(0.1320) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0781(0.1226) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.0970(0.1197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1043  avg_val_loss: 0.1185  time: 192s\n",
      "Epoch 3 - Score: 0.4887  Scores: [0.5206363814038106, 0.47270614486652557, 0.4469204895587388, 0.5015708648656858, 0.50117034368979, 0.48948658204014706]\n",
      "Epoch 3 - Save Best Score: 0.4887 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0894(0.1184) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1648(0.1185) \n",
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 6m 20s) Loss: 0.0584(0.0584) Grad: 91571.3359  LR: 0.00001001  \n",
      "Epoch: [4][20/366] Elapsed 0m 9s (remain 2m 32s) Loss: 0.0588(0.0969) Grad: 82768.7344  LR: 0.00000972  \n",
      "Epoch: [4][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.0914(0.0984) Grad: 143518.2500  LR: 0.00000944  \n",
      "Epoch: [4][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.0728(0.0963) Grad: 67333.7031  LR: 0.00000915  \n",
      "Epoch: [4][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.0849(0.0954) Grad: 88670.8125  LR: 0.00000887  \n",
      "Epoch: [4][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.0779(0.0953) Grad: 67258.1250  LR: 0.00000858  \n",
      "Epoch: [4][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0769(0.0937) Grad: 99216.6562  LR: 0.00000830  \n",
      "Epoch: [4][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.1467(0.0956) Grad: 316108.7500  LR: 0.00000802  \n",
      "Epoch: [4][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.1244(0.0949) Grad: 238894.3906  LR: 0.00000774  \n",
      "Epoch: [4][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.0576(0.0936) Grad: 102777.6250  LR: 0.00000746  \n",
      "Epoch: [4][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0656(0.0936) Grad: 146163.3906  LR: 0.00000719  \n",
      "Epoch: [4][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0969(0.0935) Grad: 101063.0547  LR: 0.00000692  \n",
      "Epoch: [4][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0868(0.0926) Grad: 139427.7812  LR: 0.00000664  \n",
      "Epoch: [4][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1402(0.0929) Grad: 210789.4219  LR: 0.00000638  \n",
      "Epoch: [4][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.1036(0.0934) Grad: 161360.7188  LR: 0.00000611  \n",
      "Epoch: [4][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0779(0.0931) Grad: 115246.5078  LR: 0.00000585  \n",
      "Epoch: [4][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0896(0.0929) Grad: 104300.5469  LR: 0.00000559  \n",
      "Epoch: [4][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0690(0.0926) Grad: 155839.5000  LR: 0.00000534  \n",
      "Epoch: [4][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1174(0.0927) Grad: 108925.6016  LR: 0.00000509  \n",
      "Epoch: [4][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.1637(0.0929) Grad: 238488.0312  LR: 0.00000502  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 29s) Loss: 0.1350(0.1350) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0942(0.1174) \n",
      "EVAL: [40/62] Elapsed 0m 25s (remain 0m 12s) Loss: 0.1014(0.1176) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0929  avg_val_loss: 0.1171  time: 192s\n",
      "Epoch 4 - Score: 0.4858  Scores: [0.5069004498407091, 0.473251467525136, 0.4424977653378701, 0.4837594063390147, 0.5158390329136917, 0.492734651866762]\n",
      "Epoch 4 - Save Best Score: 0.4858 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0915(0.1169) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.2107(0.1171) \n",
      "Epoch: [5][0/366] Elapsed 0m 1s (remain 6m 5s) Loss: 0.0598(0.0598) Grad: 118827.4375  LR: 0.00000501  \n",
      "Epoch: [5][20/366] Elapsed 0m 9s (remain 2m 31s) Loss: 0.1044(0.0760) Grad: 141265.3281  LR: 0.00000477  \n",
      "Epoch: [5][40/366] Elapsed 0m 17s (remain 2m 20s) Loss: 0.0906(0.0822) Grad: 136535.3125  LR: 0.00000453  \n",
      "Epoch: [5][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.0954(0.0838) Grad: 104137.6641  LR: 0.00000429  \n",
      "Epoch: [5][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.0900(0.0861) Grad: 227865.3594  LR: 0.00000406  \n",
      "Epoch: [5][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.0611(0.0863) Grad: 53470.4258  LR: 0.00000383  \n",
      "Epoch: [5][120/366] Elapsed 0m 51s (remain 1m 43s) Loss: 0.0707(0.0866) Grad: 117683.9453  LR: 0.00000361  \n",
      "Epoch: [5][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.1397(0.0866) Grad: 233851.1562  LR: 0.00000339  \n",
      "Epoch: [5][160/366] Elapsed 1m 8s (remain 1m 26s) Loss: 0.0635(0.0874) Grad: 217265.5000  LR: 0.00000318  \n",
      "Epoch: [5][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.0778(0.0871) Grad: 95706.8906  LR: 0.00000297  \n",
      "Epoch: [5][200/366] Elapsed 1m 25s (remain 1m 9s) Loss: 0.0773(0.0867) Grad: 123662.9688  LR: 0.00000277  \n",
      "Epoch: [5][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0702(0.0866) Grad: 84053.7734  LR: 0.00000258  \n",
      "Epoch: [5][240/366] Elapsed 1m 42s (remain 0m 52s) Loss: 0.1124(0.0865) Grad: 127784.7188  LR: 0.00000239  \n",
      "Epoch: [5][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1214(0.0864) Grad: 205418.9375  LR: 0.00000221  \n",
      "Epoch: [5][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0714(0.0856) Grad: 73680.8516  LR: 0.00000203  \n",
      "Epoch: [5][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0964(0.0849) Grad: 94554.3047  LR: 0.00000186  \n",
      "Epoch: [5][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1086(0.0846) Grad: 118818.1172  LR: 0.00000170  \n",
      "Epoch: [5][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.1060(0.0843) Grad: 165512.1719  LR: 0.00000154  \n",
      "Epoch: [5][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1212(0.0843) Grad: 115265.9531  LR: 0.00000139  \n",
      "Epoch: [5][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.1062(0.0844) Grad: 165293.0000  LR: 0.00000136  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 29s) Loss: 0.1330(0.1330) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0900(0.1194) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1060(0.1182) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0844  avg_val_loss: 0.1179  time: 192s\n",
      "Epoch 5 - Score: 0.4873  Scores: [0.5099748260073663, 0.4724910752750794, 0.442151038711251, 0.4885947435690077, 0.5241502830867781, 0.4866264827418444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0911(0.1177) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.2111(0.1179) \n",
      "Epoch: [6][0/366] Elapsed 0m 0s (remain 4m 39s) Loss: 0.0666(0.0666) Grad: 87782.2188  LR: 0.00000135  \n",
      "Epoch: [6][20/366] Elapsed 0m 9s (remain 2m 31s) Loss: 0.1144(0.0760) Grad: 216000.2812  LR: 0.00000121  \n",
      "Epoch: [6][40/366] Elapsed 0m 17s (remain 2m 20s) Loss: 0.0705(0.0766) Grad: 167404.5469  LR: 0.00000108  \n",
      "Epoch: [6][60/366] Elapsed 0m 26s (remain 2m 11s) Loss: 0.0849(0.0776) Grad: 115874.6719  LR: 0.00000095  \n",
      "Epoch: [6][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.0621(0.0791) Grad: 84129.0781  LR: 0.00000083  \n",
      "Epoch: [6][100/366] Elapsed 0m 43s (remain 1m 53s) Loss: 0.0763(0.0790) Grad: 91415.3594  LR: 0.00000072  \n",
      "Epoch: [6][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0650(0.0792) Grad: 85135.1953  LR: 0.00000062  \n",
      "Epoch: [6][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.0885(0.0798) Grad: 79652.1016  LR: 0.00000053  \n",
      "Epoch: [6][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0488(0.0789) Grad: 70338.4922  LR: 0.00000044  \n",
      "Epoch: [6][180/366] Elapsed 1m 17s (remain 1m 18s) Loss: 0.0806(0.0793) Grad: 183703.5625  LR: 0.00000036  \n",
      "Epoch: [6][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0524(0.0794) Grad: 80498.9531  LR: 0.00000029  \n",
      "Epoch: [6][220/366] Elapsed 1m 34s (remain 1m 1s) Loss: 0.0780(0.0795) Grad: 129411.1250  LR: 0.00000022  \n",
      "Epoch: [6][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0544(0.0793) Grad: 62080.4336  LR: 0.00000017  \n",
      "Epoch: [6][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1048(0.0795) Grad: 108535.1250  LR: 0.00000012  \n",
      "Epoch: [6][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0985(0.0793) Grad: 102571.9531  LR: 0.00000008  \n",
      "Epoch: [6][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.1030(0.0792) Grad: 179472.3438  LR: 0.00000005  \n",
      "Epoch: [6][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1011(0.0792) Grad: 174101.7344  LR: 0.00000002  \n",
      "Epoch: [6][340/366] Elapsed 2m 25s (remain 0m 10s) Loss: 0.0738(0.0794) Grad: 183686.7500  LR: 0.00000001  \n",
      "Epoch: [6][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0884(0.0797) Grad: 162368.2656  LR: 0.00000000  \n",
      "Epoch: [6][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0567(0.0797) Grad: 98398.1562  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 26s) Loss: 0.1312(0.1312) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0889(0.1188) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1030(0.1177) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0797  avg_val_loss: 0.1173  time: 193s\n",
      "Epoch 6 - Score: 0.4861  Scores: [0.5104621005798398, 0.4724375624183125, 0.44205791139084827, 0.4888516747537548, 0.5177205040815306, 0.4849458255958081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0889(0.1171) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.2018(0.1173) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 result ==========\n",
      "Score: 0.4858  Scores: [0.5069004498407091, 0.473251467525136, 0.4424977653378701, 0.4837594063390147, 0.5158390329136917, 0.492734651866762]\n",
      "========== fold: 3 training ==========\n",
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/electra/base-discriminator/\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/electra/base-discriminator/ were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 1s (remain 6m 28s) Loss: 2.6970(2.6970) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 9s (remain 2m 34s) Loss: 0.9367(1.8989) Grad: 219033.1875  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 0m 17s (remain 2m 20s) Loss: 0.3062(1.2062) Grad: 112261.1484  LR: 0.00001998  \n",
      "Epoch: [1][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.1904(0.8815) Grad: 249324.1875  LR: 0.00001996  \n",
      "Epoch: [1][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.1612(0.7061) Grad: 41312.7773  LR: 0.00001993  \n",
      "Epoch: [1][100/366] Elapsed 0m 43s (remain 1m 53s) Loss: 0.0786(0.6028) Grad: 42629.5508  LR: 0.00001990  \n",
      "Epoch: [1][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0755(0.5281) Grad: 44004.4570  LR: 0.00001985  \n",
      "Epoch: [1][140/366] Elapsed 1m 0s (remain 1m 36s) Loss: 0.1337(0.4787) Grad: 208159.3125  LR: 0.00001980  \n",
      "Epoch: [1][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.1590(0.4381) Grad: 115270.3359  LR: 0.00001974  \n",
      "Epoch: [1][180/366] Elapsed 1m 17s (remain 1m 18s) Loss: 0.1543(0.4047) Grad: 181831.9688  LR: 0.00001967  \n",
      "Epoch: [1][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.1843(0.3795) Grad: 246893.2500  LR: 0.00001959  \n",
      "Epoch: [1][220/366] Elapsed 1m 34s (remain 1m 1s) Loss: 0.1938(0.3578) Grad: 67603.5703  LR: 0.00001951  \n",
      "Epoch: [1][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.1155(0.3394) Grad: 64618.2383  LR: 0.00001941  \n",
      "Epoch: [1][260/366] Elapsed 1m 51s (remain 0m 44s) Loss: 0.1303(0.3254) Grad: 82218.3359  LR: 0.00001931  \n",
      "Epoch: [1][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.2296(0.3122) Grad: 217505.9062  LR: 0.00001920  \n",
      "Epoch: [1][300/366] Elapsed 2m 8s (remain 0m 27s) Loss: 0.1481(0.3004) Grad: 47125.6562  LR: 0.00001909  \n",
      "Epoch: [1][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0800(0.2900) Grad: 59695.2188  LR: 0.00001897  \n",
      "Epoch: [1][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.1394(0.2810) Grad: 99509.6406  LR: 0.00001884  \n",
      "Epoch: [1][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1706(0.2735) Grad: 108789.9219  LR: 0.00001870  \n",
      "Epoch: [1][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0785(0.2717) Grad: 53586.1602  LR: 0.00001866  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 20s) Loss: 0.1509(0.1509) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0902(0.1230) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1336(0.1264) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2717  avg_val_loss: 0.1243  time: 192s\n",
      "Epoch 1 - Score: 0.5008  Scores: [0.5138001094449572, 0.485708838744576, 0.4549262217587731, 0.47410439881354577, 0.5611156920176107, 0.515154992273808]\n",
      "Epoch 1 - Save Best Score: 0.5008 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1081(0.1244) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0587(0.1243) \n",
      "Epoch: [2][0/366] Elapsed 0m 0s (remain 6m 2s) Loss: 0.1754(0.1754) Grad: 101413.2891  LR: 0.00001866  \n",
      "Epoch: [2][20/366] Elapsed 0m 9s (remain 2m 33s) Loss: 0.0598(0.1185) Grad: 93211.9922  LR: 0.00001851  \n",
      "Epoch: [2][40/366] Elapsed 0m 17s (remain 2m 20s) Loss: 0.0831(0.1196) Grad: 94951.2969  LR: 0.00001836  \n",
      "Epoch: [2][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.0828(0.1235) Grad: 164330.1875  LR: 0.00001820  \n",
      "Epoch: [2][80/366] Elapsed 0m 34s (remain 2m 2s) Loss: 0.0962(0.1221) Grad: 198050.8438  LR: 0.00001803  \n",
      "Epoch: [2][100/366] Elapsed 0m 43s (remain 1m 53s) Loss: 0.1054(0.1200) Grad: 138952.7969  LR: 0.00001786  \n",
      "Epoch: [2][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.1282(0.1184) Grad: 139679.3281  LR: 0.00001768  \n",
      "Epoch: [2][140/366] Elapsed 1m 0s (remain 1m 35s) Loss: 0.0638(0.1175) Grad: 51419.0703  LR: 0.00001749  \n",
      "Epoch: [2][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0938(0.1177) Grad: 141987.3125  LR: 0.00001730  \n",
      "Epoch: [2][180/366] Elapsed 1m 17s (remain 1m 18s) Loss: 0.1646(0.1177) Grad: 96706.2344  LR: 0.00001710  \n",
      "Epoch: [2][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.1065(0.1171) Grad: 181566.2188  LR: 0.00001689  \n",
      "Epoch: [2][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0989(0.1170) Grad: 74675.9141  LR: 0.00001668  \n",
      "Epoch: [2][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0859(0.1159) Grad: 153552.3438  LR: 0.00001647  \n",
      "Epoch: [2][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1001(0.1163) Grad: 94523.0156  LR: 0.00001625  \n",
      "Epoch: [2][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.1078(0.1162) Grad: 61382.2617  LR: 0.00001602  \n",
      "Epoch: [2][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0901(0.1156) Grad: 82503.7266  LR: 0.00001579  \n",
      "Epoch: [2][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1506(0.1149) Grad: 89856.9922  LR: 0.00001556  \n",
      "Epoch: [2][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0965(0.1144) Grad: 132071.7344  LR: 0.00001532  \n",
      "Epoch: [2][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1206(0.1145) Grad: 212182.8438  LR: 0.00001507  \n",
      "Epoch: [2][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.1010(0.1142) Grad: 58487.8086  LR: 0.00001501  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 26s) Loss: 0.1117(0.1117) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0949(0.1099) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1236(0.1138) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1142  avg_val_loss: 0.1125  time: 192s\n",
      "Epoch 2 - Score: 0.4759  Scores: [0.5049261368045435, 0.4583911376755427, 0.45546415859719164, 0.46583432273328285, 0.4956168020438334, 0.47511228125414956]\n",
      "Epoch 2 - Save Best Score: 0.4759 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0930(0.1126) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0724(0.1125) \n",
      "Epoch: [3][0/366] Elapsed 0m 0s (remain 5m 32s) Loss: 0.1352(0.1352) Grad: 143896.0938  LR: 0.00001500  \n",
      "Epoch: [3][20/366] Elapsed 0m 9s (remain 2m 29s) Loss: 0.1087(0.1101) Grad: 245954.8594  LR: 0.00001475  \n",
      "Epoch: [3][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.0802(0.1079) Grad: 111667.3359  LR: 0.00001450  \n",
      "Epoch: [3][60/366] Elapsed 0m 25s (remain 2m 9s) Loss: 0.0688(0.1102) Grad: 61583.2734  LR: 0.00001424  \n",
      "Epoch: [3][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.1207(0.1087) Grad: 215440.1875  LR: 0.00001398  \n",
      "Epoch: [3][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.0609(0.1070) Grad: 120583.7031  LR: 0.00001372  \n",
      "Epoch: [3][120/366] Elapsed 0m 51s (remain 1m 43s) Loss: 0.1063(0.1064) Grad: 97830.5547  LR: 0.00001345  \n",
      "Epoch: [3][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.1253(0.1056) Grad: 82746.2734  LR: 0.00001318  \n",
      "Epoch: [3][160/366] Elapsed 1m 8s (remain 1m 26s) Loss: 0.1292(0.1044) Grad: 79226.1953  LR: 0.00001291  \n",
      "Epoch: [3][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1459(0.1056) Grad: 72194.7734  LR: 0.00001263  \n",
      "Epoch: [3][200/366] Elapsed 1m 25s (remain 1m 9s) Loss: 0.0689(0.1051) Grad: 101865.6641  LR: 0.00001236  \n",
      "Epoch: [3][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.1586(0.1050) Grad: 152197.0938  LR: 0.00001208  \n",
      "Epoch: [3][240/366] Elapsed 1m 42s (remain 0m 52s) Loss: 0.0809(0.1048) Grad: 190116.0000  LR: 0.00001180  \n",
      "Epoch: [3][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.1050(0.1047) Grad: 155790.3750  LR: 0.00001152  \n",
      "Epoch: [3][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0820(0.1045) Grad: 110081.3594  LR: 0.00001123  \n",
      "Epoch: [3][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.1025(0.1039) Grad: 118622.3125  LR: 0.00001095  \n",
      "Epoch: [3][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0640(0.1032) Grad: 88338.7578  LR: 0.00001066  \n",
      "Epoch: [3][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0973(0.1027) Grad: 157182.6719  LR: 0.00001038  \n",
      "Epoch: [3][360/366] Elapsed 2m 32s (remain 0m 2s) Loss: 0.1350(0.1031) Grad: 140014.8594  LR: 0.00001009  \n",
      "Epoch: [3][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0806(0.1030) Grad: 95876.1094  LR: 0.00001002  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 22s) Loss: 0.1056(0.1056) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0910(0.1077) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1249(0.1106) \n",
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0898(0.1091) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0741(0.1090) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.1030  avg_val_loss: 0.1090  time: 192s\n",
      "Epoch 3 - Score: 0.4681  Scores: [0.5020400606775144, 0.4549934137533291, 0.4305292100424088, 0.46376448943144644, 0.49203331900545233, 0.4653704583695002]\n",
      "Epoch 3 - Save Best Score: 0.4681 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/366] Elapsed 0m 0s (remain 6m 1s) Loss: 0.0927(0.0927) Grad: 131178.9688  LR: 0.00001001  \n",
      "Epoch: [4][20/366] Elapsed 0m 9s (remain 2m 31s) Loss: 0.0552(0.0955) Grad: 81150.7031  LR: 0.00000972  \n",
      "Epoch: [4][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.1498(0.0962) Grad: 233155.9688  LR: 0.00000944  \n",
      "Epoch: [4][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.0707(0.0942) Grad: 105201.3750  LR: 0.00000915  \n",
      "Epoch: [4][80/366] Elapsed 0m 34s (remain 2m 0s) Loss: 0.1224(0.0934) Grad: 150666.5469  LR: 0.00000887  \n",
      "Epoch: [4][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.0913(0.0921) Grad: 82333.0703  LR: 0.00000858  \n",
      "Epoch: [4][120/366] Elapsed 0m 51s (remain 1m 43s) Loss: 0.0616(0.0908) Grad: 171654.8594  LR: 0.00000830  \n",
      "Epoch: [4][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.0892(0.0908) Grad: 196610.5625  LR: 0.00000802  \n",
      "Epoch: [4][160/366] Elapsed 1m 8s (remain 1m 26s) Loss: 0.0939(0.0914) Grad: 67323.1797  LR: 0.00000774  \n",
      "Epoch: [4][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.1003(0.0918) Grad: 181684.4531  LR: 0.00000746  \n",
      "Epoch: [4][200/366] Elapsed 1m 25s (remain 1m 9s) Loss: 0.0773(0.0910) Grad: 93791.7188  LR: 0.00000719  \n",
      "Epoch: [4][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0447(0.0919) Grad: 53608.0430  LR: 0.00000692  \n",
      "Epoch: [4][240/366] Elapsed 1m 41s (remain 0m 52s) Loss: 0.0757(0.0914) Grad: 115550.5781  LR: 0.00000664  \n",
      "Epoch: [4][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.0760(0.0906) Grad: 119772.9531  LR: 0.00000638  \n",
      "Epoch: [4][280/366] Elapsed 1m 59s (remain 0m 35s) Loss: 0.0651(0.0898) Grad: 120897.3984  LR: 0.00000611  \n",
      "Epoch: [4][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0865(0.0897) Grad: 129156.4922  LR: 0.00000585  \n",
      "Epoch: [4][320/366] Elapsed 2m 15s (remain 0m 19s) Loss: 0.1189(0.0898) Grad: 92024.5234  LR: 0.00000559  \n",
      "Epoch: [4][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0813(0.0895) Grad: 87955.3750  LR: 0.00000534  \n",
      "Epoch: [4][360/366] Elapsed 2m 32s (remain 0m 2s) Loss: 0.0654(0.0899) Grad: 132445.6406  LR: 0.00000509  \n",
      "Epoch: [4][365/366] Elapsed 2m 34s (remain 0m 0s) Loss: 0.0865(0.0899) Grad: 142896.1562  LR: 0.00000502  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 22s) Loss: 0.1059(0.1059) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0861(0.1085) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1272(0.1101) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0899  avg_val_loss: 0.1089  time: 192s\n",
      "Epoch 4 - Score: 0.4677  Scores: [0.5013072727685947, 0.45494449455776914, 0.43517049963297827, 0.4528440612502409, 0.4985108914899993, 0.4634419606623158]\n",
      "Epoch 4 - Save Best Score: 0.4677 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0894(0.1089) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0836(0.1089) \n",
      "Epoch: [5][0/366] Elapsed 0m 1s (remain 6m 13s) Loss: 0.0965(0.0965) Grad: 82326.8906  LR: 0.00000501  \n",
      "Epoch: [5][20/366] Elapsed 0m 9s (remain 2m 31s) Loss: 0.0626(0.0824) Grad: 100823.3594  LR: 0.00000477  \n",
      "Epoch: [5][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.1006(0.0830) Grad: 75957.9609  LR: 0.00000453  \n",
      "Epoch: [5][60/366] Elapsed 0m 25s (remain 2m 9s) Loss: 0.0713(0.0832) Grad: 110659.8281  LR: 0.00000429  \n",
      "Epoch: [5][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.1282(0.0829) Grad: 175232.9062  LR: 0.00000406  \n",
      "Epoch: [5][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.0733(0.0812) Grad: 116208.6406  LR: 0.00000383  \n",
      "Epoch: [5][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0871(0.0805) Grad: 95489.2578  LR: 0.00000361  \n",
      "Epoch: [5][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.0885(0.0801) Grad: 100792.6094  LR: 0.00000339  \n",
      "Epoch: [5][160/366] Elapsed 1m 8s (remain 1m 27s) Loss: 0.0607(0.0798) Grad: 100298.5078  LR: 0.00000318  \n",
      "Epoch: [5][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.0661(0.0798) Grad: 84827.0312  LR: 0.00000297  \n",
      "Epoch: [5][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0804(0.0800) Grad: 108602.8125  LR: 0.00000277  \n",
      "Epoch: [5][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0895(0.0798) Grad: 150529.2031  LR: 0.00000258  \n",
      "Epoch: [5][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0845(0.0798) Grad: 82898.5625  LR: 0.00000239  \n",
      "Epoch: [5][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.0705(0.0795) Grad: 145475.7031  LR: 0.00000221  \n",
      "Epoch: [5][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0776(0.0798) Grad: 140025.2812  LR: 0.00000203  \n",
      "Epoch: [5][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0817(0.0800) Grad: 147289.2031  LR: 0.00000186  \n",
      "Epoch: [5][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.1064(0.0802) Grad: 69992.5078  LR: 0.00000170  \n",
      "Epoch: [5][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0847(0.0801) Grad: 200619.5781  LR: 0.00000154  \n",
      "Epoch: [5][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.0823(0.0801) Grad: 110453.6016  LR: 0.00000139  \n",
      "Epoch: [5][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0976(0.0802) Grad: 177594.9844  LR: 0.00000136  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 27s) Loss: 0.1044(0.1044) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0795(0.1078) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1332(0.1099) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0802  avg_val_loss: 0.1082  time: 192s\n",
      "Epoch 5 - Score: 0.4666  Scores: [0.5021403096441055, 0.4564748935810538, 0.4333362739912066, 0.4550782460269854, 0.491249983027315, 0.461181051804876]\n",
      "Epoch 5 - Save Best Score: 0.4666 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0891(0.1083) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0717(0.1082) \n",
      "Epoch: [6][0/366] Elapsed 0m 1s (remain 6m 7s) Loss: 0.0540(0.0540) Grad: 75234.7188  LR: 0.00000135  \n",
      "Epoch: [6][20/366] Elapsed 0m 9s (remain 2m 30s) Loss: 0.0498(0.0663) Grad: 83420.5938  LR: 0.00000121  \n",
      "Epoch: [6][40/366] Elapsed 0m 17s (remain 2m 19s) Loss: 0.0738(0.0715) Grad: 97280.8438  LR: 0.00000108  \n",
      "Epoch: [6][60/366] Elapsed 0m 26s (remain 2m 10s) Loss: 0.0557(0.0723) Grad: 138307.8438  LR: 0.00000095  \n",
      "Epoch: [6][80/366] Elapsed 0m 34s (remain 2m 1s) Loss: 0.1092(0.0731) Grad: 199476.2031  LR: 0.00000083  \n",
      "Epoch: [6][100/366] Elapsed 0m 42s (remain 1m 52s) Loss: 0.1305(0.0748) Grad: 101161.5391  LR: 0.00000072  \n",
      "Epoch: [6][120/366] Elapsed 0m 51s (remain 1m 44s) Loss: 0.0773(0.0749) Grad: 67015.6641  LR: 0.00000062  \n",
      "Epoch: [6][140/366] Elapsed 0m 59s (remain 1m 35s) Loss: 0.0718(0.0744) Grad: 243803.9375  LR: 0.00000053  \n",
      "Epoch: [6][160/366] Elapsed 1m 8s (remain 1m 26s) Loss: 0.1022(0.0757) Grad: 102775.7578  LR: 0.00000044  \n",
      "Epoch: [6][180/366] Elapsed 1m 16s (remain 1m 18s) Loss: 0.0503(0.0754) Grad: 92422.5781  LR: 0.00000036  \n",
      "Epoch: [6][200/366] Elapsed 1m 25s (remain 1m 10s) Loss: 0.0707(0.0757) Grad: 102323.6016  LR: 0.00000029  \n",
      "Epoch: [6][220/366] Elapsed 1m 33s (remain 1m 1s) Loss: 0.0668(0.0755) Grad: 84965.9531  LR: 0.00000022  \n",
      "Epoch: [6][240/366] Elapsed 1m 42s (remain 0m 53s) Loss: 0.0749(0.0759) Grad: 87710.9219  LR: 0.00000017  \n",
      "Epoch: [6][260/366] Elapsed 1m 50s (remain 0m 44s) Loss: 0.0369(0.0761) Grad: 59527.4062  LR: 0.00000012  \n",
      "Epoch: [6][280/366] Elapsed 1m 59s (remain 0m 36s) Loss: 0.0710(0.0755) Grad: 203664.8594  LR: 0.00000008  \n",
      "Epoch: [6][300/366] Elapsed 2m 7s (remain 0m 27s) Loss: 0.0471(0.0749) Grad: 144686.5156  LR: 0.00000005  \n",
      "Epoch: [6][320/366] Elapsed 2m 16s (remain 0m 19s) Loss: 0.0801(0.0750) Grad: 88202.0391  LR: 0.00000002  \n",
      "Epoch: [6][340/366] Elapsed 2m 24s (remain 0m 10s) Loss: 0.0864(0.0749) Grad: 84502.7578  LR: 0.00000001  \n",
      "Epoch: [6][360/366] Elapsed 2m 33s (remain 0m 2s) Loss: 0.1058(0.0749) Grad: 136553.6875  LR: 0.00000000  \n",
      "Epoch: [6][365/366] Elapsed 2m 35s (remain 0m 0s) Loss: 0.0871(0.0748) Grad: 166070.7812  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 1s (remain 1m 23s) Loss: 0.1065(0.1065) \n",
      "EVAL: [20/62] Elapsed 0m 13s (remain 0m 25s) Loss: 0.0803(0.1080) \n",
      "EVAL: [40/62] Elapsed 0m 24s (remain 0m 12s) Loss: 0.1327(0.1097) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0748  avg_val_loss: 0.1080  time: 192s\n",
      "Epoch 6 - Score: 0.4661  Scores: [0.5020580432203838, 0.45653098309285717, 0.4317305247153159, 0.4545991737551393, 0.48999720802643887, 0.4615269401499844]\n",
      "Epoch 6 - Save Best Score: 0.4661 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0873(0.1081) \n",
      "EVAL: [61/62] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0801(0.1080) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score: 0.4661  Scores: [0.5020580432203838, 0.45653098309285717, 0.4317305247153159, 0.4545991737551393, 0.48999720802643887, 0.4615269401499844]\n",
      "========== CV ==========\n",
      "Score: 0.4741  Scores: [0.5044130520221506, 0.46270701619075333, 0.43258794601400796, 0.4695460026731505, 0.5009179834654026, 0.4742955108320805]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▂▂▂▁▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>▆█▁▅▂▂</td></tr><tr><td>[fold0] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold0] loss</td><td>█▂▂▃▃▃▃▂▂▂▂▃▂▂▁▁▂▂▂▂▁▂▁▂▁▁▁▁▂▂▂▂▁▁▂▁▁▂▁▁</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>▆█▁▅▃▂</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▂▂▁▁▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>█▂▂▁▁▁</td></tr><tr><td>[fold1] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold1] loss</td><td>█▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] score</td><td>█▃▂▁▁▁</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▂▂▁▁▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>█▄▂▁▁▁</td></tr><tr><td>[fold2] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold2] loss</td><td>█▃▂▂▂▂▂▃▂▂▃▂▁▂▂▂▂▂▂▁▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂</td></tr><tr><td>[fold2] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] score</td><td>█▄▂▁▁▁</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▂▂▂▁▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>█▃▁▁▁▁</td></tr><tr><td>[fold3] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold3] loss</td><td>█▄▃▂▂▃▂▂▂▁▂▂▂▂▂▁▂▁▁▁▂▂▂▁▂▁▁▂▁▂▁▂▂▂▁▁▁▂▂▂</td></tr><tr><td>[fold3] lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] score</td><td>█▃▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.07234</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.11066</td></tr><tr><td>[fold0] epoch</td><td>6</td></tr><tr><td>[fold0] loss</td><td>0.05002</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.47117</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.08429</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.11331</td></tr><tr><td>[fold1] epoch</td><td>6</td></tr><tr><td>[fold1] loss</td><td>0.05137</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] score</td><td>0.47708</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.07965</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.11728</td></tr><tr><td>[fold2] epoch</td><td>6</td></tr><tr><td>[fold2] loss</td><td>0.05668</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] score</td><td>0.48608</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.07482</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.10803</td></tr><tr><td>[fold3] epoch</td><td>6</td></tr><tr><td>[fold3] loss</td><td>0.08713</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] score</td><td>0.46607</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">002_202210161155</strong>: <a href=\"https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning/runs/8d4syu96\" target=\"_blank\">https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning/runs/8d4syu96</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221016_025544-8d4syu96/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "        return score\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                score = get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        score = round(get_result(oof_df),3)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+f'oof_df.pkl')\n",
    "        \n",
    "    # if CFG.DEBUG:\n",
    "    #     import send2trash\n",
    "    #     send2trash.send2trash(OUTPUT_DIR)\n",
    "    CFG.OUTPUT_DIR = OUTPUT_DIR\n",
    "    dict_cfg = {k: vars(CFG)[k] for k in vars(CFG) if \"__\" not in k}\n",
    "    with open(OUTPUT_DIR+\"dict_cfg\", 'wb') as web:\n",
    "        pickle.dump(dict_cfg , web)\n",
    "    with open(OUTPUT_DIR+\"class_cfg\", 'wb') as web:\n",
    "        pickle.dump(CFG , web)\n",
    "    \n",
    "    \n",
    "    if CFG.wandb:\n",
    "        wandb.config.update(class2dict(CFG))\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.DL:\n",
    "    import shutil\n",
    "    print(shutil.make_archive(OUTPUT_DIR, 'zip', root_dir=OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #OUTPUT_DIR = \"/home/jupyter/output/ex/electra/base-discriminator/001/202210151522/\"\n",
    "# PREDICT_DIR = OUTPUT_DIR.replace(\"output\",\"predict\")\n",
    "# if not os.path.exists(PREDICT_DIR):\n",
    "#     os.makedirs(PREDICT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/output/ex/electra/base-discriminator/002/202210161155/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '/home/jupyter/output/ex/electra/base-discriminator/002/202210161155/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file base-discriminator_fold0_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 416M/416M [00:12<00:00, 34.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: base-discriminator_fold0_best.pth (416MB)\n",
      "Starting upload for file base-discriminator_fold3_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 416M/416M [00:10<00:00, 41.2MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: base-discriminator_fold3_best.pth (416MB)\n",
      "Starting upload for file config.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2.48k/2.48k [00:02<00:00, 954B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.pth (2KB)\n",
      "Starting upload for file base-discriminator_fold1_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 416M/416M [00:12<00:00, 34.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: base-discriminator_fold1_best.pth (416MB)\n",
      "Starting upload for file class_cfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 19.0/19.0 [00:02<00:00, 7.40B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: class_cfg (19B)\n",
      "Starting upload for file oof_df.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 9.09M/9.09M [00:02<00:00, 4.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: oof_df.pkl (9MB)\n",
      "Starting upload for file train.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10.4k/10.4k [00:02<00:00, 5.12kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train.log (10KB)\n",
      "Starting upload for file base-discriminator_fold2_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 416M/416M [00:11<00:00, 38.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: base-discriminator_fold2_best.pth (416MB)\n",
      "Starting upload for file dict_cfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 824k/824k [00:01<00:00, 515kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: dict_cfg (824KB)\n"
     ]
    }
   ],
   "source": [
    "if CFG.TO_KAGGLE:\n",
    "    UPLOAD_DIR = OUTPUT_DIR\n",
    "    EX_NO = f\"{CFG.name}-{CFG.model}-{CFG.file_name}\" # 実験番号などを入れる、folderのpathにする\n",
    "    USERID = 'your_id'\n",
    "\n",
    "\n",
    "    def dataset_upload():\n",
    "        import json\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "        id = f'{USERID}/{EX_NO}'\n",
    "\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata['id'] = id\n",
    "        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "        dataset_metadata['title'] = f'{EX_NO}'\n",
    "\n",
    "        with open(UPLOAD_DIR +'dataset-metadata.json', 'w') as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "\n",
    "        # データセットがない場合\n",
    "        if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n",
    "            api.dataset_create_new(folder=UPLOAD_DIR,\n",
    "                                   convert_to_csv=False,\n",
    "                                   dir_mode='skip')\n",
    "        # データセットがある場合\n",
    "        else:\n",
    "            api.dataset_create_version(folder=UPLOAD_DIR,\n",
    "                                       version_notes='update',\n",
    "                                       convert_to_csv=False,\n",
    "                                       delete_old_versions=True,\n",
    "                                       dir_mode='skip')\n",
    "    dataset_upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
