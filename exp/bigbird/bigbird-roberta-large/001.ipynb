{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.015868,
     "end_time": "2022-08-31T07:01:57.417077",
     "exception": false,
     "start_time": "2022-08-31T07:01:57.401209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "import numpy as np\n",
    "class CFG:\n",
    "    wandb = True\n",
    "    DEBUG = False\n",
    "    TO_KAGGLE = True\n",
    "    score_path = \"gs://feedback3/output/scores/scores3.csv\"\n",
    "    MEMO = \"ベースライン\"\n",
    "    file_name = \"001\"\n",
    "    model=\"bigbird/roberta-large\"\n",
    "    patience = 3\n",
    "    n_fold=4\n",
    "    trn_fold=[0,1,2,3]\n",
    "    model_config_path = f\"/home/jupyter/models/{model}/\"\n",
    "    model_bin_path = f\"/home/jupyter/models/{model}/\"\n",
    "    competition='FB3'\n",
    "    apex=True\n",
    "    print_freq=20\n",
    "    num_workers=4\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=10\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=8\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    train=True\n",
    "    \n",
    "if CFG.DEBUG:\n",
    "    CFG.epochs = 2\n",
    "    #CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhiroki8383\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/exp/bigbird/bigbird-roberta-large/wandb/run-20221105_053829-1dzvzwzv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning/runs/1dzvzwzv\" target=\"_blank\">001_202211051438</a></strong> to <a href=\"https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# ====================================================\n",
    "# datetime\n",
    "# ====================================================\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)\n",
    "date = now.strftime('%Y%m%d')\n",
    "date2 = now.strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# file_path\n",
    "# ====================================================\n",
    "if \"/\" in CFG.model:\n",
    "    model_name = CFG.model.replace(\"/\",\"-\")\n",
    "else:\n",
    "    model_name = CFG.model\n",
    "\n",
    "path =\"/home/jupyter/feedback-prize-english-language-learning/\"\n",
    "if CFG.DEBUG:\n",
    "    OUTPUT_DIR = f'/home/jupyter/output/ex/DEBUG/{model_name}/{CFG.file_name}/{date2}/'\n",
    "else:\n",
    "    OUTPUT_DIR = f'/home/jupyter/output/ex/{model_name}/{CFG.file_name}/{date2}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# wandb \n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    import wandb\n",
    "    \n",
    "    def class2dict(f):\n",
    "            return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "        \n",
    "    project='Feedback Prize - English Language Learning'\n",
    "    if CFG.DEBUG:\n",
    "        group = \"DEBUG\"\n",
    "    else:\n",
    "        group = model_name\n",
    "    wandb_name = f\"{CFG.file_name}_{date2}\"\n",
    "    job_type = CFG.file_name  #\"train\"\n",
    "\n",
    "    wandb_api = \"your_id\"\n",
    "    wandb.login(key=wandb_api)\n",
    "    anony = None\n",
    "    run = wandb.init(project=project, \n",
    "                         name = wandb_name,\n",
    "                         config=class2dict(CFG),\n",
    "                         group=group,\n",
    "                         job_type=job_type,\n",
    "                         anonymous=anony)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008876,
     "end_time": "2022-08-31T07:02:06.467728",
     "exception": false,
     "start_time": "2022-08-31T07:02:06.458852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip download transformers==4.21.2\n",
    "# !pip download tokenizers==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 57.587416,
     "end_time": "2022-08-31T07:03:04.064247",
     "exception": false,
     "start_time": "2022-08-31T07:02:06.476831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iterative-stratification==0.1.7 in /opt/conda/lib/python3.7/site-packages (0.1.7)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.21.6)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.7) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.7) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.21.2\n",
      "Uninstalling transformers-4.21.2:\n",
      "  Successfully uninstalled transformers-4.21.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tokenizers 0.12.1\n",
      "Uninstalling tokenizers-0.12.1:\n",
      "  Successfully uninstalled tokenizers-0.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /home/jupyter/code_baseline/FB3_pip_wheels\n",
      "Processing /home/jupyter/code_baseline/FB3_pip_wheels/transformers-4.21.2-py3-none-any.whl\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Processing /home/jupyter/code_baseline/FB3_pip_wheels/tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: tokenizers, transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "allennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 4.21.2 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed tokenizers-0.12.1 transformers-4.21.2\n",
      "Looking in links: /home/jupyter/code_baseline/FB3_pip_wheels\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.12.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "os.system('pip install iterative-stratification==0.1.7')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('pip uninstall -y tokenizers')\n",
    "os.system('python -m pip install --no-index --find-links=/home/jupyter/code_baseline/FB3_pip_wheels transformers')\n",
    "os.system('python -m pip install --no-index --find-links=/home/jupyter/code_baseline/FB3_pip_wheels tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "# print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "# print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007998,
     "end_time": "2022-08-31T07:03:04.079768",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.07177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.024132,
     "end_time": "2022-08-31T07:03:04.111108",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.086976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012589,
     "end_time": "2022-08-31T07:03:04.13341",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.120821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.242687,
     "end_time": "2022-08-31T07:03:04.383434",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.140747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "\n",
    "train = pd.read_csv(path+'train.csv')\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "submission = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "\n",
    "if CFG.DEBUG:\n",
    "    # display(train.groupby('fold').size())\n",
    "    train = train.sample(n=50, random_state=0).reset_index(drop=True)\n",
    "    # display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007561,
     "end_time": "2022-08-31T07:03:04.604916",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.597355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 7.351568,
     "end_time": "2022-08-31T07:03:11.964298",
     "exception": false,
     "start_time": "2022-08-31T07:03:04.61273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer models/roberta/roberta-base/config.json\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"/home/jupyter/models/{CFG.model}/\")\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008127,
     "end_time": "2022-08-31T07:03:11.985369",
     "exception": false,
     "start_time": "2022-08-31T07:03:11.977242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 5.893032,
     "end_time": "2022-08-31T07:03:17.886504",
     "exception": false,
     "start_time": "2022-08-31T07:03:11.993472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "# lengths = []\n",
    "# tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
    "# for text in tk0:\n",
    "#     length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "#     lengths.append(length)\n",
    "# CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
    "# LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.020447,
     "end_time": "2022-08-31T07:03:17.916566",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.896119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008073,
     "end_time": "2022-08-31T07:03:17.933189",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.925116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.033105,
     "end_time": "2022-08-31T07:03:17.97447",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.941365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "#MeanPoolingはoutput_hidden_statesに関係している   https://qiita.com/niship2/items/f84751aed893da869cec\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model_config_path, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model_bin_path, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config_path = f\"/home/jupyter/models/deberta/{model}/model\"\n",
    "# model_bin_path = f\"/home/jupyter/models/deberta/{CFG.model}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = AutoConfig.from_pretrained(f\"/home/jupyter/models/deberta/{CFG.model}/model\", output_hidden_states=True)\n",
    "# AutoModel.from_pretrained(f\"/home/jupyter/models/deberta/{CFG.model}/model\", config=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008106,
     "end_time": "2022-08-31T07:03:17.993861",
     "exception": false,
     "start_time": "2022-08-31T07:03:17.985755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.021697,
     "end_time": "2022-08-31T07:03:18.02376",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.002063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Loss\n",
    "# ====================================================\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008452,
     "end_time": "2022-08-31T07:03:18.041557",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.033105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.030759,
     "end_time": "2022-08-31T07:03:18.08056",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.049801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0081,
     "end_time": "2022-08-31T07:03:18.100232",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.092132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.033332,
     "end_time": "2022-08-31T07:03:18.141812",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.10848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
    "    \n",
    "    best_score = np.inf\n",
    "    patience = CFG.patience\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                break\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 11935.46951,
     "end_time": "2022-08-31T10:22:13.621316",
     "exception": false,
     "start_time": "2022-08-31T07:03:18.151806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "BigBirdConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/bigbird/roberta-large/\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/bigbird/roberta-large/ were not used when initializing BigBirdModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 3s (remain 19m 16s) Loss: 2.7904(2.7904) Grad: 554615.4375  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 33s (remain 9m 16s) Loss: 0.2035(1.0616) Grad: 87720.0547  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 1m 5s (remain 8m 36s) Loss: 0.1698(0.6358) Grad: 46153.5508  LR: 0.00001999  \n",
      "Epoch: [1][60/366] Elapsed 1m 37s (remain 8m 9s) Loss: 0.1886(0.4795) Grad: 27059.5762  LR: 0.00001999  \n",
      "Epoch: [1][80/366] Elapsed 2m 12s (remain 7m 45s) Loss: 0.0819(0.3995) Grad: 21619.7539  LR: 0.00001998  \n",
      "Epoch: [1][100/366] Elapsed 2m 46s (remain 7m 15s) Loss: 0.2448(0.3526) Grad: 388693.3750  LR: 0.00001996  \n",
      "Epoch: [1][120/366] Elapsed 3m 19s (remain 6m 44s) Loss: 0.4416(0.3457) Grad: 12572.2012  LR: 0.00001995  \n",
      "Epoch: [1][140/366] Elapsed 3m 53s (remain 6m 13s) Loss: 0.3027(0.3275) Grad: 6763.8135  LR: 0.00001993  \n",
      "Epoch: [1][160/366] Elapsed 4m 28s (remain 5m 41s) Loss: 0.1123(0.3054) Grad: 2688.1367  LR: 0.00001990  \n",
      "Epoch: [1][180/366] Elapsed 5m 2s (remain 5m 8s) Loss: 0.2286(0.2885) Grad: 5048.3921  LR: 0.00001988  \n",
      "Epoch: [1][200/366] Elapsed 5m 36s (remain 4m 36s) Loss: 0.1067(0.2715) Grad: 137751.4062  LR: 0.00001985  \n",
      "Epoch: [1][220/366] Elapsed 6m 10s (remain 4m 3s) Loss: 0.1483(0.2591) Grad: 5002.8965  LR: 0.00001982  \n",
      "Epoch: [1][240/366] Elapsed 6m 44s (remain 3m 29s) Loss: 0.1834(0.2492) Grad: 2731.0105  LR: 0.00001979  \n",
      "Epoch: [1][260/366] Elapsed 7m 19s (remain 2m 56s) Loss: 0.1150(0.2395) Grad: 1878.9167  LR: 0.00001975  \n",
      "Epoch: [1][280/366] Elapsed 7m 53s (remain 2m 23s) Loss: 0.0938(0.2320) Grad: 1495.8719  LR: 0.00001971  \n",
      "Epoch: [1][300/366] Elapsed 8m 27s (remain 1m 49s) Loss: 0.1183(0.2247) Grad: 2958.3210  LR: 0.00001967  \n",
      "Epoch: [1][320/366] Elapsed 9m 1s (remain 1m 15s) Loss: 0.0879(0.2202) Grad: 1810.9702  LR: 0.00001962  \n",
      "Epoch: [1][340/366] Elapsed 9m 35s (remain 0m 42s) Loss: 0.2384(0.2157) Grad: 3707.7693  LR: 0.00001958  \n",
      "Epoch: [1][360/366] Elapsed 10m 9s (remain 0m 8s) Loss: 0.2444(0.2148) Grad: 1827.3137  LR: 0.00001953  \n",
      "Epoch: [1][365/366] Elapsed 10m 17s (remain 0m 0s) Loss: 0.1855(0.2147) Grad: 2838.8647  LR: 0.00001951  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 23s) Loss: 0.1664(0.1664) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1332(0.1609) \n",
      "EVAL: [40/62] Elapsed 1m 25s (remain 0m 43s) Loss: 0.1439(0.1632) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2147  avg_val_loss: 0.1669  time: 745s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1512(0.1670) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1103(0.1669) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Score: 0.5866  Scores: [0.5789554145697193, 0.5772803458577113, 0.5376759432452557, 0.5780712343687882, 0.6674355734098052, 0.579993157416882]\n",
      "Epoch 1 - Save Best Score: 0.5866 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 11m 45s) Loss: 0.1448(0.1448) Grad: 160020.3125  LR: 0.00001951  \n",
      "Epoch: [2][20/366] Elapsed 0m 35s (remain 9m 46s) Loss: 0.2989(0.2726) Grad: inf  LR: 0.00001946  \n",
      "Epoch: [2][40/366] Elapsed 1m 9s (remain 9m 10s) Loss: 0.1818(0.2646) Grad: 14377.2266  LR: 0.00001940  \n",
      "Epoch: [2][60/366] Elapsed 1m 43s (remain 8m 35s) Loss: 0.3246(0.2382) Grad: 11195.7529  LR: 0.00001934  \n",
      "Epoch: [2][80/366] Elapsed 2m 17s (remain 8m 2s) Loss: 0.1515(0.2152) Grad: 7382.4653  LR: 0.00001928  \n",
      "Epoch: [2][100/366] Elapsed 2m 51s (remain 7m 29s) Loss: 0.1136(0.1958) Grad: 8479.0605  LR: 0.00001921  \n",
      "Epoch: [2][120/366] Elapsed 3m 25s (remain 6m 55s) Loss: 0.1227(0.1804) Grad: 4232.3696  LR: 0.00001914  \n",
      "Epoch: [2][140/366] Elapsed 3m 59s (remain 6m 21s) Loss: 0.0504(0.1684) Grad: 2743.9431  LR: 0.00001907  \n",
      "Epoch: [2][160/366] Elapsed 4m 33s (remain 5m 48s) Loss: 0.1210(0.1610) Grad: 5246.9512  LR: 0.00001900  \n",
      "Epoch: [2][180/366] Elapsed 5m 7s (remain 5m 14s) Loss: 0.1336(0.1542) Grad: 3988.0918  LR: 0.00001892  \n",
      "Epoch: [2][200/366] Elapsed 5m 41s (remain 4m 40s) Loss: 0.1659(0.1504) Grad: 3750.7666  LR: 0.00001884  \n",
      "Epoch: [2][220/366] Elapsed 6m 15s (remain 4m 6s) Loss: 0.1672(0.1467) Grad: 7894.5557  LR: 0.00001876  \n",
      "Epoch: [2][240/366] Elapsed 6m 49s (remain 3m 32s) Loss: 0.0946(0.1428) Grad: 3054.2361  LR: 0.00001868  \n",
      "Epoch: [2][260/366] Elapsed 7m 23s (remain 2m 58s) Loss: 0.1078(0.1400) Grad: 4353.3521  LR: 0.00001859  \n",
      "Epoch: [2][280/366] Elapsed 7m 57s (remain 2m 24s) Loss: 0.1062(0.1369) Grad: 5831.3052  LR: 0.00001850  \n",
      "Epoch: [2][300/366] Elapsed 8m 31s (remain 1m 50s) Loss: 0.1137(0.1344) Grad: 8900.8164  LR: 0.00001841  \n",
      "Epoch: [2][320/366] Elapsed 9m 5s (remain 1m 16s) Loss: 0.1182(0.1325) Grad: 3673.4766  LR: 0.00001832  \n",
      "Epoch: [2][340/366] Elapsed 9m 39s (remain 0m 42s) Loss: 0.0822(0.1301) Grad: 3992.1050  LR: 0.00001822  \n",
      "Epoch: [2][360/366] Elapsed 10m 13s (remain 0m 8s) Loss: 0.1188(0.1284) Grad: 7881.7744  LR: 0.00001812  \n",
      "Epoch: [2][365/366] Elapsed 10m 21s (remain 0m 0s) Loss: 0.1491(0.1282) Grad: 4036.0393  LR: 0.00001810  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 19s) Loss: 0.1028(0.1028) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0921(0.1203) \n",
      "EVAL: [40/62] Elapsed 1m 25s (remain 0m 44s) Loss: 0.0833(0.1194) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.1282  avg_val_loss: 0.1194  time: 750s\n",
      "Epoch 2 - Score: 0.4902  Scores: [0.5331778971310817, 0.471582492690946, 0.44959090837671645, 0.48119326296797144, 0.5070675599699868, 0.49834985884411415]\n",
      "Epoch 2 - Save Best Score: 0.4902 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1071(0.1194) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1010(0.1194) \n",
      "Epoch: [3][0/366] Elapsed 0m 2s (remain 12m 35s) Loss: 0.1117(0.1117) Grad: 138953.7344  LR: 0.00001809  \n",
      "Epoch: [3][20/366] Elapsed 0m 36s (remain 9m 56s) Loss: 0.0721(0.0991) Grad: 145503.7812  LR: 0.00001799  \n",
      "Epoch: [3][40/366] Elapsed 1m 10s (remain 9m 19s) Loss: 0.1284(0.1000) Grad: 162604.1562  LR: 0.00001789  \n",
      "Epoch: [3][60/366] Elapsed 1m 44s (remain 8m 43s) Loss: 0.0602(0.0965) Grad: 123178.4141  LR: 0.00001778  \n",
      "Epoch: [3][80/366] Elapsed 2m 19s (remain 8m 9s) Loss: 0.0952(0.0917) Grad: 175762.7656  LR: 0.00001767  \n",
      "Epoch: [3][100/366] Elapsed 2m 53s (remain 7m 34s) Loss: 0.0546(0.0902) Grad: 83334.2969  LR: 0.00001756  \n",
      "Epoch: [3][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.1321(0.0896) Grad: 105184.5078  LR: 0.00001745  \n",
      "Epoch: [3][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.0770(0.0871) Grad: 59115.7031  LR: 0.00001733  \n",
      "Epoch: [3][160/366] Elapsed 4m 35s (remain 5m 51s) Loss: 0.0584(0.0854) Grad: 41844.8516  LR: 0.00001721  \n",
      "Epoch: [3][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.0883(0.0854) Grad: 76510.2734  LR: 0.00001709  \n",
      "Epoch: [3][200/366] Elapsed 5m 43s (remain 4m 42s) Loss: 0.0899(0.0845) Grad: 90367.5781  LR: 0.00001697  \n",
      "Epoch: [3][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.0845(0.0837) Grad: 58618.2695  LR: 0.00001685  \n",
      "Epoch: [3][240/366] Elapsed 6m 52s (remain 3m 33s) Loss: 0.0757(0.0828) Grad: 60407.0547  LR: 0.00001672  \n",
      "Epoch: [3][260/366] Elapsed 7m 26s (remain 2m 59s) Loss: 0.0433(0.0820) Grad: 22854.8086  LR: 0.00001659  \n",
      "Epoch: [3][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.1013(0.0814) Grad: 315556.5938  LR: 0.00001646  \n",
      "Epoch: [3][300/366] Elapsed 8m 34s (remain 1m 51s) Loss: 0.0594(0.0809) Grad: 54424.4102  LR: 0.00001633  \n",
      "Epoch: [3][320/366] Elapsed 9m 8s (remain 1m 16s) Loss: 0.0653(0.0803) Grad: 38871.6602  LR: 0.00001620  \n",
      "Epoch: [3][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.0591(0.0801) Grad: 53196.3711  LR: 0.00001606  \n",
      "Epoch: [3][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.0627(0.0793) Grad: 47113.8555  LR: 0.00001592  \n",
      "Epoch: [3][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.0845(0.0792) Grad: 102213.6641  LR: 0.00001589  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.0753(0.0753) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0870(0.1071) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.0849(0.1065) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0792  avg_val_loss: 0.1067  time: 754s\n",
      "Epoch 3 - Score: 0.4624  Scores: [0.5028900627913816, 0.4542999001769223, 0.4152129572886457, 0.45924516146939554, 0.4855458277220232, 0.45748724914757555]\n",
      "Epoch 3 - Save Best Score: 0.4624 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1035(0.1066) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1715(0.1067) \n",
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 11m 33s) Loss: 0.0521(0.0521) Grad: 72852.6719  LR: 0.00001588  \n",
      "Epoch: [4][20/366] Elapsed 0m 36s (remain 10m 1s) Loss: 0.0793(0.0639) Grad: 142396.7031  LR: 0.00001574  \n",
      "Epoch: [4][40/366] Elapsed 1m 11s (remain 9m 22s) Loss: 0.0610(0.0640) Grad: 137357.9375  LR: 0.00001560  \n",
      "Epoch: [4][60/366] Elapsed 1m 44s (remain 8m 43s) Loss: 0.0846(0.0655) Grad: 147882.4219  LR: 0.00001546  \n",
      "Epoch: [4][80/366] Elapsed 2m 19s (remain 8m 9s) Loss: 0.0848(0.0677) Grad: 49659.2305  LR: 0.00001532  \n",
      "Epoch: [4][100/366] Elapsed 2m 53s (remain 7m 34s) Loss: 0.0554(0.0675) Grad: 42349.3984  LR: 0.00001517  \n",
      "Epoch: [4][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.0765(0.0683) Grad: 95410.0234  LR: 0.00001502  \n",
      "Epoch: [4][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.0962(0.0690) Grad: 56336.4805  LR: 0.00001487  \n",
      "Epoch: [4][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.1035(0.0687) Grad: 91632.5156  LR: 0.00001472  \n",
      "Epoch: [4][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.0697(0.0691) Grad: 51238.4219  LR: 0.00001457  \n",
      "Epoch: [4][200/366] Elapsed 5m 44s (remain 4m 42s) Loss: 0.0540(0.0686) Grad: 41222.2578  LR: 0.00001442  \n",
      "Epoch: [4][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.0438(0.0691) Grad: 38300.5664  LR: 0.00001426  \n",
      "Epoch: [4][240/366] Elapsed 6m 52s (remain 3m 33s) Loss: 0.0600(0.0694) Grad: 60259.3750  LR: 0.00001411  \n",
      "Epoch: [4][260/366] Elapsed 7m 26s (remain 2m 59s) Loss: 0.0945(0.0694) Grad: 61429.3164  LR: 0.00001395  \n",
      "Epoch: [4][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.0556(0.0694) Grad: 58877.5391  LR: 0.00001379  \n",
      "Epoch: [4][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.0830(0.0689) Grad: 93963.1719  LR: 0.00001363  \n",
      "Epoch: [4][320/366] Elapsed 9m 9s (remain 1m 17s) Loss: 0.1049(0.0692) Grad: 84907.5078  LR: 0.00001347  \n",
      "Epoch: [4][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.0722(0.0696) Grad: 47956.0742  LR: 0.00001331  \n",
      "Epoch: [4][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.0754(0.0696) Grad: 55608.4297  LR: 0.00001315  \n",
      "Epoch: [4][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.0568(0.0696) Grad: 75195.6016  LR: 0.00001311  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 21s) Loss: 0.0740(0.0740) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0886(0.1066) \n",
      "EVAL: [40/62] Elapsed 1m 25s (remain 0m 43s) Loss: 0.0891(0.1064) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0696  avg_val_loss: 0.1064  time: 754s\n",
      "Epoch 4 - Score: 0.4619  Scores: [0.49940359582929483, 0.4541099458620447, 0.41632322824755186, 0.4577079313694581, 0.48509274402360136, 0.4586784627531665]\n",
      "Epoch 4 - Save Best Score: 0.4619 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1042(0.1063) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1788(0.1064) \n",
      "Epoch: [5][0/366] Elapsed 0m 1s (remain 11m 32s) Loss: 0.0935(0.0935) Grad: 116159.8281  LR: 0.00001310  \n",
      "Epoch: [5][20/366] Elapsed 0m 36s (remain 10m 3s) Loss: 0.0580(0.0640) Grad: 98511.4688  LR: 0.00001294  \n",
      "Epoch: [5][40/366] Elapsed 1m 11s (remain 9m 23s) Loss: 0.0640(0.0667) Grad: 75924.2500  LR: 0.00001277  \n",
      "Epoch: [5][60/366] Elapsed 1m 45s (remain 8m 45s) Loss: 0.0619(0.0643) Grad: 89625.7109  LR: 0.00001261  \n",
      "Epoch: [5][80/366] Elapsed 2m 19s (remain 8m 10s) Loss: 0.0780(0.0640) Grad: 78766.6953  LR: 0.00001244  \n",
      "Epoch: [5][100/366] Elapsed 2m 53s (remain 7m 35s) Loss: 0.0754(0.0637) Grad: 116345.1562  LR: 0.00001228  \n",
      "Epoch: [5][120/366] Elapsed 3m 28s (remain 7m 1s) Loss: 0.0825(0.0646) Grad: 100500.7031  LR: 0.00001211  \n",
      "Epoch: [5][140/366] Elapsed 4m 2s (remain 6m 26s) Loss: 0.0768(0.0654) Grad: 116165.4453  LR: 0.00001194  \n",
      "Epoch: [5][160/366] Elapsed 4m 36s (remain 5m 52s) Loss: 0.0675(0.0657) Grad: 147452.1719  LR: 0.00001177  \n",
      "Epoch: [5][180/366] Elapsed 5m 10s (remain 5m 17s) Loss: 0.0682(0.0658) Grad: 151802.2188  LR: 0.00001160  \n",
      "Epoch: [5][200/366] Elapsed 5m 44s (remain 4m 43s) Loss: 0.0452(0.0659) Grad: 73719.6328  LR: 0.00001143  \n",
      "Epoch: [5][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.0517(0.0652) Grad: 76144.6172  LR: 0.00001126  \n",
      "Epoch: [5][240/366] Elapsed 6m 53s (remain 3m 34s) Loss: 0.0548(0.0650) Grad: 112973.8516  LR: 0.00001109  \n",
      "Epoch: [5][260/366] Elapsed 7m 27s (remain 2m 59s) Loss: 0.0426(0.0647) Grad: 63366.7695  LR: 0.00001092  \n",
      "Epoch: [5][280/366] Elapsed 8m 1s (remain 2m 25s) Loss: 0.0891(0.0646) Grad: 278762.5625  LR: 0.00001075  \n",
      "Epoch: [5][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.0654(0.0650) Grad: 111548.6797  LR: 0.00001058  \n",
      "Epoch: [5][320/366] Elapsed 9m 9s (remain 1m 17s) Loss: 0.0522(0.0649) Grad: 80298.3047  LR: 0.00001041  \n",
      "Epoch: [5][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.0477(0.0648) Grad: 122573.9844  LR: 0.00001024  \n",
      "Epoch: [5][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.0484(0.0647) Grad: 91872.5312  LR: 0.00001007  \n",
      "Epoch: [5][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.0627(0.0649) Grad: 75802.6250  LR: 0.00001003  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 29s) Loss: 0.0765(0.0765) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0906(0.1082) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.0903(0.1076) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0649  avg_val_loss: 0.1074  time: 755s\n",
      "Epoch 5 - Score: 0.4640  Scores: [0.5005739804220435, 0.4567186802548698, 0.4174262205879974, 0.45842205433524336, 0.4900153018097913, 0.4608191549287593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 8s (remain 0m 2s) Loss: 0.1044(0.1072) \n",
      "EVAL: [61/62] Elapsed 2m 8s (remain 0m 0s) Loss: 0.1775(0.1074) \n",
      "Epoch: [6][0/366] Elapsed 0m 1s (remain 11m 40s) Loss: 0.0682(0.0682) Grad: 95594.0938  LR: 0.00001002  \n",
      "Epoch: [6][20/366] Elapsed 0m 35s (remain 9m 51s) Loss: 0.0377(0.0678) Grad: 91182.4219  LR: 0.00000985  \n",
      "Epoch: [6][40/366] Elapsed 1m 10s (remain 9m 15s) Loss: 0.0561(0.0629) Grad: 126314.6641  LR: 0.00000967  \n",
      "Epoch: [6][60/366] Elapsed 1m 44s (remain 8m 41s) Loss: 0.0593(0.0633) Grad: 90857.4531  LR: 0.00000950  \n",
      "Epoch: [6][80/366] Elapsed 2m 18s (remain 8m 6s) Loss: 0.0751(0.0633) Grad: 136560.4375  LR: 0.00000933  \n",
      "Epoch: [6][100/366] Elapsed 2m 52s (remain 7m 32s) Loss: 0.0591(0.0631) Grad: 90588.6406  LR: 0.00000916  \n",
      "Epoch: [6][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.0616(0.0632) Grad: 120071.9297  LR: 0.00000899  \n",
      "Epoch: [6][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.0436(0.0628) Grad: 67477.4141  LR: 0.00000882  \n",
      "Epoch: [6][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.0642(0.0626) Grad: 81817.3203  LR: 0.00000865  \n",
      "Epoch: [6][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.0752(0.0632) Grad: 158319.3594  LR: 0.00000848  \n",
      "Epoch: [6][200/366] Elapsed 5m 43s (remain 4m 42s) Loss: 0.0516(0.0629) Grad: 75814.2031  LR: 0.00000831  \n",
      "Epoch: [6][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.0763(0.0623) Grad: 216565.7656  LR: 0.00000814  \n",
      "Epoch: [6][240/366] Elapsed 6m 52s (remain 3m 33s) Loss: 0.0557(0.0621) Grad: 71880.4219  LR: 0.00000797  \n",
      "Epoch: [6][260/366] Elapsed 7m 26s (remain 2m 59s) Loss: 0.0555(0.0619) Grad: 72944.6406  LR: 0.00000781  \n",
      "Epoch: [6][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.0681(0.0618) Grad: 96336.9219  LR: 0.00000764  \n",
      "Epoch: [6][300/366] Elapsed 8m 34s (remain 1m 51s) Loss: 0.1000(0.0619) Grad: 122923.5000  LR: 0.00000747  \n",
      "Epoch: [6][320/366] Elapsed 9m 8s (remain 1m 16s) Loss: 0.0446(0.0619) Grad: 73964.7656  LR: 0.00000731  \n",
      "Epoch: [6][340/366] Elapsed 9m 42s (remain 0m 42s) Loss: 0.0465(0.0622) Grad: 71650.8516  LR: 0.00000714  \n",
      "Epoch: [6][360/366] Elapsed 10m 16s (remain 0m 8s) Loss: 0.0744(0.0623) Grad: 106018.6406  LR: 0.00000698  \n",
      "Epoch: [6][365/366] Elapsed 10m 25s (remain 0m 0s) Loss: 0.0810(0.0624) Grad: 120928.0859  LR: 0.00000694  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.0760(0.0760) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0909(0.1091) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.0924(0.1083) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0624  avg_val_loss: 0.1080  time: 754s\n",
      "Epoch 6 - Score: 0.4654  Scores: [0.5034475218223949, 0.4579040189418354, 0.4186832454802206, 0.4595434520099611, 0.49060200221869604, 0.4623701162068501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1058(0.1079) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1800(0.1080) \n",
      "Epoch: [7][0/366] Elapsed 0m 1s (remain 11m 36s) Loss: 0.0442(0.0442) Grad: 84508.1562  LR: 0.00000693  \n",
      "Epoch: [7][20/366] Elapsed 0m 36s (remain 9m 52s) Loss: 0.0471(0.0604) Grad: 104462.0469  LR: 0.00000677  \n",
      "Epoch: [7][40/366] Elapsed 1m 10s (remain 9m 16s) Loss: 0.0464(0.0599) Grad: 75437.7188  LR: 0.00000661  \n",
      "Epoch: [7][60/366] Elapsed 1m 44s (remain 8m 41s) Loss: 0.0433(0.0591) Grad: 96357.4766  LR: 0.00000645  \n",
      "Epoch: [7][80/366] Elapsed 2m 18s (remain 8m 8s) Loss: 0.0790(0.0602) Grad: 187099.3594  LR: 0.00000629  \n",
      "Epoch: [7][100/366] Elapsed 2m 53s (remain 7m 34s) Loss: 0.0466(0.0597) Grad: 58949.4102  LR: 0.00000613  \n",
      "Epoch: [7][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.0569(0.0602) Grad: 38341.9023  LR: 0.00000597  \n",
      "Epoch: [7][140/366] Elapsed 4m 1s (remain 6m 24s) Loss: 0.0457(0.0603) Grad: 39754.9648  LR: 0.00000581  \n",
      "Epoch: [7][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.0747(0.0605) Grad: 70137.8594  LR: 0.00000566  \n",
      "Epoch: [7][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.0820(0.0604) Grad: 55270.8281  LR: 0.00000551  \n",
      "Epoch: [7][200/366] Elapsed 5m 43s (remain 4m 42s) Loss: 0.0556(0.0605) Grad: 46251.6641  LR: 0.00000535  \n",
      "Epoch: [7][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.0489(0.0604) Grad: 17746.8242  LR: 0.00000520  \n",
      "Epoch: [7][240/366] Elapsed 6m 51s (remain 3m 33s) Loss: 0.0772(0.0602) Grad: 20774.5137  LR: 0.00000505  \n",
      "Epoch: [7][260/366] Elapsed 7m 25s (remain 2m 59s) Loss: 0.0928(0.0602) Grad: 38475.9570  LR: 0.00000490  \n",
      "Epoch: [7][280/366] Elapsed 7m 59s (remain 2m 25s) Loss: 0.0646(0.0600) Grad: 32856.4531  LR: 0.00000476  \n",
      "Epoch: [7][300/366] Elapsed 8m 34s (remain 1m 51s) Loss: 0.0534(0.0600) Grad: 24423.8574  LR: 0.00000461  \n",
      "Epoch: [7][320/366] Elapsed 9m 8s (remain 1m 16s) Loss: 0.0591(0.0603) Grad: 28034.2461  LR: 0.00000447  \n",
      "Epoch: [7][340/366] Elapsed 9m 42s (remain 0m 42s) Loss: 0.0799(0.0604) Grad: 28271.4316  LR: 0.00000433  \n",
      "Epoch: [7][360/366] Elapsed 10m 16s (remain 0m 8s) Loss: 0.0520(0.0603) Grad: 34312.1445  LR: 0.00000419  \n",
      "Epoch: [7][365/366] Elapsed 10m 25s (remain 0m 0s) Loss: 0.0772(0.0605) Grad: 17418.6309  LR: 0.00000415  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 19s) Loss: 0.0752(0.0752) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0900(0.1089) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.0992(0.1086) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0605  avg_val_loss: 0.1086  time: 753s\n",
      "Epoch 7 - Score: 0.4670  Scores: [0.5000925897199745, 0.4567277547265559, 0.42594980901859136, 0.4605026554498341, 0.49461169854812714, 0.46401347731197207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1138(0.1085) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1831(0.1086) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.4619  Scores: [0.49940359582929483, 0.4541099458620447, 0.41632322824755186, 0.4577079313694581, 0.48509274402360136, 0.4586784627531665]\n",
      "========== fold: 1 training ==========\n",
      "BigBirdConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/bigbird/roberta-large/\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/bigbird/roberta-large/ were not used when initializing BigBirdModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 2s (remain 15m 57s) Loss: 2.5905(2.5905) Grad: 530289.5000  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 36s (remain 10m 2s) Loss: 0.2222(1.1016) Grad: 83242.7266  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 1m 10s (remain 9m 20s) Loss: 0.1222(0.6495) Grad: 33473.9766  LR: 0.00001999  \n",
      "Epoch: [1][60/366] Elapsed 1m 44s (remain 8m 44s) Loss: 0.2074(0.4947) Grad: 103512.3516  LR: 0.00001999  \n",
      "Epoch: [1][80/366] Elapsed 2m 19s (remain 8m 9s) Loss: 0.2914(0.4108) Grad: 74952.8516  LR: 0.00001998  \n",
      "Epoch: [1][100/366] Elapsed 2m 53s (remain 7m 34s) Loss: 0.1255(0.3611) Grad: 25678.0977  LR: 0.00001996  \n",
      "Epoch: [1][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.1022(0.3231) Grad: 26739.7559  LR: 0.00001995  \n",
      "Epoch: [1][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.1304(0.2994) Grad: 18411.8086  LR: 0.00001993  \n",
      "Epoch: [1][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.0985(0.2778) Grad: 46048.7812  LR: 0.00001991  \n",
      "Epoch: [1][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.1238(0.2611) Grad: 15993.6182  LR: 0.00001988  \n",
      "Epoch: [1][200/366] Elapsed 5m 43s (remain 4m 41s) Loss: 0.1396(0.2489) Grad: 8909.2764  LR: 0.00001985  \n",
      "Epoch: [1][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.1919(0.2386) Grad: 23535.4590  LR: 0.00001982  \n",
      "Epoch: [1][240/366] Elapsed 6m 51s (remain 3m 33s) Loss: 0.1573(0.2311) Grad: 19663.9375  LR: 0.00001979  \n",
      "Epoch: [1][260/366] Elapsed 7m 25s (remain 2m 59s) Loss: 0.1133(0.2237) Grad: 16728.0273  LR: 0.00001975  \n",
      "Epoch: [1][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.1791(0.2170) Grad: 19122.7852  LR: 0.00001971  \n",
      "Epoch: [1][300/366] Elapsed 8m 34s (remain 1m 51s) Loss: 0.1544(0.2108) Grad: 17965.2051  LR: 0.00001967  \n",
      "Epoch: [1][320/366] Elapsed 9m 8s (remain 1m 16s) Loss: 0.1037(0.2056) Grad: 12612.4727  LR: 0.00001962  \n",
      "Epoch: [1][340/366] Elapsed 9m 42s (remain 0m 42s) Loss: 0.1264(0.2005) Grad: 6970.9487  LR: 0.00001958  \n",
      "Epoch: [1][360/366] Elapsed 10m 16s (remain 0m 8s) Loss: 0.1405(0.1970) Grad: 15058.8203  LR: 0.00001953  \n",
      "Epoch: [1][365/366] Elapsed 10m 25s (remain 0m 0s) Loss: 0.1184(0.1961) Grad: 9176.5127  LR: 0.00001951  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 19s) Loss: 0.0990(0.0990) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1200(0.1214) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.1178(0.1204) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1961  avg_val_loss: 0.1192  time: 753s\n",
      "Epoch 1 - Score: 0.4898  Scores: [0.5221778434888932, 0.4685511174718717, 0.44627290295622996, 0.4799069460465876, 0.507991228878336, 0.5141660398672165]\n",
      "Epoch 1 - Save Best Score: 0.4898 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1206(0.1193) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0538(0.1192) \n",
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 11m 46s) Loss: 0.0691(0.0691) Grad: 102278.8516  LR: 0.00001951  \n",
      "Epoch: [2][20/366] Elapsed 0m 36s (remain 9m 55s) Loss: 0.0923(0.1079) Grad: 166837.1250  LR: 0.00001946  \n",
      "Epoch: [2][40/366] Elapsed 1m 10s (remain 9m 18s) Loss: 0.0719(0.1062) Grad: 141016.9688  LR: 0.00001940  \n",
      "Epoch: [2][60/366] Elapsed 1m 44s (remain 8m 43s) Loss: 0.0967(0.1039) Grad: 153460.0625  LR: 0.00001934  \n",
      "Epoch: [2][80/366] Elapsed 2m 18s (remain 8m 7s) Loss: 0.1081(0.1020) Grad: 58618.4375  LR: 0.00001928  \n",
      "Epoch: [2][100/366] Elapsed 2m 52s (remain 7m 33s) Loss: 0.0813(0.1016) Grad: 63579.7578  LR: 0.00001921  \n",
      "Epoch: [2][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.1481(0.0998) Grad: 60376.2930  LR: 0.00001914  \n",
      "Epoch: [2][140/366] Elapsed 4m 1s (remain 6m 24s) Loss: 0.0895(0.1000) Grad: 43966.7500  LR: 0.00001907  \n",
      "Epoch: [2][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.0956(0.0985) Grad: 73543.0078  LR: 0.00001900  \n",
      "Epoch: [2][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.1009(0.0976) Grad: 69349.5859  LR: 0.00001892  \n",
      "Epoch: [2][200/366] Elapsed 5m 43s (remain 4m 42s) Loss: 0.1374(0.0981) Grad: 48515.8438  LR: 0.00001884  \n",
      "Epoch: [2][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.0806(0.0975) Grad: 31034.2266  LR: 0.00001876  \n",
      "Epoch: [2][240/366] Elapsed 6m 51s (remain 3m 33s) Loss: 0.0972(0.0968) Grad: 44646.5273  LR: 0.00001868  \n",
      "Epoch: [2][260/366] Elapsed 7m 25s (remain 2m 59s) Loss: 0.1810(0.0977) Grad: 73897.0078  LR: 0.00001859  \n",
      "Epoch: [2][280/366] Elapsed 7m 59s (remain 2m 25s) Loss: 0.1248(0.0975) Grad: 22922.1191  LR: 0.00001850  \n",
      "Epoch: [2][300/366] Elapsed 8m 33s (remain 1m 50s) Loss: 0.0886(0.0971) Grad: 23717.2539  LR: 0.00001841  \n",
      "Epoch: [2][320/366] Elapsed 9m 7s (remain 1m 16s) Loss: 0.1445(0.0985) Grad: 15565.9346  LR: 0.00001832  \n",
      "Epoch: [2][340/366] Elapsed 9m 41s (remain 0m 42s) Loss: 0.0944(0.0987) Grad: 17180.2422  LR: 0.00001822  \n",
      "Epoch: [2][360/366] Elapsed 10m 16s (remain 0m 8s) Loss: 0.1098(0.0996) Grad: 18380.5977  LR: 0.00001812  \n",
      "Epoch: [2][365/366] Elapsed 10m 24s (remain 0m 0s) Loss: 0.0898(0.0996) Grad: 21516.3477  LR: 0.00001810  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 19s) Loss: 0.1057(0.1057) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1270(0.1234) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.1172(0.1232) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0996  avg_val_loss: 0.1229  time: 753s\n",
      "Epoch 2 - Score: 0.4966  Scores: [0.5175926560540838, 0.4794504027082865, 0.4281723010852294, 0.5111747820100137, 0.5712993250964769, 0.4721925771573032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1247(0.1229) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0742(0.1229) \n",
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 11m 24s) Loss: 0.0624(0.0624) Grad: 133083.8281  LR: 0.00001809  \n",
      "Epoch: [3][20/366] Elapsed 0m 36s (remain 9m 52s) Loss: 0.1031(0.0872) Grad: 185402.7031  LR: 0.00001799  \n",
      "Epoch: [3][40/366] Elapsed 1m 10s (remain 9m 15s) Loss: 0.1132(0.0820) Grad: 48929.4375  LR: 0.00001789  \n",
      "Epoch: [3][60/366] Elapsed 1m 44s (remain 8m 41s) Loss: 0.0473(0.0794) Grad: 18378.2402  LR: 0.00001778  \n",
      "Epoch: [3][80/366] Elapsed 2m 18s (remain 8m 7s) Loss: 0.0707(0.0800) Grad: 26526.2480  LR: 0.00001767  \n",
      "Epoch: [3][100/366] Elapsed 2m 52s (remain 7m 33s) Loss: 0.1285(0.0798) Grad: 30166.7910  LR: 0.00001756  \n",
      "Epoch: [3][120/366] Elapsed 3m 26s (remain 6m 58s) Loss: 0.1288(0.0809) Grad: 19014.8867  LR: 0.00001745  \n",
      "Epoch: [3][140/366] Elapsed 4m 1s (remain 6m 24s) Loss: 0.0611(0.0802) Grad: 12334.4229  LR: 0.00001733  \n",
      "Epoch: [3][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.0846(0.0808) Grad: 48931.0703  LR: 0.00001721  \n",
      "Epoch: [3][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.0713(0.0812) Grad: 21835.0859  LR: 0.00001709  \n",
      "Epoch: [3][200/366] Elapsed 5m 43s (remain 4m 41s) Loss: 0.0674(0.0822) Grad: 10180.2412  LR: 0.00001697  \n",
      "Epoch: [3][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.1130(0.0838) Grad: 34415.2969  LR: 0.00001685  \n",
      "Epoch: [3][240/366] Elapsed 6m 51s (remain 3m 33s) Loss: 0.0674(0.0835) Grad: 13304.7471  LR: 0.00001672  \n",
      "Epoch: [3][260/366] Elapsed 7m 25s (remain 2m 59s) Loss: 0.1354(0.0828) Grad: 27113.5996  LR: 0.00001659  \n",
      "Epoch: [3][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.0879(0.0836) Grad: 13092.6309  LR: 0.00001646  \n",
      "Epoch: [3][300/366] Elapsed 8m 34s (remain 1m 51s) Loss: 0.1537(0.0843) Grad: 29193.7871  LR: 0.00001633  \n",
      "Epoch: [3][320/366] Elapsed 9m 8s (remain 1m 16s) Loss: 0.0581(0.0846) Grad: 11724.4072  LR: 0.00001620  \n",
      "Epoch: [3][340/366] Elapsed 9m 42s (remain 0m 42s) Loss: 0.0789(0.0844) Grad: 14624.9795  LR: 0.00001606  \n",
      "Epoch: [3][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.0690(0.0845) Grad: 14760.2217  LR: 0.00001593  \n",
      "Epoch: [3][365/366] Elapsed 10m 25s (remain 0m 0s) Loss: 0.0652(0.0845) Grad: 13362.3945  LR: 0.00001589  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 21s) Loss: 0.0781(0.0781) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1362(0.1140) \n",
      "EVAL: [40/62] Elapsed 1m 25s (remain 0m 44s) Loss: 0.1122(0.1119) \n",
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1063(0.1096) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0545(0.1096) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0845  avg_val_loss: 0.1096  time: 754s\n",
      "Epoch 3 - Score: 0.4692  Scores: [0.5039765265281719, 0.45408201301676304, 0.42829427568538736, 0.4783967546049974, 0.47985853857512456, 0.47045201265443876]\n",
      "Epoch 3 - Save Best Score: 0.4692 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 11m 36s) Loss: 0.0612(0.0612) Grad: 84369.6172  LR: 0.00001589  \n",
      "Epoch: [4][20/366] Elapsed 0m 36s (remain 9m 58s) Loss: 0.0607(0.0696) Grad: 68034.4688  LR: 0.00001575  \n",
      "Epoch: [4][40/366] Elapsed 1m 10s (remain 9m 22s) Loss: 0.0411(0.0666) Grad: 103130.1641  LR: 0.00001561  \n",
      "Epoch: [4][60/366] Elapsed 1m 44s (remain 8m 43s) Loss: 0.0794(0.0646) Grad: 82487.0156  LR: 0.00001546  \n",
      "Epoch: [4][80/366] Elapsed 2m 19s (remain 8m 9s) Loss: 0.0647(0.0637) Grad: 50599.1680  LR: 0.00001532  \n",
      "Epoch: [4][100/366] Elapsed 2m 53s (remain 7m 34s) Loss: 0.0506(0.0652) Grad: 24214.4492  LR: 0.00001517  \n",
      "Epoch: [4][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.0651(0.0644) Grad: 19877.2852  LR: 0.00001502  \n",
      "Epoch: [4][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.0569(0.0641) Grad: 31671.0352  LR: 0.00001488  \n",
      "Epoch: [4][160/366] Elapsed 4m 35s (remain 5m 51s) Loss: 0.0310(0.0633) Grad: 19042.8320  LR: 0.00001473  \n",
      "Epoch: [4][180/366] Elapsed 5m 10s (remain 5m 17s) Loss: 0.0540(0.0633) Grad: 29113.4551  LR: 0.00001457  \n",
      "Epoch: [4][200/366] Elapsed 5m 44s (remain 4m 42s) Loss: 0.0422(0.0633) Grad: 19596.9648  LR: 0.00001442  \n",
      "Epoch: [4][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.0512(0.0636) Grad: 29049.8301  LR: 0.00001427  \n",
      "Epoch: [4][240/366] Elapsed 6m 52s (remain 3m 34s) Loss: 0.0525(0.0637) Grad: 75556.8047  LR: 0.00001411  \n",
      "Epoch: [4][260/366] Elapsed 7m 27s (remain 2m 59s) Loss: 0.0768(0.0639) Grad: 294266.5312  LR: 0.00001395  \n",
      "Epoch: [4][280/366] Elapsed 8m 1s (remain 2m 25s) Loss: 0.0575(0.0637) Grad: 18602.0488  LR: 0.00001380  \n",
      "Epoch: [4][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.0566(0.0634) Grad: 24453.8613  LR: 0.00001364  \n",
      "Epoch: [4][320/366] Elapsed 9m 9s (remain 1m 17s) Loss: 0.0703(0.0634) Grad: 25964.9531  LR: 0.00001348  \n",
      "Epoch: [4][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.0830(0.0634) Grad: 30187.6504  LR: 0.00001332  \n",
      "Epoch: [4][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.0617(0.0632) Grad: 26538.4004  LR: 0.00001315  \n",
      "Epoch: [4][365/366] Elapsed 10m 25s (remain 0m 0s) Loss: 0.0745(0.0633) Grad: 28186.0410  LR: 0.00001311  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.0888(0.0888) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1354(0.1112) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.1009(0.1086) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0633  avg_val_loss: 0.1073  time: 754s\n",
      "Epoch 4 - Score: 0.4641  Scores: [0.4943763593387223, 0.44967145024338967, 0.4216568938985813, 0.4651927800197515, 0.48588222769004397, 0.467924643355395]\n",
      "Epoch 4 - Save Best Score: 0.4641 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1033(0.1074) \n",
      "EVAL: [61/62] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0497(0.1073) \n",
      "Epoch: [5][0/366] Elapsed 0m 2s (remain 13m 1s) Loss: 0.0481(0.0481) Grad: 100226.2188  LR: 0.00001310  \n",
      "Epoch: [5][20/366] Elapsed 0m 36s (remain 10m 3s) Loss: 0.0398(0.0494) Grad: 107304.6250  LR: 0.00001294  \n",
      "Epoch: [5][40/366] Elapsed 1m 11s (remain 9m 24s) Loss: 0.0570(0.0477) Grad: 85646.0703  LR: 0.00001278  \n",
      "Epoch: [5][60/366] Elapsed 1m 45s (remain 8m 45s) Loss: 0.0308(0.0477) Grad: 51425.9961  LR: 0.00001261  \n",
      "Epoch: [5][80/366] Elapsed 2m 19s (remain 8m 11s) Loss: 0.0459(0.0472) Grad: 150677.0938  LR: 0.00001245  \n",
      "Epoch: [5][100/366] Elapsed 2m 53s (remain 7m 35s) Loss: 0.0642(0.0463) Grad: 98198.2031  LR: 0.00001228  \n",
      "Epoch: [5][120/366] Elapsed 3m 27s (remain 7m 0s) Loss: 0.0324(0.0470) Grad: 78727.2812  LR: 0.00001211  \n",
      "Epoch: [5][140/366] Elapsed 4m 2s (remain 6m 26s) Loss: 0.0531(0.0472) Grad: 89155.6406  LR: 0.00001195  \n",
      "Epoch: [5][160/366] Elapsed 4m 36s (remain 5m 51s) Loss: 0.0582(0.0473) Grad: 50431.4141  LR: 0.00001178  \n",
      "Epoch: [5][180/366] Elapsed 5m 10s (remain 5m 17s) Loss: 0.0454(0.0472) Grad: 108590.9453  LR: 0.00001161  \n",
      "Epoch: [5][200/366] Elapsed 5m 44s (remain 4m 43s) Loss: 0.0494(0.0471) Grad: 67795.2266  LR: 0.00001144  \n",
      "Epoch: [5][220/366] Elapsed 6m 19s (remain 4m 8s) Loss: 0.0511(0.0476) Grad: 80857.0938  LR: 0.00001127  \n",
      "Epoch: [5][240/366] Elapsed 6m 53s (remain 3m 34s) Loss: 0.0576(0.0472) Grad: 65812.7969  LR: 0.00001110  \n",
      "Epoch: [5][260/366] Elapsed 7m 27s (remain 3m 0s) Loss: 0.0582(0.0474) Grad: 84734.6094  LR: 0.00001093  \n",
      "Epoch: [5][280/366] Elapsed 8m 1s (remain 2m 25s) Loss: 0.0395(0.0474) Grad: 102477.3672  LR: 0.00001076  \n",
      "Epoch: [5][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.0446(0.0475) Grad: 84703.2578  LR: 0.00001059  \n",
      "Epoch: [5][320/366] Elapsed 9m 10s (remain 1m 17s) Loss: 0.0447(0.0472) Grad: 85483.7734  LR: 0.00001042  \n",
      "Epoch: [5][340/366] Elapsed 9m 44s (remain 0m 42s) Loss: 0.0332(0.0476) Grad: 38753.2227  LR: 0.00001024  \n",
      "Epoch: [5][360/366] Elapsed 10m 18s (remain 0m 8s) Loss: 0.0396(0.0477) Grad: 43545.2891  LR: 0.00001007  \n",
      "Epoch: [5][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.0349(0.0477) Grad: 33723.7617  LR: 0.00001003  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 21s) Loss: 0.0884(0.0884) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1323(0.1110) \n",
      "EVAL: [40/62] Elapsed 1m 25s (remain 0m 44s) Loss: 0.1009(0.1091) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0477  avg_val_loss: 0.1082  time: 754s\n",
      "Epoch 5 - Score: 0.4662  Scores: [0.4987238544147335, 0.44895498372498466, 0.42319684221713183, 0.47228365679884926, 0.48697832776676997, 0.46709507826562985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.0993(0.1083) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0467(0.1082) \n",
      "Epoch: [6][0/366] Elapsed 0m 1s (remain 11m 25s) Loss: 0.0348(0.0348) Grad: 78880.1562  LR: 0.00001002  \n",
      "Epoch: [6][20/366] Elapsed 0m 36s (remain 9m 53s) Loss: 0.0319(0.0408) Grad: 61417.8594  LR: 0.00000985  \n",
      "Epoch: [6][40/366] Elapsed 1m 10s (remain 9m 17s) Loss: 0.0528(0.0421) Grad: 26438.9668  LR: 0.00000968  \n",
      "Epoch: [6][60/366] Elapsed 1m 44s (remain 8m 42s) Loss: 0.0547(0.0414) Grad: 69086.4453  LR: 0.00000951  \n",
      "Epoch: [6][80/366] Elapsed 2m 18s (remain 8m 8s) Loss: 0.0459(0.0417) Grad: 30489.6406  LR: 0.00000934  \n",
      "Epoch: [6][100/366] Elapsed 2m 52s (remain 7m 33s) Loss: 0.0331(0.0415) Grad: 43821.0469  LR: 0.00000917  \n",
      "Epoch: [6][120/366] Elapsed 3m 27s (remain 6m 59s) Loss: 0.0320(0.0416) Grad: 37900.9492  LR: 0.00000900  \n",
      "Epoch: [6][140/366] Elapsed 4m 1s (remain 6m 24s) Loss: 0.0528(0.0422) Grad: 284364.3125  LR: 0.00000882  \n",
      "Epoch: [6][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.0483(0.0424) Grad: 36328.6406  LR: 0.00000865  \n",
      "Epoch: [6][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.0346(0.0422) Grad: 32880.6445  LR: 0.00000849  \n",
      "Epoch: [6][200/366] Elapsed 5m 43s (remain 4m 42s) Loss: 0.0326(0.0421) Grad: 21066.9668  LR: 0.00000832  \n",
      "Epoch: [6][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.0503(0.0422) Grad: 17640.7441  LR: 0.00000815  \n",
      "Epoch: [6][240/366] Elapsed 6m 51s (remain 3m 33s) Loss: 0.0436(0.0422) Grad: 23437.6953  LR: 0.00000798  \n",
      "Epoch: [6][260/366] Elapsed 7m 26s (remain 2m 59s) Loss: 0.0417(0.0426) Grad: 18843.2637  LR: 0.00000781  \n",
      "Epoch: [6][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.0525(0.0425) Grad: 22760.4746  LR: 0.00000764  \n",
      "Epoch: [6][300/366] Elapsed 8m 34s (remain 1m 51s) Loss: 0.0637(0.0429) Grad: 25400.4668  LR: 0.00000748  \n",
      "Epoch: [6][320/366] Elapsed 9m 8s (remain 1m 16s) Loss: 0.0512(0.0430) Grad: 23814.6777  LR: 0.00000731  \n",
      "Epoch: [6][340/366] Elapsed 9m 42s (remain 0m 42s) Loss: 0.0432(0.0430) Grad: 23088.0566  LR: 0.00000715  \n",
      "Epoch: [6][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.0320(0.0436) Grad: 16216.9355  LR: 0.00000698  \n",
      "Epoch: [6][365/366] Elapsed 10m 25s (remain 0m 0s) Loss: 0.0422(0.0437) Grad: 18908.0312  LR: 0.00000694  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 35s) Loss: 0.0888(0.0888) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1318(0.1144) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.1012(0.1132) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0437  avg_val_loss: 0.1126  time: 754s\n",
      "Epoch 6 - Score: 0.4759  Scores: [0.5120903669707125, 0.4651031720999635, 0.43025916091956634, 0.47767626088911314, 0.49411117977078217, 0.47629039829082026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1005(0.1127) \n",
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0397(0.1126) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score: 0.4641  Scores: [0.4943763593387223, 0.44967145024338967, 0.4216568938985813, 0.4651927800197515, 0.48588222769004397, 0.467924643355395]\n",
      "========== fold: 2 training ==========\n",
      "BigBirdConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/bigbird/roberta-large/\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/bigbird/roberta-large/ were not used when initializing BigBirdModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 2s (remain 15m 51s) Loss: 2.4718(2.4718) Grad: 531634.4375  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 36s (remain 10m 2s) Loss: 0.2837(1.0933) Grad: 131944.6719  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 1m 10s (remain 9m 21s) Loss: 0.2506(0.6544) Grad: 86073.8594  LR: 0.00001999  \n",
      "Epoch: [1][60/366] Elapsed 1m 45s (remain 8m 45s) Loss: 0.1009(0.4927) Grad: 89298.4609  LR: 0.00001999  \n",
      "Epoch: [1][80/366] Elapsed 2m 19s (remain 8m 9s) Loss: 0.0939(0.4084) Grad: 39347.5938  LR: 0.00001998  \n",
      "Epoch: [1][100/366] Elapsed 2m 53s (remain 7m 35s) Loss: 0.1090(0.3540) Grad: 21558.8711  LR: 0.00001996  \n",
      "Epoch: [1][120/366] Elapsed 3m 27s (remain 7m 0s) Loss: 0.0745(0.3185) Grad: 34386.4062  LR: 0.00001995  \n",
      "Epoch: [1][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.1158(0.2914) Grad: 55190.0352  LR: 0.00001993  \n",
      "Epoch: [1][160/366] Elapsed 4m 35s (remain 5m 51s) Loss: 0.1857(0.2703) Grad: 93067.8984  LR: 0.00001990  \n",
      "Epoch: [1][180/366] Elapsed 5m 10s (remain 5m 16s) Loss: 0.1160(0.2575) Grad: 51845.1367  LR: 0.00001988  \n",
      "Epoch: [1][200/366] Elapsed 5m 44s (remain 4m 42s) Loss: 0.0902(0.2448) Grad: 43617.1016  LR: 0.00001985  \n",
      "Epoch: [1][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.1555(0.2337) Grad: 52669.4688  LR: 0.00001982  \n",
      "Epoch: [1][240/366] Elapsed 6m 52s (remain 3m 34s) Loss: 0.1006(0.2244) Grad: 33909.6992  LR: 0.00001979  \n",
      "Epoch: [1][260/366] Elapsed 7m 26s (remain 2m 59s) Loss: 0.1333(0.2182) Grad: 60713.2852  LR: 0.00001975  \n",
      "Epoch: [1][280/366] Elapsed 8m 1s (remain 2m 25s) Loss: 0.1090(0.2103) Grad: 42120.9922  LR: 0.00001971  \n",
      "Epoch: [1][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.0823(0.2045) Grad: 30419.1875  LR: 0.00001967  \n",
      "Epoch: [1][320/366] Elapsed 9m 9s (remain 1m 17s) Loss: 0.1243(0.1990) Grad: 28944.4473  LR: 0.00001962  \n",
      "Epoch: [1][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.1507(0.1940) Grad: 54806.4609  LR: 0.00001958  \n",
      "Epoch: [1][360/366] Elapsed 10m 18s (remain 0m 8s) Loss: 0.1197(0.1897) Grad: 58326.4648  LR: 0.00001953  \n",
      "Epoch: [1][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.1278(0.1887) Grad: 44255.9062  LR: 0.00001951  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 21s) Loss: 0.1938(0.1938) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.2012(0.1578) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.1600(0.1575) \n",
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1421(0.1579) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1887  avg_val_loss: 0.1582  time: 755s\n",
      "Epoch 1 - Score: 0.5618  Scores: [0.5769005949101883, 0.5228775694273895, 0.4504302097132755, 0.5308895306852296, 0.7187912792141848, 0.570915843908425]\n",
      "Epoch 1 - Save Best Score: 0.5618 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.3100(0.1582) \n",
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 11m 5s) Loss: 0.1102(0.1102) Grad: 297975.8750  LR: 0.00001951  \n",
      "Epoch: [2][20/366] Elapsed 0m 36s (remain 9m 52s) Loss: 0.1164(0.1131) Grad: 156428.4062  LR: 0.00001946  \n",
      "Epoch: [2][40/366] Elapsed 1m 10s (remain 9m 18s) Loss: 0.0845(0.1081) Grad: 157333.0156  LR: 0.00001940  \n",
      "Epoch: [2][60/366] Elapsed 1m 44s (remain 8m 43s) Loss: 0.1035(0.1055) Grad: 142331.0938  LR: 0.00001934  \n",
      "Epoch: [2][80/366] Elapsed 2m 19s (remain 8m 9s) Loss: 0.1093(0.1054) Grad: 93248.7656  LR: 0.00001928  \n",
      "Epoch: [2][100/366] Elapsed 2m 53s (remain 7m 34s) Loss: 0.0501(0.1017) Grad: 55364.6641  LR: 0.00001921  \n",
      "Epoch: [2][120/366] Elapsed 3m 27s (remain 7m 0s) Loss: 0.0769(0.0997) Grad: 112456.8125  LR: 0.00001914  \n",
      "Epoch: [2][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.0748(0.0987) Grad: 149031.0000  LR: 0.00001907  \n",
      "Epoch: [2][160/366] Elapsed 4m 36s (remain 5m 51s) Loss: 0.0831(0.0987) Grad: 120384.7812  LR: 0.00001900  \n",
      "Epoch: [2][180/366] Elapsed 5m 10s (remain 5m 17s) Loss: 0.0527(0.0982) Grad: 92738.4375  LR: 0.00001892  \n",
      "Epoch: [2][200/366] Elapsed 5m 44s (remain 4m 42s) Loss: 0.0793(0.0973) Grad: 122425.4688  LR: 0.00001884  \n",
      "Epoch: [2][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.1001(0.0967) Grad: 175827.7656  LR: 0.00001876  \n",
      "Epoch: [2][240/366] Elapsed 6m 52s (remain 3m 34s) Loss: 0.0828(0.0965) Grad: 137461.2031  LR: 0.00001868  \n",
      "Epoch: [2][260/366] Elapsed 7m 26s (remain 2m 59s) Loss: 0.0867(0.0962) Grad: 180618.7969  LR: 0.00001859  \n",
      "Epoch: [2][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.0771(0.0956) Grad: 140679.1406  LR: 0.00001850  \n",
      "Epoch: [2][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.0511(0.0958) Grad: 78686.9219  LR: 0.00001841  \n",
      "Epoch: [2][320/366] Elapsed 9m 9s (remain 1m 16s) Loss: 0.0936(0.0952) Grad: 139976.5781  LR: 0.00001832  \n",
      "Epoch: [2][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.0643(0.0948) Grad: 75112.9219  LR: 0.00001822  \n",
      "Epoch: [2][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.0841(0.0942) Grad: 105652.7500  LR: 0.00001812  \n",
      "Epoch: [2][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.0810(0.0941) Grad: 125032.5625  LR: 0.00001810  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.1273(0.1273) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0835(0.1155) \n",
      "EVAL: [40/62] Elapsed 1m 25s (remain 0m 44s) Loss: 0.0974(0.1130) \n",
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1030(0.1132) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0941  avg_val_loss: 0.1132  time: 754s\n",
      "Epoch 2 - Score: 0.4773  Scores: [0.4970755881615178, 0.49293286039328876, 0.43281195054210436, 0.477773657058673, 0.4867576385973171, 0.47665143630799217]\n",
      "Epoch 2 - Save Best Score: 0.4773 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1302(0.1132) \n",
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 11m 33s) Loss: 0.0503(0.0503) Grad: 112124.4141  LR: 0.00001809  \n",
      "Epoch: [3][20/366] Elapsed 0m 36s (remain 10m 1s) Loss: 0.0826(0.0842) Grad: 124803.8438  LR: 0.00001799  \n",
      "Epoch: [3][40/366] Elapsed 1m 11s (remain 9m 22s) Loss: 0.0620(0.0811) Grad: 205324.7656  LR: 0.00001789  \n",
      "Epoch: [3][60/366] Elapsed 1m 44s (remain 8m 44s) Loss: 0.0734(0.0827) Grad: 127537.2500  LR: 0.00001778  \n",
      "Epoch: [3][80/366] Elapsed 2m 19s (remain 8m 10s) Loss: 0.0972(0.0819) Grad: 99890.9688  LR: 0.00001767  \n",
      "Epoch: [3][100/366] Elapsed 2m 53s (remain 7m 34s) Loss: 0.0937(0.0817) Grad: 113577.2266  LR: 0.00001756  \n",
      "Epoch: [3][120/366] Elapsed 3m 27s (remain 7m 0s) Loss: 0.0817(0.0826) Grad: 106108.4688  LR: 0.00001745  \n",
      "Epoch: [3][140/366] Elapsed 4m 1s (remain 6m 25s) Loss: 0.0817(0.0852) Grad: 92585.9453  LR: 0.00001733  \n",
      "Epoch: [3][160/366] Elapsed 4m 35s (remain 5m 51s) Loss: 0.0799(0.0863) Grad: 98803.8203  LR: 0.00001721  \n",
      "Epoch: [3][180/366] Elapsed 5m 10s (remain 5m 17s) Loss: 0.0578(0.0867) Grad: 85010.1875  LR: 0.00001709  \n",
      "Epoch: [3][200/366] Elapsed 5m 44s (remain 4m 42s) Loss: 0.1061(0.0864) Grad: 84855.2969  LR: 0.00001697  \n",
      "Epoch: [3][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.0972(0.0866) Grad: 140935.3125  LR: 0.00001685  \n",
      "Epoch: [3][240/366] Elapsed 6m 53s (remain 3m 34s) Loss: 0.0914(0.0854) Grad: 91610.7734  LR: 0.00001672  \n",
      "Epoch: [3][260/366] Elapsed 7m 27s (remain 2m 59s) Loss: 0.1011(0.0860) Grad: 56542.4609  LR: 0.00001659  \n",
      "Epoch: [3][280/366] Elapsed 8m 1s (remain 2m 25s) Loss: 0.1055(0.0854) Grad: 151414.8906  LR: 0.00001646  \n",
      "Epoch: [3][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.0619(0.0855) Grad: 54641.3359  LR: 0.00001633  \n",
      "Epoch: [3][320/366] Elapsed 9m 9s (remain 1m 17s) Loss: 0.0944(0.0857) Grad: 51655.8125  LR: 0.00001620  \n",
      "Epoch: [3][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.1445(0.0862) Grad: 103145.2031  LR: 0.00001606  \n",
      "Epoch: [3][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.1165(0.0869) Grad: 50773.0078  LR: 0.00001592  \n",
      "Epoch: [3][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.1055(0.0868) Grad: 52922.2500  LR: 0.00001589  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.1315(0.1315) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.0852(0.1202) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.0957(0.1163) \n",
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1117(0.1175) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0868  avg_val_loss: 0.1175  time: 754s\n",
      "Epoch 3 - Score: 0.4864  Scores: [0.5312329167214521, 0.5031260402401551, 0.4303921097970453, 0.4825069380603303, 0.47966027225254254, 0.49126438661554084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.1327(0.1175) \n",
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 11m 36s) Loss: 0.0946(0.0946) Grad: 201865.0156  LR: 0.00001588  \n",
      "Epoch: [4][20/366] Elapsed 0m 35s (remain 9m 48s) Loss: 0.0807(0.0684) Grad: 138336.2344  LR: 0.00001574  \n",
      "Epoch: [4][40/366] Elapsed 1m 10s (remain 9m 15s) Loss: 0.0440(0.0670) Grad: 109783.8516  LR: 0.00001560  \n",
      "Epoch: [4][60/366] Elapsed 1m 44s (remain 8m 41s) Loss: 0.0951(0.0661) Grad: 74292.2344  LR: 0.00001546  \n",
      "Epoch: [4][80/366] Elapsed 2m 18s (remain 8m 7s) Loss: 0.0735(0.0668) Grad: 106990.9531  LR: 0.00001532  \n",
      "Epoch: [4][100/366] Elapsed 2m 52s (remain 7m 33s) Loss: 0.0481(0.0663) Grad: 44917.8555  LR: 0.00001517  \n",
      "Epoch: [4][120/366] Elapsed 3m 26s (remain 6m 58s) Loss: 0.0493(0.0668) Grad: 54015.0625  LR: 0.00001502  \n",
      "Epoch: [4][140/366] Elapsed 4m 0s (remain 6m 24s) Loss: 0.0462(0.0663) Grad: 56791.7734  LR: 0.00001487  \n",
      "Epoch: [4][160/366] Elapsed 4m 35s (remain 5m 50s) Loss: 0.0656(0.0669) Grad: 100311.1875  LR: 0.00001472  \n",
      "Epoch: [4][180/366] Elapsed 5m 9s (remain 5m 16s) Loss: 0.0702(0.0673) Grad: 89045.5625  LR: 0.00001457  \n",
      "Epoch: [4][200/366] Elapsed 5m 43s (remain 4m 41s) Loss: 0.0640(0.0674) Grad: 48155.9219  LR: 0.00001442  \n",
      "Epoch: [4][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.0714(0.0673) Grad: 92726.0391  LR: 0.00001426  \n",
      "Epoch: [4][240/366] Elapsed 6m 51s (remain 3m 33s) Loss: 0.0878(0.0671) Grad: 126895.2109  LR: 0.00001411  \n",
      "Epoch: [4][260/366] Elapsed 7m 25s (remain 2m 59s) Loss: 0.0488(0.0675) Grad: 53027.5898  LR: 0.00001395  \n",
      "Epoch: [4][280/366] Elapsed 8m 0s (remain 2m 25s) Loss: 0.0726(0.0674) Grad: 57040.7031  LR: 0.00001379  \n",
      "Epoch: [4][300/366] Elapsed 8m 34s (remain 1m 51s) Loss: 0.0893(0.0678) Grad: 71060.3984  LR: 0.00001363  \n",
      "Epoch: [4][320/366] Elapsed 9m 8s (remain 1m 16s) Loss: 0.0613(0.0676) Grad: 42674.3945  LR: 0.00001347  \n",
      "Epoch: [4][340/366] Elapsed 9m 42s (remain 0m 42s) Loss: 0.0472(0.0674) Grad: 55560.6797  LR: 0.00001331  \n",
      "Epoch: [4][360/366] Elapsed 10m 16s (remain 0m 8s) Loss: 0.0511(0.0674) Grad: 52854.3242  LR: 0.00001315  \n",
      "Epoch: [4][365/366] Elapsed 10m 25s (remain 0m 0s) Loss: 0.0856(0.0676) Grad: 99309.4688  LR: 0.00001311  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.1498(0.1498) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1366(0.1203) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.1162(0.1214) \n",
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1138(0.1210) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0676  avg_val_loss: 0.1215  time: 753s\n",
      "Epoch 4 - Score: 0.4946  Scores: [0.5067145929580725, 0.46378506610263964, 0.4710943309081462, 0.4786876429673433, 0.5499555052858375, 0.4974302067670182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.3725(0.1215) \n",
      "Epoch: [5][0/366] Elapsed 0m 1s (remain 11m 36s) Loss: 0.0414(0.0414) Grad: 132510.6719  LR: 0.00001310  \n",
      "Epoch: [5][20/366] Elapsed 0m 35s (remain 9m 50s) Loss: 0.0766(0.0575) Grad: inf  LR: 0.00001294  \n",
      "Epoch: [5][40/366] Elapsed 1m 10s (remain 9m 15s) Loss: 0.0428(0.0552) Grad: 50857.5078  LR: 0.00001277  \n",
      "Epoch: [5][60/366] Elapsed 1m 44s (remain 8m 40s) Loss: 0.0500(0.0556) Grad: 33493.7148  LR: 0.00001261  \n",
      "Epoch: [5][80/366] Elapsed 2m 18s (remain 8m 6s) Loss: 0.0632(0.0579) Grad: 43513.8789  LR: 0.00001244  \n",
      "Epoch: [5][100/366] Elapsed 2m 52s (remain 7m 31s) Loss: 0.0563(0.0596) Grad: 15582.3145  LR: 0.00001228  \n",
      "Epoch: [5][120/366] Elapsed 3m 26s (remain 6m 58s) Loss: 0.0600(0.0605) Grad: 22753.2539  LR: 0.00001211  \n",
      "Epoch: [5][140/366] Elapsed 4m 0s (remain 6m 24s) Loss: 0.0730(0.0615) Grad: 17488.0527  LR: 0.00001194  \n",
      "Epoch: [5][160/366] Elapsed 4m 34s (remain 5m 49s) Loss: 0.0653(0.0621) Grad: 22942.8066  LR: 0.00001177  \n",
      "Epoch: [5][180/366] Elapsed 5m 8s (remain 5m 15s) Loss: 0.0547(0.0621) Grad: 9871.2041  LR: 0.00001160  \n",
      "Epoch: [5][200/366] Elapsed 5m 43s (remain 4m 41s) Loss: 0.0660(0.0625) Grad: 49539.7930  LR: 0.00001143  \n",
      "Epoch: [5][220/366] Elapsed 6m 17s (remain 4m 7s) Loss: 0.0754(0.0626) Grad: 32385.5273  LR: 0.00001126  \n",
      "Epoch: [5][240/366] Elapsed 6m 51s (remain 3m 33s) Loss: 0.0489(0.0628) Grad: 15393.6172  LR: 0.00001109  \n",
      "Epoch: [5][260/366] Elapsed 7m 25s (remain 2m 59s) Loss: 0.0760(0.0629) Grad: 9424.6494  LR: 0.00001092  \n",
      "Epoch: [5][280/366] Elapsed 7m 59s (remain 2m 24s) Loss: 0.0726(0.0640) Grad: 5263.4546  LR: 0.00001075  \n",
      "Epoch: [5][300/366] Elapsed 8m 33s (remain 1m 50s) Loss: 0.0662(0.0662) Grad: 3450.2583  LR: 0.00001058  \n",
      "Epoch: [5][320/366] Elapsed 9m 7s (remain 1m 16s) Loss: 0.0653(0.0670) Grad: 3416.4573  LR: 0.00001041  \n",
      "Epoch: [5][340/366] Elapsed 9m 41s (remain 0m 42s) Loss: 0.0802(0.0678) Grad: 5304.6250  LR: 0.00001024  \n",
      "Epoch: [5][360/366] Elapsed 10m 15s (remain 0m 8s) Loss: 0.0897(0.0686) Grad: 8479.3203  LR: 0.00001007  \n",
      "Epoch: [5][365/366] Elapsed 10m 24s (remain 0m 0s) Loss: 0.0479(0.0685) Grad: 2835.9863  LR: 0.00001003  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.1687(0.1687) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.1281(0.1343) \n",
      "EVAL: [40/62] Elapsed 1m 26s (remain 0m 44s) Loss: 0.1216(0.1312) \n",
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.1092(0.1305) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0685  avg_val_loss: 0.1309  time: 752s\n",
      "Epoch 5 - Score: 0.5144  Scores: [0.5231550817927025, 0.5118205834730793, 0.47304200031801485, 0.5468347791748877, 0.5348367560431085, 0.49654178615526057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.2880(0.1309) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 result ==========\n",
      "Score: 0.4773  Scores: [0.4970755881615178, 0.49293286039328876, 0.43281195054210436, 0.477773657058673, 0.4867576385973171, 0.47665143630799217]\n",
      "========== fold: 3 training ==========\n",
      "BigBirdConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/models/bigbird/roberta-large/\",\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at /home/jupyter/models/bigbird/roberta-large/ were not used when initializing BigBirdModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/366] Elapsed 0m 2s (remain 16m 1s) Loss: 2.9169(2.9169) Grad: 515144.8750  LR: 0.00002000  \n",
      "Epoch: [1][20/366] Elapsed 0m 36s (remain 10m 5s) Loss: 0.1714(1.1651) Grad: 156783.6719  LR: 0.00002000  \n",
      "Epoch: [1][40/366] Elapsed 1m 10s (remain 9m 21s) Loss: 0.1429(0.6775) Grad: 64700.2070  LR: 0.00001999  \n",
      "Epoch: [1][60/366] Elapsed 1m 44s (remain 8m 44s) Loss: 0.1480(0.5024) Grad: 63977.6562  LR: 0.00001999  \n",
      "Epoch: [1][80/366] Elapsed 2m 19s (remain 8m 9s) Loss: 0.0876(0.4224) Grad: 48169.9062  LR: 0.00001998  \n",
      "Epoch: [1][100/366] Elapsed 2m 53s (remain 7m 35s) Loss: 0.0919(0.3657) Grad: 46214.3281  LR: 0.00001996  \n",
      "Epoch: [1][120/366] Elapsed 3m 27s (remain 7m 0s) Loss: 0.0800(0.3315) Grad: 33278.5117  LR: 0.00001995  \n",
      "Epoch: [1][140/366] Elapsed 4m 2s (remain 6m 26s) Loss: 0.0930(0.3026) Grad: 33637.7891  LR: 0.00001993  \n",
      "Epoch: [1][160/366] Elapsed 4m 36s (remain 5m 51s) Loss: 0.1373(0.2818) Grad: 63231.6992  LR: 0.00001990  \n",
      "Epoch: [1][180/366] Elapsed 5m 10s (remain 5m 17s) Loss: 0.1202(0.2653) Grad: 80564.6953  LR: 0.00001988  \n",
      "Epoch: [1][200/366] Elapsed 5m 44s (remain 4m 43s) Loss: 0.2171(0.2560) Grad: 59334.0273  LR: 0.00001985  \n",
      "Epoch: [1][220/366] Elapsed 6m 18s (remain 4m 8s) Loss: 0.1544(0.2454) Grad: 76507.8516  LR: 0.00001982  \n",
      "Epoch: [1][240/366] Elapsed 6m 53s (remain 3m 34s) Loss: 0.0846(0.2353) Grad: 114079.8984  LR: 0.00001979  \n",
      "Epoch: [1][260/366] Elapsed 7m 27s (remain 2m 59s) Loss: 0.0794(0.2268) Grad: 34429.2539  LR: 0.00001975  \n",
      "Epoch: [1][280/366] Elapsed 8m 1s (remain 2m 25s) Loss: 0.1476(0.2212) Grad: 61311.0508  LR: 0.00001971  \n",
      "Epoch: [1][300/366] Elapsed 8m 35s (remain 1m 51s) Loss: 0.1446(0.2144) Grad: 67310.1797  LR: 0.00001967  \n",
      "Epoch: [1][320/366] Elapsed 9m 9s (remain 1m 17s) Loss: 0.1181(0.2086) Grad: 27671.3008  LR: 0.00001962  \n",
      "Epoch: [1][340/366] Elapsed 9m 43s (remain 0m 42s) Loss: 0.1260(0.2050) Grad: 22955.0840  LR: 0.00001958  \n",
      "Epoch: [1][360/366] Elapsed 10m 17s (remain 0m 8s) Loss: 0.1092(0.2009) Grad: 18169.2051  LR: 0.00001953  \n",
      "Epoch: [1][365/366] Elapsed 10m 26s (remain 0m 0s) Loss: 0.6414(0.2026) Grad: 71613.3516  LR: 0.00001951  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 20s) Loss: 0.7850(0.7850) \n",
      "EVAL: [20/62] Elapsed 0m 44s (remain 1m 26s) Loss: 0.5965(0.6980) \n",
      "EVAL: [40/62] Elapsed 1m 25s (remain 0m 43s) Loss: 1.0774(0.6586) \n",
      "EVAL: [60/62] Elapsed 2m 7s (remain 0m 2s) Loss: 0.6799(0.6581) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2026  avg_val_loss: 0.6575  time: 754s\n",
      "Epoch 1 - Score: 1.3736  Scores: [1.506419053941096, 1.3239982909019399, 1.4297697060414538, 1.3135152620660333, 1.2294761944584491, 1.4385164880965708]\n",
      "Epoch 1 - Save Best Score: 1.3736 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 7s (remain 0m 0s) Loss: 0.3586(0.6575) \n",
      "Epoch: [2][0/366] Elapsed 0m 1s (remain 12m 0s) Loss: 0.9846(0.9846) Grad: nan  LR: 0.00001951  \n",
      "Epoch: [2][20/366] Elapsed 0m 35s (remain 9m 37s) Loss: 0.3873(0.6916) Grad: 4317.5063  LR: 0.00001946  \n",
      "Epoch: [2][40/366] Elapsed 1m 8s (remain 8m 59s) Loss: 0.1982(0.4669) Grad: 7670.1084  LR: 0.00001940  \n",
      "Epoch: [2][60/366] Elapsed 1m 40s (remain 8m 24s) Loss: 0.2631(0.3839) Grad: 5097.2637  LR: 0.00001934  \n",
      "Epoch: [2][80/366] Elapsed 2m 13s (remain 7m 48s) Loss: 0.2229(0.3465) Grad: 5203.7881  LR: 0.00001928  \n",
      "Epoch: [2][100/366] Elapsed 2m 45s (remain 7m 14s) Loss: 0.1962(0.3241) Grad: 5740.7471  LR: 0.00001921  \n",
      "Epoch: [2][120/366] Elapsed 3m 17s (remain 6m 40s) Loss: 0.3040(0.3080) Grad: 7973.8105  LR: 0.00001914  \n",
      "Epoch: [2][140/366] Elapsed 3m 50s (remain 6m 7s) Loss: 0.2516(0.2925) Grad: 5861.8940  LR: 0.00001907  \n",
      "Epoch: [2][160/366] Elapsed 4m 22s (remain 5m 34s) Loss: 0.1590(0.2829) Grad: 5097.8076  LR: 0.00001900  \n",
      "Epoch: [2][180/366] Elapsed 4m 54s (remain 5m 1s) Loss: 0.1583(0.2737) Grad: 3516.1877  LR: 0.00001892  \n",
      "Epoch: [2][200/366] Elapsed 5m 27s (remain 4m 28s) Loss: 0.1323(0.2675) Grad: 2667.6445  LR: 0.00001884  \n",
      "Epoch: [2][220/366] Elapsed 5m 59s (remain 3m 55s) Loss: 0.1562(0.2636) Grad: 5471.9380  LR: 0.00001876  \n",
      "Epoch: [2][240/366] Elapsed 6m 31s (remain 3m 23s) Loss: 0.3160(0.2596) Grad: 3912.4136  LR: 0.00001868  \n",
      "Epoch: [2][260/366] Elapsed 7m 4s (remain 2m 50s) Loss: 0.4103(0.2560) Grad: 10468.8252  LR: 0.00001859  \n",
      "Epoch: [2][280/366] Elapsed 7m 36s (remain 2m 18s) Loss: 0.1833(0.2529) Grad: 3327.6228  LR: 0.00001850  \n",
      "Epoch: [2][300/366] Elapsed 8m 8s (remain 1m 45s) Loss: 0.3296(0.2505) Grad: 3252.6814  LR: 0.00001841  \n",
      "Epoch: [2][320/366] Elapsed 8m 41s (remain 1m 13s) Loss: 0.3500(0.2485) Grad: 2892.2339  LR: 0.00001832  \n",
      "Epoch: [2][340/366] Elapsed 9m 13s (remain 0m 40s) Loss: 0.2267(0.2459) Grad: 2342.4192  LR: 0.00001822  \n",
      "Epoch: [2][360/366] Elapsed 9m 45s (remain 0m 8s) Loss: 0.2003(0.2426) Grad: 5951.0264  LR: 0.00001812  \n",
      "Epoch: [2][365/366] Elapsed 9m 53s (remain 0m 0s) Loss: 0.2020(0.2417) Grad: 7929.2705  LR: 0.00001810  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 14s) Loss: 0.2467(0.2467) \n",
      "EVAL: [20/62] Elapsed 0m 41s (remain 1m 21s) Loss: 0.2696(0.2172) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1625(0.2214) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1957(0.2158) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.2417  avg_val_loss: 0.2159  time: 716s\n",
      "Epoch 2 - Score: 0.6705  Scores: [0.6703683141764495, 0.656705324615311, 0.5877744371265525, 0.668370730660832, 0.7480769652112459, 0.6917595189638961]\n",
      "Epoch 2 - Save Best Score: 0.6705 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2840(0.2159) \n",
      "Epoch: [3][0/366] Elapsed 0m 1s (remain 11m 21s) Loss: 0.3246(0.3246) Grad: 238736.5469  LR: 0.00001809  \n",
      "Epoch: [3][20/366] Elapsed 0m 34s (remain 9m 28s) Loss: 0.1842(0.2229) Grad: 64565.6797  LR: 0.00001799  \n",
      "Epoch: [3][40/366] Elapsed 1m 7s (remain 8m 53s) Loss: 0.2047(0.2155) Grad: 100875.2344  LR: 0.00001789  \n",
      "Epoch: [3][60/366] Elapsed 1m 39s (remain 8m 17s) Loss: 0.1628(0.2193) Grad: 49826.2227  LR: 0.00001778  \n",
      "Epoch: [3][80/366] Elapsed 2m 12s (remain 7m 45s) Loss: 0.1577(0.2213) Grad: 234351.1250  LR: 0.00001767  \n",
      "Epoch: [3][100/366] Elapsed 2m 44s (remain 7m 12s) Loss: 0.1386(0.2151) Grad: 175578.7500  LR: 0.00001756  \n",
      "Epoch: [3][120/366] Elapsed 3m 17s (remain 6m 39s) Loss: 0.2874(0.2144) Grad: 127082.2344  LR: 0.00001745  \n",
      "Epoch: [3][140/366] Elapsed 3m 49s (remain 6m 6s) Loss: 0.2399(0.2149) Grad: 125880.5156  LR: 0.00001733  \n",
      "Epoch: [3][160/366] Elapsed 4m 22s (remain 5m 34s) Loss: 0.1450(0.2136) Grad: 157386.3750  LR: 0.00001721  \n",
      "Epoch: [3][180/366] Elapsed 4m 55s (remain 5m 1s) Loss: 0.1536(0.2118) Grad: 144433.8594  LR: 0.00001709  \n",
      "Epoch: [3][200/366] Elapsed 5m 27s (remain 4m 28s) Loss: 0.1531(0.2103) Grad: 75304.8594  LR: 0.00001697  \n",
      "Epoch: [3][220/366] Elapsed 6m 0s (remain 3m 56s) Loss: 0.2188(0.2125) Grad: 90323.7031  LR: 0.00001685  \n",
      "Epoch: [3][240/366] Elapsed 6m 32s (remain 3m 23s) Loss: 0.2475(0.2125) Grad: 75102.2969  LR: 0.00001672  \n",
      "Epoch: [3][260/366] Elapsed 7m 5s (remain 2m 51s) Loss: 0.1538(0.2143) Grad: 115902.1484  LR: 0.00001659  \n",
      "Epoch: [3][280/366] Elapsed 7m 37s (remain 2m 18s) Loss: 0.1723(0.2143) Grad: 82559.4922  LR: 0.00001646  \n",
      "Epoch: [3][300/366] Elapsed 8m 10s (remain 1m 45s) Loss: 0.1603(0.2152) Grad: 91107.5234  LR: 0.00001633  \n",
      "Epoch: [3][320/366] Elapsed 8m 42s (remain 1m 13s) Loss: 0.2504(0.2140) Grad: 123441.1250  LR: 0.00001620  \n",
      "Epoch: [3][340/366] Elapsed 9m 14s (remain 0m 40s) Loss: 0.2229(0.2131) Grad: 133624.7812  LR: 0.00001606  \n",
      "Epoch: [3][360/366] Elapsed 9m 47s (remain 0m 8s) Loss: 0.3154(0.2129) Grad: 130641.8594  LR: 0.00001592  \n",
      "Epoch: [3][365/366] Elapsed 9m 55s (remain 0m 0s) Loss: 0.1958(0.2120) Grad: 95052.0078  LR: 0.00001589  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 31s) Loss: 0.2367(0.2367) \n",
      "EVAL: [20/62] Elapsed 0m 42s (remain 1m 22s) Loss: 0.2638(0.2114) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1552(0.2150) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1909(0.2108) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.2120  avg_val_loss: 0.2110  time: 718s\n",
      "Epoch 3 - Score: 0.6611  Scores: [0.6671411136566862, 0.6523835511467777, 0.5866887895014554, 0.6627856418941331, 0.7114081601209412, 0.6859944739695611]\n",
      "Epoch 3 - Save Best Score: 0.6611 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2989(0.2110) \n",
      "Epoch: [4][0/366] Elapsed 0m 1s (remain 11m 16s) Loss: 0.2007(0.2007) Grad: 135751.0625  LR: 0.00001588  \n",
      "Epoch: [4][20/366] Elapsed 0m 34s (remain 9m 28s) Loss: 0.1967(0.2079) Grad: 117209.6641  LR: 0.00001574  \n",
      "Epoch: [4][40/366] Elapsed 1m 7s (remain 8m 53s) Loss: 0.3411(0.2040) Grad: 167657.7500  LR: 0.00001560  \n",
      "Epoch: [4][60/366] Elapsed 1m 39s (remain 8m 18s) Loss: 0.1608(0.2088) Grad: 81406.5625  LR: 0.00001546  \n",
      "Epoch: [4][80/366] Elapsed 2m 12s (remain 7m 44s) Loss: 0.2719(0.2025) Grad: 75051.8281  LR: 0.00001532  \n",
      "Epoch: [4][100/366] Elapsed 2m 44s (remain 7m 11s) Loss: 0.2305(0.2033) Grad: 91765.1953  LR: 0.00001517  \n",
      "Epoch: [4][120/366] Elapsed 3m 17s (remain 6m 39s) Loss: 0.1511(0.2021) Grad: 62015.2656  LR: 0.00001502  \n",
      "Epoch: [4][140/366] Elapsed 3m 49s (remain 6m 6s) Loss: 0.1308(0.2001) Grad: 117635.9219  LR: 0.00001487  \n",
      "Epoch: [4][160/366] Elapsed 4m 22s (remain 5m 33s) Loss: 0.2322(0.2013) Grad: 85529.1953  LR: 0.00001472  \n",
      "Epoch: [4][180/366] Elapsed 4m 54s (remain 5m 1s) Loss: 0.0998(0.2022) Grad: 102902.3359  LR: 0.00001457  \n",
      "Epoch: [4][200/366] Elapsed 5m 27s (remain 4m 28s) Loss: 0.1990(0.2003) Grad: 161827.4844  LR: 0.00001442  \n",
      "Epoch: [4][220/366] Elapsed 5m 59s (remain 3m 56s) Loss: 0.2979(0.2021) Grad: 72264.7578  LR: 0.00001426  \n",
      "Epoch: [4][240/366] Elapsed 6m 32s (remain 3m 23s) Loss: 0.3512(0.2033) Grad: 71789.5938  LR: 0.00001411  \n",
      "Epoch: [4][260/366] Elapsed 7m 4s (remain 2m 50s) Loss: 0.3035(0.2031) Grad: 108069.0625  LR: 0.00001395  \n",
      "Epoch: [4][280/366] Elapsed 7m 37s (remain 2m 18s) Loss: 0.1093(0.2034) Grad: 63923.9766  LR: 0.00001379  \n",
      "Epoch: [4][300/366] Elapsed 8m 9s (remain 1m 45s) Loss: 0.2225(0.2035) Grad: 108350.5312  LR: 0.00001363  \n",
      "Epoch: [4][320/366] Elapsed 8m 42s (remain 1m 13s) Loss: 0.1190(0.2036) Grad: 134162.3594  LR: 0.00001347  \n",
      "Epoch: [4][340/366] Elapsed 9m 14s (remain 0m 40s) Loss: 0.1934(0.2044) Grad: 231372.0625  LR: 0.00001331  \n",
      "Epoch: [4][360/366] Elapsed 9m 46s (remain 0m 8s) Loss: 0.1190(0.2045) Grad: 107553.2969  LR: 0.00001315  \n",
      "Epoch: [4][365/366] Elapsed 9m 55s (remain 0m 0s) Loss: 0.1585(0.2045) Grad: 85905.5469  LR: 0.00001311  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 13s) Loss: 0.2362(0.2362) \n",
      "EVAL: [20/62] Elapsed 0m 41s (remain 1m 21s) Loss: 0.2636(0.2114) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1542(0.2149) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1910(0.2107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.2045  avg_val_loss: 0.2109  time: 717s\n",
      "Epoch 4 - Score: 0.6608  Scores: [0.6672111996719403, 0.6523867489074089, 0.5866987201282304, 0.6615214173930927, 0.7120571021804191, 0.6849495237612844]\n",
      "Epoch 4 - Save Best Score: 0.6608 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2942(0.2109) \n",
      "Epoch: [5][0/366] Elapsed 0m 1s (remain 11m 18s) Loss: 0.0954(0.0954) Grad: 128027.2656  LR: 0.00001310  \n",
      "Epoch: [5][20/366] Elapsed 0m 34s (remain 9m 30s) Loss: 0.2114(0.2056) Grad: 59324.6992  LR: 0.00001294  \n",
      "Epoch: [5][40/366] Elapsed 1m 7s (remain 8m 54s) Loss: 0.3501(0.1987) Grad: 260253.8906  LR: 0.00001277  \n",
      "Epoch: [5][60/366] Elapsed 1m 39s (remain 8m 18s) Loss: 0.1750(0.1989) Grad: 112753.2266  LR: 0.00001261  \n",
      "Epoch: [5][80/366] Elapsed 2m 12s (remain 7m 45s) Loss: 0.2788(0.2063) Grad: 79303.6328  LR: 0.00001244  \n",
      "Epoch: [5][100/366] Elapsed 2m 44s (remain 7m 12s) Loss: 0.1777(0.2031) Grad: 200365.6406  LR: 0.00001228  \n",
      "Epoch: [5][120/366] Elapsed 3m 17s (remain 6m 39s) Loss: 0.1187(0.1989) Grad: 95681.0703  LR: 0.00001211  \n",
      "Epoch: [5][140/366] Elapsed 3m 49s (remain 6m 6s) Loss: 0.2239(0.2003) Grad: 168515.0156  LR: 0.00001194  \n",
      "Epoch: [5][160/366] Elapsed 4m 22s (remain 5m 34s) Loss: 0.2554(0.2027) Grad: 118349.3047  LR: 0.00001177  \n",
      "Epoch: [5][180/366] Elapsed 4m 55s (remain 5m 1s) Loss: 0.2722(0.2006) Grad: 170082.2031  LR: 0.00001160  \n",
      "Epoch: [5][200/366] Elapsed 5m 27s (remain 4m 28s) Loss: 0.2206(0.2020) Grad: 104684.5625  LR: 0.00001143  \n",
      "Epoch: [5][220/366] Elapsed 6m 0s (remain 3m 56s) Loss: 0.2228(0.2019) Grad: 123219.7656  LR: 0.00001126  \n",
      "Epoch: [5][240/366] Elapsed 6m 32s (remain 3m 23s) Loss: 0.1992(0.2016) Grad: 116888.8203  LR: 0.00001109  \n",
      "Epoch: [5][260/366] Elapsed 7m 5s (remain 2m 51s) Loss: 0.2088(0.2018) Grad: 112242.8984  LR: 0.00001092  \n",
      "Epoch: [5][280/366] Elapsed 7m 37s (remain 2m 18s) Loss: 0.1631(0.2025) Grad: 133061.4688  LR: 0.00001075  \n",
      "Epoch: [5][300/366] Elapsed 8m 10s (remain 1m 45s) Loss: 0.1399(0.2035) Grad: 105183.6562  LR: 0.00001058  \n",
      "Epoch: [5][320/366] Elapsed 8m 42s (remain 1m 13s) Loss: 0.2140(0.2051) Grad: 114773.0391  LR: 0.00001041  \n",
      "Epoch: [5][340/366] Elapsed 9m 15s (remain 0m 40s) Loss: 0.1484(0.2041) Grad: 101691.4219  LR: 0.00001024  \n",
      "Epoch: [5][360/366] Elapsed 9m 47s (remain 0m 8s) Loss: 0.2157(0.2041) Grad: 117787.9766  LR: 0.00001007  \n",
      "Epoch: [5][365/366] Elapsed 9m 56s (remain 0m 0s) Loss: 0.3117(0.2046) Grad: 72646.8047  LR: 0.00001003  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 16s) Loss: 0.2374(0.2374) \n",
      "EVAL: [20/62] Elapsed 0m 42s (remain 1m 22s) Loss: 0.2628(0.2118) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1539(0.2152) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1931(0.2108) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.2046  avg_val_loss: 0.2110  time: 718s\n",
      "Epoch 5 - Score: 0.6611  Scores: [0.6674765820306274, 0.6537163085934696, 0.5867621226369107, 0.6614671016504643, 0.7118806863444178, 0.6850739226481138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2846(0.2110) \n",
      "Epoch: [6][0/366] Elapsed 0m 1s (remain 11m 8s) Loss: 0.1958(0.1958) Grad: 68793.2266  LR: 0.00001002  \n",
      "Epoch: [6][20/366] Elapsed 0m 34s (remain 9m 23s) Loss: 0.2645(0.2232) Grad: 146884.8281  LR: 0.00000985  \n",
      "Epoch: [6][40/366] Elapsed 1m 6s (remain 8m 49s) Loss: 0.2730(0.2146) Grad: 49459.2383  LR: 0.00000967  \n",
      "Epoch: [6][60/366] Elapsed 1m 39s (remain 8m 16s) Loss: 0.1907(0.2073) Grad: 156947.7656  LR: 0.00000950  \n",
      "Epoch: [6][80/366] Elapsed 2m 11s (remain 7m 43s) Loss: 0.1847(0.2018) Grad: 160620.0938  LR: 0.00000933  \n",
      "Epoch: [6][100/366] Elapsed 2m 44s (remain 7m 10s) Loss: 0.3154(0.2020) Grad: 37374.7852  LR: 0.00000916  \n",
      "Epoch: [6][120/366] Elapsed 3m 16s (remain 6m 38s) Loss: 0.2890(0.2073) Grad: 143119.3906  LR: 0.00000899  \n",
      "Epoch: [6][140/366] Elapsed 3m 49s (remain 6m 5s) Loss: 0.1877(0.2066) Grad: 138360.6250  LR: 0.00000882  \n",
      "Epoch: [6][160/366] Elapsed 4m 21s (remain 5m 33s) Loss: 0.4302(0.2071) Grad: 133845.8906  LR: 0.00000865  \n",
      "Epoch: [6][180/366] Elapsed 4m 54s (remain 5m 0s) Loss: 0.2447(0.2048) Grad: 91132.0703  LR: 0.00000848  \n",
      "Epoch: [6][200/366] Elapsed 5m 26s (remain 4m 28s) Loss: 0.1702(0.2034) Grad: 106689.0859  LR: 0.00000831  \n",
      "Epoch: [6][220/366] Elapsed 5m 59s (remain 3m 55s) Loss: 0.1573(0.2001) Grad: 116516.9375  LR: 0.00000814  \n",
      "Epoch: [6][240/366] Elapsed 6m 31s (remain 3m 23s) Loss: 0.3407(0.2017) Grad: 111470.6094  LR: 0.00000797  \n",
      "Epoch: [6][260/366] Elapsed 7m 4s (remain 2m 50s) Loss: 0.2246(0.2022) Grad: 130944.2969  LR: 0.00000781  \n",
      "Epoch: [6][280/366] Elapsed 7m 36s (remain 2m 18s) Loss: 0.1413(0.2025) Grad: 69828.0859  LR: 0.00000764  \n",
      "Epoch: [6][300/366] Elapsed 8m 9s (remain 1m 45s) Loss: 0.2275(0.2024) Grad: 276147.0625  LR: 0.00000747  \n",
      "Epoch: [6][320/366] Elapsed 8m 41s (remain 1m 13s) Loss: 0.2006(0.2031) Grad: 77591.7188  LR: 0.00000731  \n",
      "Epoch: [6][340/366] Elapsed 9m 14s (remain 0m 40s) Loss: 0.3540(0.2042) Grad: 164550.2969  LR: 0.00000714  \n",
      "Epoch: [6][360/366] Elapsed 9m 46s (remain 0m 8s) Loss: 0.2510(0.2051) Grad: 125515.0547  LR: 0.00000698  \n",
      "Epoch: [6][365/366] Elapsed 9m 54s (remain 0m 0s) Loss: 0.1314(0.2047) Grad: 141204.4375  LR: 0.00000694  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 14s) Loss: 0.2372(0.2372) \n",
      "EVAL: [20/62] Elapsed 0m 41s (remain 1m 21s) Loss: 0.2633(0.2114) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1546(0.2149) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1913(0.2106) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.2047  avg_val_loss: 0.2107  time: 717s\n",
      "Epoch 6 - Score: 0.6607  Scores: [0.6670649047075855, 0.6525799576908001, 0.5867629400102974, 0.6614643010983524, 0.7112547559606897, 0.68526446973636]\n",
      "Epoch 6 - Save Best Score: 0.6607 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2923(0.2107) \n",
      "Epoch: [7][0/366] Elapsed 0m 1s (remain 11m 20s) Loss: 0.1908(0.1908) Grad: 246633.2969  LR: 0.00000693  \n",
      "Epoch: [7][20/366] Elapsed 0m 34s (remain 9m 29s) Loss: 0.1966(0.2445) Grad: 97451.7031  LR: 0.00000677  \n",
      "Epoch: [7][40/366] Elapsed 1m 7s (remain 8m 53s) Loss: 0.1987(0.2275) Grad: 111961.7188  LR: 0.00000661  \n",
      "Epoch: [7][60/366] Elapsed 1m 39s (remain 8m 18s) Loss: 0.2541(0.2145) Grad: 68821.8047  LR: 0.00000645  \n",
      "Epoch: [7][80/366] Elapsed 2m 12s (remain 7m 44s) Loss: 0.1727(0.2098) Grad: 77832.7969  LR: 0.00000629  \n",
      "Epoch: [7][100/366] Elapsed 2m 44s (remain 7m 12s) Loss: 0.3693(0.2068) Grad: 208428.8438  LR: 0.00000613  \n",
      "Epoch: [7][120/366] Elapsed 3m 17s (remain 6m 39s) Loss: 0.1000(0.2085) Grad: 85749.2266  LR: 0.00000597  \n",
      "Epoch: [7][140/366] Elapsed 3m 49s (remain 6m 6s) Loss: 0.1357(0.2030) Grad: 97797.4062  LR: 0.00000581  \n",
      "Epoch: [7][160/366] Elapsed 4m 22s (remain 5m 33s) Loss: 0.2337(0.2046) Grad: 258790.1406  LR: 0.00000566  \n",
      "Epoch: [7][180/366] Elapsed 4m 54s (remain 5m 0s) Loss: 0.1722(0.2038) Grad: 60298.8203  LR: 0.00000551  \n",
      "Epoch: [7][200/366] Elapsed 5m 26s (remain 4m 28s) Loss: 0.2576(0.2050) Grad: 165953.5156  LR: 0.00000535  \n",
      "Epoch: [7][220/366] Elapsed 5m 59s (remain 3m 55s) Loss: 0.1270(0.2047) Grad: 119066.0000  LR: 0.00000520  \n",
      "Epoch: [7][240/366] Elapsed 6m 31s (remain 3m 23s) Loss: 0.1695(0.2043) Grad: 144459.4219  LR: 0.00000505  \n",
      "Epoch: [7][260/366] Elapsed 7m 4s (remain 2m 50s) Loss: 0.3121(0.2049) Grad: 49042.3711  LR: 0.00000490  \n",
      "Epoch: [7][280/366] Elapsed 7m 36s (remain 2m 18s) Loss: 0.2100(0.2052) Grad: 173694.5312  LR: 0.00000476  \n",
      "Epoch: [7][300/366] Elapsed 8m 9s (remain 1m 45s) Loss: 0.1507(0.2039) Grad: 94361.1562  LR: 0.00000461  \n",
      "Epoch: [7][320/366] Elapsed 8m 42s (remain 1m 13s) Loss: 0.1768(0.2049) Grad: 89100.4609  LR: 0.00000447  \n",
      "Epoch: [7][340/366] Elapsed 9m 14s (remain 0m 40s) Loss: 0.2133(0.2049) Grad: 59460.3672  LR: 0.00000433  \n",
      "Epoch: [7][360/366] Elapsed 9m 47s (remain 0m 8s) Loss: 0.1614(0.2048) Grad: 67689.8438  LR: 0.00000419  \n",
      "Epoch: [7][365/366] Elapsed 9m 55s (remain 0m 0s) Loss: 0.1918(0.2044) Grad: 57551.3359  LR: 0.00000415  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 15s) Loss: 0.2360(0.2360) \n",
      "EVAL: [20/62] Elapsed 0m 41s (remain 1m 21s) Loss: 0.2637(0.2109) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1545(0.2147) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1911(0.2106) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.2044  avg_val_loss: 0.2108  time: 717s\n",
      "Epoch 7 - Score: 0.6607  Scores: [0.6672612642752044, 0.6523122988639771, 0.5869205383778587, 0.6609769470816681, 0.7116434508134477, 0.6848720561805909]\n",
      "Epoch 7 - Save Best Score: 0.6607 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2953(0.2108) \n",
      "Epoch: [8][0/366] Elapsed 0m 1s (remain 11m 22s) Loss: 0.1987(0.1987) Grad: 145371.2812  LR: 0.00000414  \n",
      "Epoch: [8][20/366] Elapsed 0m 34s (remain 9m 30s) Loss: 0.1667(0.2066) Grad: 182452.7344  LR: 0.00000401  \n",
      "Epoch: [8][40/366] Elapsed 1m 7s (remain 8m 53s) Loss: 0.1391(0.2079) Grad: 46531.3867  LR: 0.00000387  \n",
      "Epoch: [8][60/366] Elapsed 1m 39s (remain 8m 18s) Loss: 0.2192(0.2074) Grad: 55552.0273  LR: 0.00000374  \n",
      "Epoch: [8][80/366] Elapsed 2m 12s (remain 7m 45s) Loss: 0.4128(0.2132) Grad: 73830.3125  LR: 0.00000360  \n",
      "Epoch: [8][100/366] Elapsed 2m 44s (remain 7m 12s) Loss: 0.1740(0.2071) Grad: 220330.7812  LR: 0.00000347  \n",
      "Epoch: [8][120/366] Elapsed 3m 17s (remain 6m 39s) Loss: 0.1872(0.2074) Grad: 124975.1484  LR: 0.00000334  \n",
      "Epoch: [8][140/366] Elapsed 3m 49s (remain 6m 6s) Loss: 0.1140(0.2046) Grad: 146573.1562  LR: 0.00000322  \n",
      "Epoch: [8][160/366] Elapsed 4m 22s (remain 5m 33s) Loss: 0.2845(0.2046) Grad: 39134.5977  LR: 0.00000309  \n",
      "Epoch: [8][180/366] Elapsed 4m 54s (remain 5m 1s) Loss: 0.1623(0.2054) Grad: 168673.0156  LR: 0.00000297  \n",
      "Epoch: [8][200/366] Elapsed 5m 27s (remain 4m 28s) Loss: 0.1905(0.2054) Grad: 24498.0840  LR: 0.00000285  \n",
      "Epoch: [8][220/366] Elapsed 5m 59s (remain 3m 56s) Loss: 0.1804(0.2043) Grad: 99564.0391  LR: 0.00000273  \n",
      "Epoch: [8][240/366] Elapsed 6m 32s (remain 3m 23s) Loss: 0.2538(0.2049) Grad: 67496.5547  LR: 0.00000261  \n",
      "Epoch: [8][260/366] Elapsed 7m 4s (remain 2m 50s) Loss: 0.1379(0.2051) Grad: 65577.2109  LR: 0.00000250  \n",
      "Epoch: [8][280/366] Elapsed 7m 37s (remain 2m 18s) Loss: 0.1455(0.2034) Grad: 72015.3828  LR: 0.00000239  \n",
      "Epoch: [8][300/366] Elapsed 8m 9s (remain 1m 45s) Loss: 0.1487(0.2040) Grad: 75806.7656  LR: 0.00000228  \n",
      "Epoch: [8][320/366] Elapsed 8m 42s (remain 1m 13s) Loss: 0.2968(0.2042) Grad: 99676.3047  LR: 0.00000217  \n",
      "Epoch: [8][340/366] Elapsed 9m 14s (remain 0m 40s) Loss: 0.0986(0.2035) Grad: 82674.3828  LR: 0.00000206  \n",
      "Epoch: [8][360/366] Elapsed 9m 47s (remain 0m 8s) Loss: 0.1478(0.2037) Grad: 113884.6172  LR: 0.00000196  \n",
      "Epoch: [8][365/366] Elapsed 9m 55s (remain 0m 0s) Loss: 0.2102(0.2039) Grad: 158330.1719  LR: 0.00000193  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 15s) Loss: 0.2364(0.2364) \n",
      "EVAL: [20/62] Elapsed 0m 41s (remain 1m 21s) Loss: 0.2635(0.2109) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1548(0.2147) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1910(0.2106) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.2039  avg_val_loss: 0.2107  time: 717s\n",
      "Epoch 8 - Score: 0.6607  Scores: [0.6670990500584536, 0.6520953793176665, 0.5867288746920071, 0.661556764992435, 0.7115448944278793, 0.684972787422879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2971(0.2107) \n",
      "Epoch: [9][0/366] Elapsed 0m 1s (remain 11m 18s) Loss: 0.2425(0.2425) Grad: 116573.3359  LR: 0.00000193  \n",
      "Epoch: [9][20/366] Elapsed 0m 34s (remain 9m 24s) Loss: 0.1812(0.2033) Grad: 63022.0352  LR: 0.00000183  \n",
      "Epoch: [9][40/366] Elapsed 1m 6s (remain 8m 49s) Loss: 0.2964(0.1995) Grad: 307660.4062  LR: 0.00000173  \n",
      "Epoch: [9][60/366] Elapsed 1m 39s (remain 8m 16s) Loss: 0.1641(0.1970) Grad: 184156.2656  LR: 0.00000164  \n",
      "Epoch: [9][80/366] Elapsed 2m 11s (remain 7m 43s) Loss: 0.2077(0.2051) Grad: 114263.1562  LR: 0.00000154  \n",
      "Epoch: [9][100/366] Elapsed 2m 44s (remain 7m 11s) Loss: 0.1647(0.2063) Grad: 85287.8047  LR: 0.00000145  \n",
      "Epoch: [9][120/366] Elapsed 3m 16s (remain 6m 38s) Loss: 0.2525(0.2148) Grad: 157021.2656  LR: 0.00000137  \n",
      "Epoch: [9][140/366] Elapsed 3m 48s (remain 6m 5s) Loss: 0.2204(0.2128) Grad: 133838.5156  LR: 0.00000128  \n",
      "Epoch: [9][160/366] Elapsed 4m 21s (remain 5m 33s) Loss: 0.1393(0.2128) Grad: 168754.4844  LR: 0.00000120  \n",
      "Epoch: [9][180/366] Elapsed 4m 53s (remain 5m 0s) Loss: 0.2775(0.2112) Grad: 85093.8984  LR: 0.00000112  \n",
      "Epoch: [9][200/366] Elapsed 5m 26s (remain 4m 28s) Loss: 0.2818(0.2083) Grad: 172243.7812  LR: 0.00000104  \n",
      "Epoch: [9][220/366] Elapsed 5m 58s (remain 3m 55s) Loss: 0.1821(0.2082) Grad: 90651.5625  LR: 0.00000097  \n",
      "Epoch: [9][240/366] Elapsed 6m 31s (remain 3m 23s) Loss: 0.3506(0.2069) Grad: 298436.6250  LR: 0.00000089  \n",
      "Epoch: [9][260/366] Elapsed 7m 3s (remain 2m 50s) Loss: 0.1331(0.2043) Grad: 66108.8828  LR: 0.00000082  \n",
      "Epoch: [9][280/366] Elapsed 7m 36s (remain 2m 18s) Loss: 0.2432(0.2034) Grad: 82791.1875  LR: 0.00000076  \n",
      "Epoch: [9][300/366] Elapsed 8m 9s (remain 1m 45s) Loss: 0.1407(0.2027) Grad: 80409.4766  LR: 0.00000069  \n",
      "Epoch: [9][320/366] Elapsed 8m 41s (remain 1m 13s) Loss: 0.3031(0.2039) Grad: 53050.0469  LR: 0.00000063  \n",
      "Epoch: [9][340/366] Elapsed 9m 14s (remain 0m 40s) Loss: 0.1754(0.2041) Grad: 187455.0938  LR: 0.00000057  \n",
      "Epoch: [9][360/366] Elapsed 9m 46s (remain 0m 8s) Loss: 0.1137(0.2037) Grad: 121095.9531  LR: 0.00000052  \n",
      "Epoch: [9][365/366] Elapsed 9m 54s (remain 0m 0s) Loss: 0.3267(0.2040) Grad: 177171.3438  LR: 0.00000050  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 15s) Loss: 0.2363(0.2363) \n",
      "EVAL: [20/62] Elapsed 0m 41s (remain 1m 21s) Loss: 0.2636(0.2109) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1546(0.2147) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1909(0.2105) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.2040  avg_val_loss: 0.2107  time: 716s\n",
      "Epoch 9 - Score: 0.6606  Scores: [0.6672193327059874, 0.6521403184575735, 0.5867304035182022, 0.6611197268843215, 0.7114674562623535, 0.6848720488139345]\n",
      "Epoch 9 - Save Best Score: 0.6606 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2959(0.2107) \n",
      "Epoch: [10][0/366] Elapsed 0m 1s (remain 11m 14s) Loss: 0.2114(0.2114) Grad: 184505.3750  LR: 0.00000050  \n",
      "Epoch: [10][20/366] Elapsed 0m 34s (remain 9m 29s) Loss: 0.1709(0.2128) Grad: 77954.6250  LR: 0.00000045  \n",
      "Epoch: [10][40/366] Elapsed 1m 7s (remain 8m 54s) Loss: 0.2964(0.2158) Grad: 158189.6719  LR: 0.00000040  \n",
      "Epoch: [10][60/366] Elapsed 1m 39s (remain 8m 19s) Loss: 0.1023(0.2124) Grad: 82194.7734  LR: 0.00000035  \n",
      "Epoch: [10][80/366] Elapsed 2m 12s (remain 7m 46s) Loss: 0.1430(0.2108) Grad: 132195.2969  LR: 0.00000031  \n",
      "Epoch: [10][100/366] Elapsed 2m 44s (remain 7m 12s) Loss: 0.3386(0.2102) Grad: 80715.1094  LR: 0.00000027  \n",
      "Epoch: [10][120/366] Elapsed 3m 17s (remain 6m 39s) Loss: 0.1767(0.2075) Grad: 185763.6094  LR: 0.00000023  \n",
      "Epoch: [10][140/366] Elapsed 3m 49s (remain 6m 6s) Loss: 0.1920(0.2068) Grad: 164433.0000  LR: 0.00000020  \n",
      "Epoch: [10][160/366] Elapsed 4m 22s (remain 5m 34s) Loss: 0.2056(0.2048) Grad: 190149.5312  LR: 0.00000016  \n",
      "Epoch: [10][180/366] Elapsed 4m 54s (remain 5m 1s) Loss: 0.1738(0.2041) Grad: 180772.9531  LR: 0.00000013  \n",
      "Epoch: [10][200/366] Elapsed 5m 27s (remain 4m 28s) Loss: 0.1669(0.2057) Grad: 66285.4453  LR: 0.00000011  \n",
      "Epoch: [10][220/366] Elapsed 6m 0s (remain 3m 56s) Loss: 0.1740(0.2061) Grad: 165629.1875  LR: 0.00000008  \n",
      "Epoch: [10][240/366] Elapsed 6m 32s (remain 3m 23s) Loss: 0.1905(0.2049) Grad: 154448.8438  LR: 0.00000006  \n",
      "Epoch: [10][260/366] Elapsed 7m 5s (remain 2m 51s) Loss: 0.0999(0.2059) Grad: 126915.6172  LR: 0.00000005  \n",
      "Epoch: [10][280/366] Elapsed 7m 37s (remain 2m 18s) Loss: 0.1901(0.2050) Grad: 127889.7891  LR: 0.00000003  \n",
      "Epoch: [10][300/366] Elapsed 8m 10s (remain 1m 45s) Loss: 0.2932(0.2045) Grad: 61139.8438  LR: 0.00000002  \n",
      "Epoch: [10][320/366] Elapsed 8m 42s (remain 1m 13s) Loss: 0.2320(0.2041) Grad: 270733.0000  LR: 0.00000001  \n",
      "Epoch: [10][340/366] Elapsed 9m 15s (remain 0m 40s) Loss: 0.3039(0.2037) Grad: 108555.1094  LR: 0.00000000  \n",
      "Epoch: [10][360/366] Elapsed 9m 47s (remain 0m 8s) Loss: 0.2700(0.2043) Grad: 188840.7344  LR: 0.00000000  \n",
      "Epoch: [10][365/366] Elapsed 9m 55s (remain 0m 0s) Loss: 0.3323(0.2042) Grad: 99937.2812  LR: 0.00000000  \n",
      "EVAL: [0/62] Elapsed 0m 2s (remain 2m 15s) Loss: 0.2364(0.2364) \n",
      "EVAL: [20/62] Elapsed 0m 42s (remain 1m 22s) Loss: 0.2635(0.2109) \n",
      "EVAL: [40/62] Elapsed 1m 21s (remain 0m 41s) Loss: 0.1544(0.2146) \n",
      "EVAL: [60/62] Elapsed 2m 1s (remain 0m 1s) Loss: 0.1910(0.2105) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.2042  avg_val_loss: 0.2107  time: 718s\n",
      "Epoch 10 - Score: 0.6605  Scores: [0.6671299492067517, 0.6520870529751401, 0.5866942877059446, 0.6611265339840913, 0.7114630274343143, 0.6847768386743754]\n",
      "Epoch 10 - Save Best Score: 0.6605 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [61/62] Elapsed 2m 1s (remain 0m 0s) Loss: 0.2952(0.2107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score: 0.6605  Scores: [0.6671299492067517, 0.6520870529751401, 0.5866942877059446, 0.6611265339840913, 0.7114630274343143, 0.6847768386743754]\n",
      "========== CV ==========\n",
      "Score: 0.5228  Scores: [0.5445208243236211, 0.5188181424869024, 0.469760519655802, 0.5223300029449622, 0.5510395245509064, 0.5304524354641935]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▄▂▁▁▁▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>█▃▁▁▁▁▁</td></tr><tr><td>[fold0] epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>[fold0] loss</td><td>▂█▃▄▃▄▇▃▂▃▃▂▃▂▂▂▂▂▂▁▂▁▂▁▂▂▂▁▂▂▁▂▁▂▁▂▁▂▂▂</td></tr><tr><td>[fold0] lr</td><td>█████████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>[fold0] score</td><td>█▃▁▁▁▁▁</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▄▃▂▁▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>▆█▂▁▁▃</td></tr><tr><td>[fold1] epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>[fold1] loss</td><td>▅▇█▅▅▆▅▄▄▄▅▃▅▃▃▄▃▃▃▂▃▃▂▂▄▂▃▁▂▂▂▂▂▂▁▂▁▂▂▁</td></tr><tr><td>[fold1] lr</td><td>█████████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁</td></tr><tr><td>[fold1] score</td><td>▇█▂▁▁▄</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▃▂▁▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>█▁▂▂▄</td></tr><tr><td>[fold2] epoch</td><td>▁▃▅▆█</td></tr><tr><td>[fold2] loss</td><td>█▄▃▅▄▅▃▄▃▃▃▃▃▄▃▃▁▂▄▃▂▃▂▂▁▂▂▃▁▂▂▃▂▁▃▂▁▂▂▄</td></tr><tr><td>[fold2] lr</td><td>██████████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>[fold2] score</td><td>█▁▂▂▄</td></tr><tr><td>[fold3] avg_train_loss</td><td>▁█▃▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>[fold3] loss</td><td>▃▂▂▃█▄▅█▆▄▄▅▁▂▁▆▃▅▅█▅▇▃▂▅▅▃▂█▄▄▄▃▃▃▇▄▄▃▄</td></tr><tr><td>[fold3] lr</td><td>███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] score</td><td>█▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.06048</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.10863</td></tr><tr><td>[fold0] epoch</td><td>7</td></tr><tr><td>[fold0] loss</td><td>0.07718</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.46698</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.04368</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.11263</td></tr><tr><td>[fold1] epoch</td><td>6</td></tr><tr><td>[fold1] loss</td><td>0.04224</td></tr><tr><td>[fold1] lr</td><td>1e-05</td></tr><tr><td>[fold1] score</td><td>0.47592</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.06854</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.13086</td></tr><tr><td>[fold2] epoch</td><td>5</td></tr><tr><td>[fold2] loss</td><td>0.04791</td></tr><tr><td>[fold2] lr</td><td>1e-05</td></tr><tr><td>[fold2] score</td><td>0.51437</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.20415</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.21067</td></tr><tr><td>[fold3] epoch</td><td>10</td></tr><tr><td>[fold3] loss</td><td>0.33225</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] score</td><td>0.66055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">001_202211051438</strong>: <a href=\"https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning/runs/1dzvzwzv\" target=\"_blank\">https://wandb.ai/hiroki8383/Feedback%20Prize%20-%20English%20Language%20Learning/runs/1dzvzwzv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221105_053829-1dzvzwzv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[CFG.target_cols].values\n",
    "        preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "        score, scores = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "        return score\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                score = get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        score = round(get_result(oof_df),3)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+f'oof_df.pkl')\n",
    "        \n",
    "    # if CFG.DEBUG:\n",
    "    #     import send2trash\n",
    "    #     send2trash.send2trash(OUTPUT_DIR)\n",
    "    CFG.OUTPUT_DIR = OUTPUT_DIR\n",
    "    dict_cfg = {k: vars(CFG)[k] for k in vars(CFG) if \"__\" not in k}\n",
    "    with open(OUTPUT_DIR+\"dict_cfg\", 'wb') as web:\n",
    "        pickle.dump(dict_cfg , web)\n",
    "    with open(OUTPUT_DIR+\"class_cfg\", 'wb') as web:\n",
    "        pickle.dump(CFG , web)\n",
    "    \n",
    "    \n",
    "    if CFG.wandb:\n",
    "        wandb.config.update(class2dict(CFG))\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/output/ex/bigbird-roberta-large/001/202211051438/\n"
     ]
    }
   ],
   "source": [
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# OUTPUT_DIR = \"/home/jupyter/output/ex/roberta-base/004/202210151736/\"\n",
    "# PREDICT_DIR = OUTPUT_DIR.replace(\"output\",\"predict\")\n",
    "# if not os.path.exists(PREDICT_DIR):\n",
    "#     os.makedirs(PREDICT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file config.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2.55k/2.55k [00:02<00:00, 1.17kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.pth (3KB)\n",
      "Starting upload for file class_cfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 19.0/19.0 [00:02<00:00, 7.45B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: class_cfg (19B)\n",
      "Starting upload for file bigbird-roberta-large_fold1_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1.34G/1.34G [00:30<00:00, 46.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: bigbird-roberta-large_fold1_best.pth (1GB)\n",
      "Starting upload for file bigbird-roberta-large_fold0_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1.34G/1.34G [00:45<00:00, 31.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: bigbird-roberta-large_fold0_best.pth (1GB)\n",
      "Starting upload for file oof_df.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 9.09M/9.09M [00:04<00:00, 2.25MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: oof_df.pkl (9MB)\n",
      "Starting upload for file bigbird-roberta-large_fold3_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1.34G/1.34G [00:34<00:00, 41.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: bigbird-roberta-large_fold3_best.pth (1GB)\n",
      "Starting upload for file train.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 11.6k/11.6k [00:02<00:00, 4.94kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: train.log (12KB)\n",
      "Starting upload for file bigbird-roberta-large_fold2_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 1.34G/1.34G [00:36<00:00, 39.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: bigbird-roberta-large_fold2_best.pth (1GB)\n",
      "Starting upload for file dict_cfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 830k/830k [00:03<00:00, 253kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: dict_cfg (830KB)\n",
      "folder upload\n"
     ]
    }
   ],
   "source": [
    "if CFG.TO_KAGGLE:\n",
    "    UPLOAD_DIR = OUTPUT_DIR\n",
    "    EX_NO = f\"{model_name}-{CFG.file_name}\" # 実験番号などを入れる、folderのpathにする\n",
    "    USERID = 'your_id'\n",
    "\n",
    "\n",
    "    def dataset_upload():\n",
    "        import json\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "        id = f'{USERID}/{EX_NO}'\n",
    "\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata['id'] = id\n",
    "        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "        dataset_metadata['title'] = f'{EX_NO}'\n",
    "\n",
    "        with open(UPLOAD_DIR +'dataset-metadata.json', 'w') as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "\n",
    "        # データセットがない場合\n",
    "        if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n",
    "            api.dataset_create_new(folder=UPLOAD_DIR,\n",
    "                                   convert_to_csv=False,\n",
    "                                   dir_mode='skip')\n",
    "            \n",
    "            \n",
    "             #フォルダーを削除\n",
    "            if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n",
    "                remove_files = glob.glob(OUTPUT_DIR+\"*\")\n",
    "                remove_files.remove(OUTPUT_DIR+\"oof_df.pkl\")\n",
    "                for file in remove_files:\n",
    "                    os.remove(file)\n",
    "                print(\"folder upload\")\n",
    "                            #apiコマンドを書き込む\n",
    "                f = open(f'{model_name}_api_command.txt', 'a')\n",
    "                api_command = f\"!kaggle datasets download -d hiroki8383/{EX_NO}\\n\"\n",
    "                f.write(api_command)\n",
    "                f.close()\n",
    "            else:\n",
    "                print(\"folder not upload\")\n",
    "            \n",
    "            \n",
    "        # データセットがある場合→更新されない場合がある（後で原因追及)\n",
    "        else:\n",
    "            print(\"this folder exsits\")\n",
    "            # api.dataset_create_version(folder=UPLOAD_DIR,\n",
    "            #                            version_notes='update',\n",
    "            #                            convert_to_csv=False,\n",
    "            #                            delete_old_versions=False,\n",
    "            #                            dir_mode='zip')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    dataset_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score: 0.5228  Scores: [0.5445208243236211, 0.5188181424869024, 0.469760519655802, 0.5223300029449622, 0.5510395245509064, 0.5304524354641935]\n"
     ]
    }
   ],
   "source": [
    "if not CFG.DEBUG:\n",
    "    def to_write_score(CFG):\n",
    "        df = pd.read_csv(CFG.score_path)\n",
    "        def get_result2(oof_df):\n",
    "                labels = oof_df[CFG.target_cols].values\n",
    "                preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "                score, scores = get_score(labels, preds)\n",
    "                LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "                return score,scores\n",
    "\n",
    "        score,scores = get_result2(oof_df)\n",
    "        name = \"-\".join(OUTPUT_DIR.split(\"/\")[-4:-2])\n",
    "        base = {\"name\":name,\"score\":score,\"memo\":CFG.MEMO} \n",
    "        base.update(dict(zip(CFG.target_cols,scores)))\n",
    "        df = df.append(base,ignore_index=True)\n",
    "        df.to_csv(CFG.score_path,index=False)\n",
    "    to_write_score(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>memo</th>\n",
       "      <th>LB</th>\n",
       "      <th>score</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>base-discriminator-001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.466957</td>\n",
       "      <td>0.501431</td>\n",
       "      <td>0.455460</td>\n",
       "      <td>0.428477</td>\n",
       "      <td>0.465690</td>\n",
       "      <td>0.485904</td>\n",
       "      <td>0.464780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>base-discriminator-002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.474078</td>\n",
       "      <td>0.504413</td>\n",
       "      <td>0.462707</td>\n",
       "      <td>0.432588</td>\n",
       "      <td>0.469546</td>\n",
       "      <td>0.500918</td>\n",
       "      <td>0.474296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deberta-v3-base-003</td>\n",
       "      <td>2epochs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.459342</td>\n",
       "      <td>0.496531</td>\n",
       "      <td>0.450482</td>\n",
       "      <td>0.419448</td>\n",
       "      <td>0.457413</td>\n",
       "      <td>0.476347</td>\n",
       "      <td>0.455832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deberta-v3-base-003</td>\n",
       "      <td>6epochs</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.456879</td>\n",
       "      <td>0.488333</td>\n",
       "      <td>0.448687</td>\n",
       "      <td>0.417134</td>\n",
       "      <td>0.458769</td>\n",
       "      <td>0.479633</td>\n",
       "      <td>0.448721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deberta-v3-base-006</td>\n",
       "      <td>15folds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.463635</td>\n",
       "      <td>0.479650</td>\n",
       "      <td>0.475910</td>\n",
       "      <td>0.442828</td>\n",
       "      <td>0.463557</td>\n",
       "      <td>0.456412</td>\n",
       "      <td>0.463457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta-v3-base-006</td>\n",
       "      <td>20folds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.449212</td>\n",
       "      <td>0.475585</td>\n",
       "      <td>0.433808</td>\n",
       "      <td>0.415852</td>\n",
       "      <td>0.457436</td>\n",
       "      <td>0.484686</td>\n",
       "      <td>0.427907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deberta-v3-base-006</td>\n",
       "      <td>10folds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.446338</td>\n",
       "      <td>0.490661</td>\n",
       "      <td>0.435001</td>\n",
       "      <td>0.409460</td>\n",
       "      <td>0.417236</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.443871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distilroberta-base-001</td>\n",
       "      <td>4folds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470528</td>\n",
       "      <td>0.501629</td>\n",
       "      <td>0.459765</td>\n",
       "      <td>0.426740</td>\n",
       "      <td>0.473205</td>\n",
       "      <td>0.501976</td>\n",
       "      <td>0.459850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distilroberta-base-002</td>\n",
       "      <td>バッチ16,4folds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.471035</td>\n",
       "      <td>0.503345</td>\n",
       "      <td>0.458993</td>\n",
       "      <td>0.427209</td>\n",
       "      <td>0.471235</td>\n",
       "      <td>0.504850</td>\n",
       "      <td>0.460580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>microsoft-deberta-v3-large-001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.450311</td>\n",
       "      <td>0.482961</td>\n",
       "      <td>0.445596</td>\n",
       "      <td>0.410873</td>\n",
       "      <td>0.449346</td>\n",
       "      <td>0.466162</td>\n",
       "      <td>0.446928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>microsoft-deberta-v3-large-002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.453782</td>\n",
       "      <td>0.486786</td>\n",
       "      <td>0.446127</td>\n",
       "      <td>0.415995</td>\n",
       "      <td>0.452892</td>\n",
       "      <td>0.472186</td>\n",
       "      <td>0.448703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>microsoft-deberta-v3-large-004</td>\n",
       "      <td>unscale</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.460115</td>\n",
       "      <td>0.494655</td>\n",
       "      <td>0.452659</td>\n",
       "      <td>0.419634</td>\n",
       "      <td>0.461738</td>\n",
       "      <td>0.476507</td>\n",
       "      <td>0.455497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>roberta-base-003</td>\n",
       "      <td>1fold</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.459033</td>\n",
       "      <td>0.486786</td>\n",
       "      <td>0.452983</td>\n",
       "      <td>0.423085</td>\n",
       "      <td>0.463976</td>\n",
       "      <td>0.484316</td>\n",
       "      <td>0.443049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>roberta-base-004</td>\n",
       "      <td>4folds</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.466575</td>\n",
       "      <td>0.498291</td>\n",
       "      <td>0.455092</td>\n",
       "      <td>0.427741</td>\n",
       "      <td>0.468486</td>\n",
       "      <td>0.493240</td>\n",
       "      <td>0.456598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>roberta-base-005</td>\n",
       "      <td>バッチ16,4folds</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.463956</td>\n",
       "      <td>0.496089</td>\n",
       "      <td>0.454223</td>\n",
       "      <td>0.425768</td>\n",
       "      <td>0.464463</td>\n",
       "      <td>0.487668</td>\n",
       "      <td>0.455523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>roberta-base-006</td>\n",
       "      <td>バッチ32,4folds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.464111</td>\n",
       "      <td>0.497241</td>\n",
       "      <td>0.453315</td>\n",
       "      <td>0.425326</td>\n",
       "      <td>0.463499</td>\n",
       "      <td>0.488895</td>\n",
       "      <td>0.456390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roberta-large-002</td>\n",
       "      <td>バッチ8_4folds</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta-large-003</td>\n",
       "      <td>バッチ16,2_4fold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.483395</td>\n",
       "      <td>0.457585</td>\n",
       "      <td>0.424750</td>\n",
       "      <td>0.480469</td>\n",
       "      <td>0.494185</td>\n",
       "      <td>0.478415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>roberta-large-004</td>\n",
       "      <td>バッチ32,2_4fold</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.462673</td>\n",
       "      <td>0.487539</td>\n",
       "      <td>0.450118</td>\n",
       "      <td>0.419088</td>\n",
       "      <td>0.469469</td>\n",
       "      <td>0.482251</td>\n",
       "      <td>0.467571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>roberta-large-006</td>\n",
       "      <td>バッチ36,2_4fold</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.466295</td>\n",
       "      <td>0.490185</td>\n",
       "      <td>0.457640</td>\n",
       "      <td>0.423379</td>\n",
       "      <td>0.469386</td>\n",
       "      <td>0.486419</td>\n",
       "      <td>0.470762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>roberta-large-007</td>\n",
       "      <td>バッチ40,2_4fold</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.466273</td>\n",
       "      <td>0.490215</td>\n",
       "      <td>0.453633</td>\n",
       "      <td>0.423818</td>\n",
       "      <td>0.475021</td>\n",
       "      <td>0.485249</td>\n",
       "      <td>0.469699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>roberta-large-008</td>\n",
       "      <td>バッチ32_4folds_epochs10</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>roberta-large-008</td>\n",
       "      <td>バッチ32_4folds_epochs10</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>roberta-large-mnli-001</td>\n",
       "      <td>4folds</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.465286</td>\n",
       "      <td>0.493436</td>\n",
       "      <td>0.456988</td>\n",
       "      <td>0.422165</td>\n",
       "      <td>0.470799</td>\n",
       "      <td>0.487859</td>\n",
       "      <td>0.460470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>svr-001</td>\n",
       "      <td>4fold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.453306</td>\n",
       "      <td>0.483167</td>\n",
       "      <td>0.448158</td>\n",
       "      <td>0.412874</td>\n",
       "      <td>0.455388</td>\n",
       "      <td>0.473294</td>\n",
       "      <td>0.446953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>svr-004</td>\n",
       "      <td>4fold</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.452746</td>\n",
       "      <td>0.481117</td>\n",
       "      <td>0.447022</td>\n",
       "      <td>0.413401</td>\n",
       "      <td>0.455160</td>\n",
       "      <td>0.472988</td>\n",
       "      <td>0.446788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>svr-005</td>\n",
       "      <td>10folds,embedding003を使用</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.455535</td>\n",
       "      <td>0.485642</td>\n",
       "      <td>0.448309</td>\n",
       "      <td>0.414582</td>\n",
       "      <td>0.458728</td>\n",
       "      <td>0.479231</td>\n",
       "      <td>0.446716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>funnel-intermediate-001</td>\n",
       "      <td>ベースライン</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.464516</td>\n",
       "      <td>0.493810</td>\n",
       "      <td>0.461812</td>\n",
       "      <td>0.423381</td>\n",
       "      <td>0.460282</td>\n",
       "      <td>0.479871</td>\n",
       "      <td>0.467938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>roberta-base-007</td>\n",
       "      <td>tensorflow ベースライン</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.460693</td>\n",
       "      <td>0.494831</td>\n",
       "      <td>0.451108</td>\n",
       "      <td>0.421364</td>\n",
       "      <td>0.462382</td>\n",
       "      <td>0.483491</td>\n",
       "      <td>0.450982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>roberta-large-020</td>\n",
       "      <td>tensorflowベースライン</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.461600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>funnel-large-001</td>\n",
       "      <td>ベースライン</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469392</td>\n",
       "      <td>0.500464</td>\n",
       "      <td>0.465704</td>\n",
       "      <td>0.426260</td>\n",
       "      <td>0.472609</td>\n",
       "      <td>0.485169</td>\n",
       "      <td>0.466142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bigbird-roberta-base-001</td>\n",
       "      <td>ベースライン</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.472852</td>\n",
       "      <td>0.504070</td>\n",
       "      <td>0.459654</td>\n",
       "      <td>0.433966</td>\n",
       "      <td>0.472433</td>\n",
       "      <td>0.500781</td>\n",
       "      <td>0.466208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bigbird-roberta-large-001</td>\n",
       "      <td>ベースライン</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.522820</td>\n",
       "      <td>0.544521</td>\n",
       "      <td>0.518818</td>\n",
       "      <td>0.469761</td>\n",
       "      <td>0.522330</td>\n",
       "      <td>0.551040</td>\n",
       "      <td>0.530452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name                     memo    LB     score  cohesion    syntax  vocabulary  phraseology   grammar  conventions\n",
       "0           base-discriminator-001                      NaN  0.46  0.466957  0.501431  0.455460    0.428477     0.465690  0.485904     0.464780\n",
       "1           base-discriminator-002                      NaN  0.45  0.474078  0.504413  0.462707    0.432588     0.469546  0.500918     0.474296\n",
       "2              deberta-v3-base-003                  2epochs   NaN  0.459342  0.496531  0.450482    0.419448     0.457413  0.476347     0.455832\n",
       "3              deberta-v3-base-003                  6epochs  0.44  0.456879  0.488333  0.448687    0.417134     0.458769  0.479633     0.448721\n",
       "4              deberta-v3-base-006                  15folds   NaN  0.463635  0.479650  0.475910    0.442828     0.463557  0.456412     0.463457\n",
       "5              deberta-v3-base-006                  20folds   NaN  0.449212  0.475585  0.433808    0.415852     0.457436  0.484686     0.427907\n",
       "6              deberta-v3-base-006                  10folds   NaN  0.446338  0.490661  0.435001    0.409460     0.417236  0.481800     0.443871\n",
       "7           distilroberta-base-001                   4folds   NaN  0.470528  0.501629  0.459765    0.426740     0.473205  0.501976     0.459850\n",
       "8           distilroberta-base-002             バッチ16,4folds   NaN  0.471035  0.503345  0.458993    0.427209     0.471235  0.504850     0.460580\n",
       "9   microsoft-deberta-v3-large-001                      NaN  0.46  0.450311  0.482961  0.445596    0.410873     0.449346  0.466162     0.446928\n",
       "10  microsoft-deberta-v3-large-002                      NaN  0.44  0.453782  0.486786  0.446127    0.415995     0.452892  0.472186     0.448703\n",
       "11  microsoft-deberta-v3-large-004                  unscale  0.44  0.460115  0.494655  0.452659    0.419634     0.461738  0.476507     0.455497\n",
       "12                roberta-base-003                    1fold  0.45  0.459033  0.486786  0.452983    0.423085     0.463976  0.484316     0.443049\n",
       "13                roberta-base-004                   4folds  0.44  0.466575  0.498291  0.455092    0.427741     0.468486  0.493240     0.456598\n",
       "14                roberta-base-005             バッチ16,4folds  0.44  0.463956  0.496089  0.454223    0.425768     0.464463  0.487668     0.455523\n",
       "15                roberta-base-006             バッチ32,4folds   NaN  0.464111  0.497241  0.453315    0.425326     0.463499  0.488895     0.456390\n",
       "16               roberta-large-002              バッチ8_4folds  0.44  0.461700       NaN       NaN         NaN          NaN       NaN          NaN\n",
       "17               roberta-large-003            バッチ16,2_4fold   NaN  0.469800  0.483395  0.457585    0.424750     0.480469  0.494185     0.478415\n",
       "18               roberta-large-004            バッチ32,2_4fold  0.45  0.462673  0.487539  0.450118    0.419088     0.469469  0.482251     0.467571\n",
       "19               roberta-large-006            バッチ36,2_4fold  0.45  0.466295  0.490185  0.457640    0.423379     0.469386  0.486419     0.470762\n",
       "20               roberta-large-007            バッチ40,2_4fold  0.45  0.466273  0.490215  0.453633    0.423818     0.475021  0.485249     0.469699\n",
       "21               roberta-large-008    バッチ32_4folds_epochs10  0.44  0.462800       NaN       NaN         NaN          NaN       NaN          NaN\n",
       "22               roberta-large-008    バッチ32_4folds_epochs10  0.44  0.462800       NaN       NaN         NaN          NaN       NaN          NaN\n",
       "23          roberta-large-mnli-001                   4folds  0.44  0.465286  0.493436  0.456988    0.422165     0.470799  0.487859     0.460470\n",
       "24                         svr-001                    4fold   NaN  0.453306  0.483167  0.448158    0.412874     0.455388  0.473294     0.446953\n",
       "25                         svr-004                    4fold   NaN  0.452746  0.481117  0.447022    0.413401     0.455160  0.472988     0.446788\n",
       "26                         svr-005  10folds,embedding003を使用   NaN  0.455535  0.485642  0.448309    0.414582     0.458728  0.479231     0.446716\n",
       "27         funnel-intermediate-001                   ベースライン  0.44  0.464516  0.493810  0.461812    0.423381     0.460282  0.479871     0.467938\n",
       "28                roberta-base-007        tensorflow ベースライン  0.44  0.460693  0.494831  0.451108    0.421364     0.462382  0.483491     0.450982\n",
       "29               roberta-large-020         tensorflowベースライン  0.44  0.461600       NaN       NaN         NaN          NaN       NaN          NaN\n",
       "30                funnel-large-001                   ベースライン   NaN  0.469392  0.500464  0.465704    0.426260     0.472609  0.485169     0.466142\n",
       "31        bigbird-roberta-base-001                   ベースライン   NaN  0.472852  0.504070  0.459654    0.433966     0.472433  0.500781     0.466208\n",
       "32       bigbird-roberta-large-001                   ベースライン   NaN  0.522820  0.544521  0.518818    0.469761     0.522330  0.551040     0.530452"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CFG.score_path)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
